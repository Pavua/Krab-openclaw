# -*- coding: utf-8 -*-
"""
AI Handler — Обработчики команд, связанных с AI: авто-ответ, reasoning, агентный цикл.

Извлечён из main.py. Включает:
- auto_reply_logic: умный автоответчик на входящие текстовые сообщения
- !think: Reasoning Mode (глубокое размышление)
- !smart: Agent Workflow (автономное решение задач)
- !code: генерация кода
- !learn: обучение RAG
- !exec: Python REPL (Owner only)
"""

import os
import sys
import time
import asyncio
import traceback
import inspect
import shlex
import re
from io import StringIO
from dataclasses import dataclass
from collections import deque
from typing import Any

from pyrogram import filters, enums
from pyrogram.types import Message
from pyrogram import raw, utils as pyro_utils

from .auth import is_owner, is_authorized, is_superuser
from ..core.markdown_sanitizer import sanitize_markdown_for_telegram, strip_backticks_from_content
from ..core.reaction_learning import ReactionLearningEngine

import structlog
logger = structlog.get_logger(__name__)

def _timeout_from_env(name: str, default_value: int) -> int:
    """Возвращает таймаут из env с безопасным fallback."""
    raw = os.getenv(name, str(default_value)).strip()
    try:
        parsed = int(raw)
        return parsed if parsed > 0 else default_value
    except Exception:
        return default_value


AUTO_REPLY_TIMEOUT_SECONDS = _timeout_from_env("AUTO_REPLY_TIMEOUT_SECONDS", 900)
THINK_TIMEOUT_SECONDS = _timeout_from_env("THINK_TIMEOUT_SECONDS", 420)
AUTO_REPLY_CONTEXT_TOKENS = _timeout_from_env("AUTO_REPLY_CONTEXT_TOKENS", 3000)
AUTO_REPLY_BUSY_NOTICE_SECONDS = _timeout_from_env("AUTO_REPLY_BUSY_NOTICE_SECONDS", 12)
AUTO_REPLY_QUEUE_ENABLED = str(os.getenv("AUTO_REPLY_QUEUE_ENABLED", "1")).strip().lower() in {
    "1", "true", "yes", "on"
}
AUTO_REPLY_QUEUE_MAX_PER_CHAT = _timeout_from_env("AUTO_REPLY_QUEUE_MAX_PER_CHAT", 50)
AUTO_REPLY_QUEUE_NOTIFY_POSITION = str(
    os.getenv("AUTO_REPLY_QUEUE_NOTIFY_POSITION", "1")
).strip().lower() in {"1", "true", "yes", "on"}
AUTO_REPLY_FORWARD_CONTEXT_ENABLED = str(
    os.getenv("AUTO_REPLY_FORWARD_CONTEXT_ENABLED", "1")
).strip().lower() in {"1", "true", "yes", "on"}
try:
    AUTO_REPLY_FORWARD_BURST_WINDOW_SECONDS = float(
        str(os.getenv("AUTO_REPLY_FORWARD_BURST_WINDOW_SECONDS", "1.8")).strip() or "1.8"
    )
    if AUTO_REPLY_FORWARD_BURST_WINDOW_SECONDS < 0.4:
        AUTO_REPLY_FORWARD_BURST_WINDOW_SECONDS = 0.4
except Exception:
    AUTO_REPLY_FORWARD_BURST_WINDOW_SECONDS = 1.8
AUTO_REPLY_FORWARD_BURST_MAX_ITEMS = _timeout_from_env("AUTO_REPLY_FORWARD_BURST_MAX_ITEMS", 8)
AUTO_REPLY_QUEUE_MAX_RETRIES = max(
    0,
    int(str(os.getenv("AUTO_REPLY_QUEUE_MAX_RETRIES", "1")).strip() or "1"),
)
AUTO_REPLY_GROUP_AUTHOR_ISOLATION_ENABLED = str(
    os.getenv("AUTO_REPLY_GROUP_AUTHOR_ISOLATION_ENABLED", "1")
).strip().lower() in {"1", "true", "yes", "on"}
REACTION_LEARNING_ENABLED = str(os.getenv("REACTION_LEARNING_ENABLED", "1")).strip().lower() in {
    "1", "true", "yes", "on"
}
CHAT_MOOD_ENABLED = str(os.getenv("CHAT_MOOD_ENABLED", "1")).strip().lower() in {
    "1", "true", "yes", "on"
}
AUTO_REACTIONS_ENABLED = str(os.getenv("AUTO_REACTIONS_ENABLED", "1")).strip().lower() in {
    "1", "true", "yes", "on"
}
REACTION_LEARNING_WEIGHT = float(str(os.getenv("REACTION_LEARNING_WEIGHT", "0.35")).strip() or "0.35")
AUTO_REPLY_STREAM_EDIT_INTERVAL_SECONDS = float(
    str(os.getenv("AUTO_REPLY_STREAM_EDIT_INTERVAL_SECONDS", "1.2")).strip() or "1.2"
)
AUTO_REPLY_HISTORY_SYNC_TIMEOUT_SECONDS = float(
    str(os.getenv("AUTO_REPLY_HISTORY_SYNC_TIMEOUT_SECONDS", "8")).strip() or "8"
)
AUTO_REPLY_CONTINUE_ON_INCOMPLETE = str(
    os.getenv("AUTO_REPLY_CONTINUE_ON_INCOMPLETE", "1")
).strip().lower() in {"1", "true", "yes", "on"}
AUTO_REPLY_SELF_PRIVATE_ENABLED = str(
    os.getenv("AUTO_REPLY_SELF_PRIVATE_ENABLED", "1")
).strip().lower() in {"1", "true", "yes", "on"}
try:
    AUTO_REPLY_MAX_NUMBERED_LIST_ITEMS = max(
        0,
        int(str(os.getenv("AUTO_REPLY_MAX_NUMBERED_LIST_ITEMS", "0")).strip() or "0"),
    )
except Exception:
    AUTO_REPLY_MAX_NUMBERED_LIST_ITEMS = 0
AUTO_REPLY_MAX_RESPONSE_CHARS = _timeout_from_env("AUTO_REPLY_MAX_RESPONSE_CHARS", 1800)
AUTO_REPLY_MAX_RESPONSE_CHARS_PRIVATE = _timeout_from_env(
    "AUTO_REPLY_MAX_RESPONSE_CHARS_PRIVATE",
    2400,
)

AUTO_SUMMARY_ENABLED = str(os.getenv("AUTO_SUMMARY_ENABLED", "0")).strip().lower() in {
    "1", "true", "yes", "on"
}

# Защита от дублей update.
_LAST_BUSY_NOTICE_TS = {}
_RECENT_MESSAGE_MARKERS = {}
_RECENT_MESSAGE_TTL_SECONDS = 180
_FORWARD_BURST_CONTEXT_MAP: dict[str, str] = {}


@dataclass
class ChatQueuedTask:
    """Одна задача автоответа в очереди чата."""

    chat_id: int
    message_id: int
    received_at: float
    priority: int
    runner: Any
    attempt: int = 0


class ChatWorkQueue:
    """
    FIFO очередь задач по чатам.
    Один worker на чат, без потери входящих сообщений.
    """

    def __init__(self, max_per_chat: int = 50, max_retries: int = 1):
        self.max_per_chat = max(1, int(max_per_chat))
        self.max_retries = max(0, int(max_retries))
        self._queues: dict[int, deque[ChatQueuedTask]] = {}
        self._workers: dict[int, asyncio.Task] = {}
        self._active_task: dict[int, ChatQueuedTask] = {}
        self._processed = 0
        self._failed = 0
        self._retried = 0

    def set_max_per_chat(self, value: int) -> None:
        self.max_per_chat = max(1, int(value))

    def enqueue(self, task: ChatQueuedTask) -> tuple[bool, int]:
        queue = self._queues.setdefault(int(task.chat_id), deque())
        if len(queue) >= self.max_per_chat:
            return False, len(queue)
        queue.append(task)
        return True, len(queue)

    def ensure_worker(self, chat_id: int) -> None:
        worker = self._workers.get(int(chat_id))
        if worker and not worker.done():
            return
        self._workers[int(chat_id)] = asyncio.create_task(self._worker_loop(int(chat_id)))

    async def _worker_loop(self, chat_id: int) -> None:
        while True:
            queue = self._queues.get(chat_id)
            if not queue:
                self._workers.pop(chat_id, None)
                return
            task = queue.popleft()
            self._active_task[chat_id] = task
            should_stop = False
            try:
                logger.debug(
                    "queue: старт обработки задачи",
                    chat_id=chat_id,
                    message_id=task.message_id,
                    attempt=task.attempt,
                    queue_left_after_pop=len(queue),
                )
                await task.runner()
                self._processed += 1
                logger.debug(
                    "queue: задача обработана успешно",
                    chat_id=chat_id,
                    message_id=task.message_id,
                    processed=self._processed,
                )
            except Exception:
                if task.attempt < self.max_retries:
                    task.attempt += 1
                    queue.appendleft(task)
                    self._retried += 1
                    logger.warning(
                        "Повтор задачи в очереди после ошибки",
                        chat_id=chat_id,
                        message_id=task.message_id,
                        attempt=task.attempt,
                        max_retries=self.max_retries,
                    )
                else:
                    self._failed += 1
                    logger.exception(
                        "Ошибка обработки задачи в очереди",
                        chat_id=chat_id,
                        message_id=task.message_id,
                        attempt=task.attempt,
                    )
            finally:
                self._active_task.pop(chat_id, None)
                if not queue:
                    # Если очередь пуста — снимаем ее, чтобы не держать лишнее состояние.
                    self._queues.pop(chat_id, None)
                    self._workers.pop(chat_id, None)
                    should_stop = True
            if should_stop:
                return

    def get_stats(self) -> dict:
        queue_lengths = {str(chat_id): len(q) for chat_id, q in self._queues.items()}
        return {
            "processed": int(self._processed),
            "failed": int(self._failed),
            "retried": int(self._retried),
            "active_chats": len(self._workers),
            "queue_lengths": queue_lengths,
            "queued_total": int(sum(queue_lengths.values())),
            "max_per_chat": int(self.max_per_chat),
            "max_retries": int(self.max_retries),
        }


class AIRuntimeControl:
    """Runtime-контроллер политики AI-обработчика (очередь, guardrails, реакции)."""

    def __init__(self, queue_manager: ChatWorkQueue, reaction_engine: ReactionLearningEngine, router):
        self.queue_manager = queue_manager
        self.reaction_engine = reaction_engine
        self.router = router
        self.queue_enabled = AUTO_REPLY_QUEUE_ENABLED
        self.queue_notify_position_enabled = AUTO_REPLY_QUEUE_NOTIFY_POSITION
        self.forward_context_enabled = AUTO_REPLY_FORWARD_CONTEXT_ENABLED
        self.reaction_learning_enabled = REACTION_LEARNING_ENABLED
        self.chat_mood_enabled = CHAT_MOOD_ENABLED
        self.auto_reactions_enabled = AUTO_REACTIONS_ENABLED
        self.group_author_isolation_enabled = AUTO_REPLY_GROUP_AUTHOR_ISOLATION_ENABLED
        self.continue_on_incomplete_enabled = AUTO_REPLY_CONTINUE_ON_INCOMPLETE
        self.last_context_snapshot: dict[str, dict] = {}

    def set_queue_enabled(self, enabled: bool) -> None:
        self.queue_enabled = bool(enabled)

    def set_queue_notify_position_enabled(self, enabled: bool) -> None:
        self.queue_notify_position_enabled = bool(enabled)

    def set_queue_max(self, max_per_chat: int) -> None:
        self.queue_manager.set_max_per_chat(max_per_chat)

    def set_queue_max_retries(self, max_retries: int) -> None:
        self.queue_manager.max_retries = max(0, int(max_retries))

    def set_forward_context_enabled(self, enabled: bool) -> None:
        self.forward_context_enabled = bool(enabled)

    def set_group_author_isolation_enabled(self, enabled: bool) -> None:
        self.group_author_isolation_enabled = bool(enabled)

    def set_continue_on_incomplete_enabled(self, enabled: bool) -> None:
        self.continue_on_incomplete_enabled = bool(enabled)

    def set_reaction_learning_enabled(self, enabled: bool) -> None:
        self.reaction_learning_enabled = bool(enabled)
        self.reaction_engine.set_enabled(enabled)

    def set_auto_reactions_enabled(self, enabled: bool) -> None:
        self.auto_reactions_enabled = bool(enabled)
        self.reaction_engine.set_auto_reactions_enabled(enabled)

    def set_guardrail(self, name: str, value: float) -> bool:
        normalized = str(name).strip().lower()
        if normalized == "reasoning_max_chars":
            self.router.local_reasoning_max_chars = max(200, int(value))
            return True
        if normalized == "stream_total_timeout_seconds":
            self.router.local_stream_total_timeout_seconds = max(5.0, float(value))
            return True
        if normalized == "stream_sock_read_timeout_seconds":
            self.router.local_stream_sock_read_timeout_seconds = max(2.0, float(value))
            return True
        if normalized == "include_reasoning":
            self.router.local_include_reasoning = bool(int(value))
            return True
        return False

    def set_context_snapshot(self, chat_id: int, payload: dict) -> None:
        self.last_context_snapshot[str(chat_id)] = dict(payload)

    def get_context_snapshot(self, chat_id: int) -> dict:
        return dict(self.last_context_snapshot.get(str(chat_id), {}))

    def get_context_snapshots(self) -> dict:
        """Возвращает все накопленные snapshot-ы контекста по чатам."""
        return {str(chat_id): dict(payload) for chat_id, payload in self.last_context_snapshot.items()}

    def get_policy_snapshot(self) -> dict:
        return {
            "queue_enabled": bool(self.queue_enabled),
            "queue_notify_position_enabled": bool(self.queue_notify_position_enabled),
            "forward_context_enabled": bool(self.forward_context_enabled),
            "group_author_isolation_enabled": bool(self.group_author_isolation_enabled),
            "continue_on_incomplete_enabled": bool(self.continue_on_incomplete_enabled),
            "reaction_learning_enabled": bool(self.reaction_learning_enabled),
            "chat_mood_enabled": bool(self.chat_mood_enabled),
            "auto_reactions_enabled": bool(self.auto_reactions_enabled),
            "queue": self.queue_manager.get_stats(),
            "guardrails": {
                "local_include_reasoning": bool(self.router.local_include_reasoning),
                "local_reasoning_max_chars": int(self.router.local_reasoning_max_chars),
                "local_stream_total_timeout_seconds": float(self.router.local_stream_total_timeout_seconds),
                "local_stream_sock_read_timeout_seconds": float(self.router.local_stream_sock_read_timeout_seconds),
            },
        }


def _is_duplicate_message(chat_id: int, message_id: int) -> bool:
    """
    Возвращает True, если это уже обработанный update.
    Нужен для редких дублей апдейтов после reconnect.
    """
    now = time.time()
    # Периодическая очистка старых маркеров.
    stale_keys = [k for k, ts in _RECENT_MESSAGE_MARKERS.items() if (now - ts) > _RECENT_MESSAGE_TTL_SECONDS]
    for key in stale_keys:
        _RECENT_MESSAGE_MARKERS.pop(key, None)

    marker = f"{chat_id}:{message_id}"
    if marker in _RECENT_MESSAGE_MARKERS:
        return True
    _RECENT_MESSAGE_MARKERS[marker] = now
    return False


def _append_forward_to_burst_state(state: dict, message: Message, max_items: int) -> bool:
    """
    Добавляет пересланное сообщение в burst-буфер без дублей.

    Возвращает:
    - True: сообщение добавлено;
    - False: это дубликат (тот же chat_id + message_id), буфер не менялся.
    """
    messages = list(state.get("messages") or [])
    chat_id = int(getattr(getattr(message, "chat", None), "id", 0) or 0)
    message_id = int(getattr(message, "id", 0) or 0)

    if chat_id and message_id:
        for item in messages:
            if int(getattr(getattr(item, "chat", None), "id", 0) or 0) != chat_id:
                continue
            if int(getattr(item, "id", 0) or 0) == message_id:
                return False

    messages.append(message)
    limit = int(max(2, max_items * 2))
    if len(messages) > limit:
        messages = messages[-limit:]
    state["messages"] = messages
    return True


def _is_self_private_message(message: Message) -> bool:
    """
    Определяет, что сообщение отправлено из этого же аккаунта
    в приватный «чат с собой» (Saved Messages / self-dialog).
    """
    if not getattr(message, "from_user", None):
        return False
    if not bool(getattr(message.from_user, "is_self", False)):
        return False
    if getattr(message.chat, "type", None) != enums.ChatType.PRIVATE:
        return False
    return int(getattr(message.chat, "id", 0) or 0) == int(getattr(message.from_user, "id", 0) or 0)


def _sanitize_model_output(text: str, router=None) -> str:
    """Удаляет служебные маркеры модели перед отправкой в Telegram."""
    if hasattr(router, "_sanitize_model_text"):
        try:
            candidate = router._sanitize_model_text(text)
            # Защита от моков/нестандартных реализаций:
            # если sanitize вернул не строку, откатываемся к локальной очистке.
            if isinstance(candidate, str):
                return candidate
        except Exception:
            pass
    if not text:
        return ""
    
    import re
    cleaned = str(text)
    # Удаляем всё в формате <|...|>
    cleaned = re.sub(r"<\|.*?\|>", "", cleaned)
    # Удаляем классические токены
    for token in ("</s>", "<s>", "<br>"):
        cleaned = cleaned.replace(token, "")
    return cleaned.strip()


def _normalize_runtime_error_message_for_user(text: str, router=None) -> tuple[str, bool]:
    """
    Нормализует «сырой» runtime-error модели в человекочитаемый ответ.

    Зачем:
    Даже если в пайплайн просочился технический ответ (например, `Connection error.`),
    пользователь должен получить понятный fallback, а не внутренний текст шлюза.
    """
    raw = str(text or "").strip()
    if not raw:
        return "", False

    is_runtime_error = False
    detector = getattr(router, "_is_runtime_error_message", None)
    if callable(detector):
        try:
            is_runtime_error = bool(detector(raw))
        except Exception:
            is_runtime_error = False

    lowered = raw.lower()
    if not is_runtime_error:
        fallback_markers = (
            "connection error",
            "network error",
            "failed to connect",
            "connection refused",
            "timeout",
            "timed out",
            "gateway timeout",
            "upstream",
            "provider unavailable",
            "no models loaded",
            "please load a model",
            "not_found",
            "not found",
            "quota exceeded",
            "billing",
            "out of credits",
        )
        is_runtime_error = any(marker in lowered for marker in fallback_markers)

    if not is_runtime_error:
        return raw, False

    detail = "временный сбой канала AI"
    if "no models loaded" in lowered or "please load a model" in lowered:
        detail = "локальная модель не загружена"
    elif "quota" in lowered or "billing" in lowered or "out of credits" in lowered:
        detail = "лимит облачного провайдера исчерпан"
    elif "not_found" in lowered or "not found" in lowered:
        detail = "запрошенная модель недоступна у провайдера"
    elif "timeout" in lowered or "timed out" in lowered:
        detail = "превышено время ожидания ответа"
    elif (
        "connection error" in lowered
        or "network error" in lowered
        or "failed to connect" in lowered
        or "connection refused" in lowered
        or "upstream" in lowered
    ):
        detail = "ошибка соединения с AI-шлюзом"

    user_text = f"⚠️ Временная ошибка AI: {detail}. Повтори запрос через 3-5 секунд."
    return user_text, True


def _is_explicit_non_russian_request(text: str) -> bool:
    """
    Определяет, просил ли пользователь явно отвечать не на русском.
    Нужен, чтобы не форсировать русский там, где пользователь хочет другой язык.
    """
    payload = str(text or "").strip().lower()
    if not payload:
        return False
    markers = (
        "на англий",
        "по-англий",
        "in english",
        "answer in english",
        "speak english",
        "write in english",
        "на испан",
        "на француз",
        "на немец",
        "на итальян",
        "на португал",
        "на турец",
        "на китай",
        "на япон",
        "на корей",
    )
    return any(marker in payload for marker in markers)


def _should_force_russian_reply(
    user_text: str,
    is_private: bool,
    is_owner_sender: bool,
    is_voice_response_needed: bool,
) -> bool:
    """
    Решает, включать ли строгий русский guardrail для генерации ответа.
    """
    if _is_explicit_non_russian_request(user_text):
        return False
    if is_voice_response_needed:
        return True
    if is_owner_sender:
        return True
    return bool(is_private)


def _build_reply_context(message: Message) -> str:
    """Формирует reply-контекст для prompt, если сообщение является ответом."""
    if not getattr(message, "reply_to_message", None):
        return ""
    reply_author = "Unknown"
    reply_from = getattr(message.reply_to_message, "from_user", None)
    if reply_from:
        reply_author = (
            f"@{reply_from.username}"
            if getattr(reply_from, "username", None)
            else (getattr(reply_from, "first_name", None) or "User")
        )
    reply_text = _message_content_hint(message.reply_to_message)
    if not reply_text:
        return ""
    return f"[REPLY CONTEXT from {reply_author}]: {reply_text}"


def _build_forward_context(message: Message, enabled: bool = True) -> str:
    """Формирует контекст форварда, чтобы модель не считала это позицией владельца."""
    if not enabled:
        return ""

    chat_id = int(getattr(getattr(message, "chat", None), "id", 0) or 0)
    message_id = int(getattr(message, "id", 0) or 0)
    burst_key = f"{chat_id}:{message_id}"
    burst_context = _FORWARD_BURST_CONTEXT_MAP.pop(burst_key, "")

    forwarded_from = None
    if getattr(message, "forward_from", None):
        fwd_user = message.forward_from
        forwarded_from = (
            f"@{fwd_user.username}"
            if getattr(fwd_user, "username", None)
            else (getattr(fwd_user, "first_name", None) or "unknown_user")
        )
    if not forwarded_from and getattr(message, "forward_sender_name", None):
        forwarded_from = str(message.forward_sender_name)
    if not forwarded_from and getattr(message, "forward_from_chat", None):
        fwd_chat = message.forward_from_chat
        forwarded_from = (
            getattr(fwd_chat, "title", None)
            or getattr(fwd_chat, "username", None)
            or "unknown_chat"
        )
    if not forwarded_from:
        return burst_context

    fwd_date = getattr(message, "forward_date", None)
    fwd_date_text = str(fwd_date) if fwd_date else "n/a"
    auto_fwd = bool(getattr(message, "is_automatic_forward", False))
    base_context = (
        "[FORWARDED CONTEXT]: это пересланный материал для анализа, "
        "не интерпретируй его как позицию владельца.\n"
        f"Источник: {forwarded_from}\n"
        f"Дата форварда: {fwd_date_text}\n"
        f"Автофорвард: {auto_fwd}"
    )
    if burst_context:
        return f"{base_context}\n\n{burst_context}"
    return base_context


def _is_forwarded_message(message: Message) -> bool:
    """
    Проверяет, что сообщение является пересланным (любой тип форварда).
    """
    return bool(
        getattr(message, "forward_date", None)
        or getattr(message, "forward_from", None)
        or getattr(message, "forward_sender_name", None)
        or getattr(message, "forward_from_chat", None)
        or getattr(message, "forward_from_message_id", None)
    )


def _compose_forward_burst_context(messages: list[Message], max_items: int = 8) -> str:
    """
    Формирует контекст «пачки форвардов», чтобы модель видела связность
    и не отвечала на каждое пересланное сообщение отдельно.
    """
    if not messages:
        return ""
    safe_max = max(1, int(max_items))
    tail = messages[-safe_max:]
    lines: list[str] = []
    for idx, msg in enumerate(tail, start=1):
        source = "unknown_source"
        if getattr(msg, "forward_from", None):
            fwd_user = msg.forward_from
            source = (
                f"@{fwd_user.username}"
                if getattr(fwd_user, "username", None)
                else (getattr(fwd_user, "first_name", None) or "unknown_user")
            )
        elif getattr(msg, "forward_sender_name", None):
            source = str(msg.forward_sender_name)
        elif getattr(msg, "forward_from_chat", None):
            fwd_chat = msg.forward_from_chat
            source = (
                getattr(fwd_chat, "title", None)
                or getattr(fwd_chat, "username", None)
                or "unknown_chat"
            )
        payload = _message_content_hint(msg)
        if not payload:
            payload = "[empty]"
        lines.append(f"{idx}. [{source}] {payload}")
    return (
        "[FORWARDED BATCH CONTEXT]: это часть одной пачки пересланных сообщений. "
        "Проанализируй их как единый контекст:\n"
        + "\n".join(lines)
    )


def _build_author_context(message: Message, is_owner_sender: bool) -> str:
    """
    Формирует контекст авторства текущего запроса для групповых чатов.
    Помогает модели не путать владельца с другими участниками.
    """
    user = getattr(message, "from_user", None)
    username = ""
    display_name = "unknown_user"
    user_id = 0
    if user:
        username = str(getattr(user, "username", "") or "").strip()
        display_name = str(getattr(user, "first_name", "") or "").strip() or "unknown_user"
        user_id = int(getattr(user, "id", 0) or 0)

    author = f"@{username}" if username else display_name
    raw_chat_type = getattr(getattr(message, "chat", None), "type", "unknown")
    if hasattr(raw_chat_type, "name"):
        chat_type = str(getattr(raw_chat_type, "name", "unknown")).lower()
    else:
        chat_type = str(raw_chat_type).lower()

    owner_marker = "owner" if is_owner_sender else "participant"
    return (
        "[AUTHOR CONTEXT]:\n"
        f"author={author}\n"
        f"author_id={user_id}\n"
        f"author_role={owner_marker}\n"
        f"chat_type={chat_type}\n"
        f"Целевой получатель ответа: {author} (author_id={user_id}).\n"
        "Отвечай только текущему author. Не подменяй автора владельцем, если author_role=participant.\n"
        "Если в [REPLY CONTEXT] цитируется другой человек, это материал для анализа, а не новый author.\n"
        "Блоки [REPLY CONTEXT] и [FORWARDED CONTEXT] являются цитатой/материалом для анализа, "
        "а не намерением текущего author."
    )


def _build_user_memory_payload(
    message: Message,
    sender: str,
    text: str,
    is_owner_sender: bool,
) -> dict:
    """
    Формирует единый payload для записи user-сообщения в память.
    Нужен, чтобы и отвеченные, и неотвеченные сообщения имели одинаковые поля автора.
    """
    chat_type_value = str(getattr(getattr(message.chat, "type", None), "name", message.chat.type)).lower()
    user_id = int(getattr(getattr(message, "from_user", None), "id", 0) or 0)
    return {
        "role": "user",
        "user": sender,
        "text": str(text or ""),
        "author_id": user_id,
        "author_username": str(sender or ""),
        "author_role": "owner" if is_owner_sender else "participant",
        "chat_type": chat_type_value,
    }


def _is_voice_reply_requested(text: str) -> bool:
    """Определяет, просит ли пользователь голосовой ответ текстом."""
    lowered = (text or "").strip().lower()
    if not lowered:
        return False
    triggers = (
        "ответь голосом",
        "голосом ответь",
        "скажи голосом",
        "озвучь ответ",
        "запиши голосовое",
        "пришли голосовое",
        "голосовое сообщение",
        "voice reply",
        "reply by voice",
        "respond with voice",
        "voice message",
        "send voice",
    )
    if any(token in lowered for token in triggers):
        return True

    # Ловим естественные формулировки вроде:
    # "Отвечай, пожалуйста, голосом" / "Ответь мне голосом".
    russian_patterns = (
        r"\bотвеч(?:ай|айте|ай-ка|айте-ка)\b.{0,40}\bголос(?:ом|овой|овое)?\b",
        r"\bответ(?:ь|ьте)\b.{0,40}\bголос(?:ом|овой|овое)?\b",
        r"\bголос(?:ом|овой|овое)?\b.{0,40}\bотвеч(?:ай|айте|ай-ка|айте-ка|у|ать)\b",
    )
    return any(re.search(pattern, lowered) for pattern in russian_patterns)


def _extract_code_prompt_flags(message_text: str) -> tuple[str, bool, bool]:
    """
    Разбирает !code команду и выделяет:
    - prompt
    - confirm_expensive
    - raw_code_mode (если включен --raw-code)
    """
    raw = message_text or ""
    try:
        argv = shlex.split(raw)
    except ValueError:
        argv = raw.split()

    if len(argv) < 2:
        return "", False, False

    confirm_expensive = False
    raw_code_mode = False
    payload_tokens: list[str] = []
    for token in argv[1:]:
        normalized = token.strip().lower()
        if normalized in {"--confirm-expensive", "--confirm", "confirm"}:
            confirm_expensive = True
            continue
        if normalized in {"--raw-code", "--raw"}:
            raw_code_mode = True
            continue
        payload_tokens.append(token)

    prompt = " ".join(payload_tokens).strip()
    return prompt, confirm_expensive, raw_code_mode


def _is_critical_code_request(prompt: str) -> bool:
    """
    Эвристика для критичных coding-задач:
    деплой, прод, безопасность, платежи, миграции и т.п.
    """
    text = (prompt or "").lower()
    markers = (
        "prod", "production", "деплой", "релиз", "security", "безопас",
        "auth", "oauth", "jwt", "billing", "платеж", "migration", "миграц",
        "db", "database", "postgres", "rollback", "infra", "k8s", "kubernetes",
    )
    return any(marker in text for marker in markers)


def _build_safe_code_prompt(prompt: str, strict_mode: bool = False) -> str:
    """
    Формирует структурированный запрос для code-режима:
    1) План
    2) Код
    3) Тесты
    4) Риски
    """
    strict_clause = (
        "Режим strict: учитывай production-ограничения, валидацию входных данных, "
        "ошибки/rollback и безопасные значения по умолчанию.\n"
        if strict_mode else ""
    )
    return (
        "Ты senior-инженер. Отвечай строго на русском языке.\n"
        f"{strict_clause}"
        "Верни результат в формате:\n"
        "1) PLAN — краткий пошаговый план (3-7 пунктов)\n"
        "2) CODE — готовый код (fenced block)\n"
        "3) TESTS — минимально достаточные тесты/проверки\n"
        "4) RISKS — короткий список рисков и ограничений\n\n"
        f"Задача пользователя:\n{prompt}"
    )


def _to_plain_stream_text(text: str) -> str:
    """
    Минимальная очистка markdown для безопасного plain-text edit.
    Нужна как fallback, если Telegram отвергает markdown-парсинг в стриме.
    """
    if not text:
        return "..."

    plain = str(text)
    replacements = {
        "```": "",
        "`": "",
        "**": "",
        "__": "",
        "~~": "",
    }
    for source, target in replacements.items():
        plain = plain.replace(source, target)
    return plain.strip() or "..."


def _prepare_tts_text(text: str) -> str:
    """
    Подготавливает текст для озвучки:
    - убирает служебные/статусные строки и markdown-мусор,
    - режет мета-вставки в квадратных скобках,
    - оставляет компактный, «человеческий» текст.
    """
    if not text:
        return ""

    cleaned = _to_plain_stream_text(text)
    cleaned = re.sub(r"\[[^\]]{1,160}\]", "", cleaned)

    skip_prefixes = (
        "связь установлена",
        "я готов к работе",
        "voice reply",
        "llm error",
        "ошибка",
        "status:",
        "system:",
    )
    lines = []
    for raw_line in cleaned.splitlines():
        line = raw_line.strip(" \t-•*")
        if not line:
            continue
        low = line.lower()
        if any(low.startswith(prefix) for prefix in skip_prefixes):
            continue
        lines.append(line)

    compact = "\n".join(lines).strip()
    compact = re.sub(r"\n{3,}", "\n\n", compact)
    return compact


def _split_tts_chunks(text: str, max_chars: int = 1100, max_chunks: int = 6) -> list[str]:
    """
    Делит длинный TTS-текст на безопасные части без потери содержания.

    Почему так:
    - edge-tts и Telegram стабильнее работают на умеренной длине входа;
    - раньше текст обрезался до ~1750 символов и хвост терялся.
    Теперь отправляем несколько voice-частей подряд.
    """
    payload = str(text or "").strip()
    if not payload:
        return []

    safe_max_chars = max(300, int(max_chars))
    safe_max_chunks = max(1, int(max_chunks))

    paragraphs = [p.strip() for p in re.split(r"\n{2,}", payload) if p.strip()]
    if not paragraphs:
        paragraphs = [payload]

    chunks: list[str] = []
    current = ""

    def _flush() -> None:
        nonlocal current
        if current.strip():
            chunks.append(current.strip())
            current = ""

    for paragraph in paragraphs:
        candidate = f"{current}\n\n{paragraph}".strip() if current else paragraph
        if len(candidate) <= safe_max_chars:
            current = candidate
            continue

        _flush()
        if len(paragraph) <= safe_max_chars:
            current = paragraph
            continue

        # Очень длинный абзац режем по предложениям/словам.
        sentences = [s.strip() for s in re.split(r"(?<=[.!?…])\s+", paragraph) if s.strip()]
        if not sentences:
            sentences = [paragraph]

        for sentence in sentences:
            candidate_sentence = f"{current} {sentence}".strip() if current else sentence
            if len(candidate_sentence) <= safe_max_chars:
                current = candidate_sentence
                continue

            _flush()
            if len(sentence) <= safe_max_chars:
                current = sentence
                continue

            # Fallback: рубим по словам, если даже одно "предложение" слишком длинное.
            words = sentence.split()
            for word in words:
                candidate_word = f"{current} {word}".strip() if current else word
                if len(candidate_word) <= safe_max_chars:
                    current = candidate_word
                else:
                    _flush()
                    current = word
            _flush()

    _flush()

    if len(chunks) <= safe_max_chunks:
        return chunks

    # Схлопываем хвост в последнюю часть, чтобы не спамить десятками voice-сообщений.
    head = chunks[: safe_max_chunks - 1]
    tail = " ".join(chunks[safe_max_chunks - 1 :]).strip()
    if tail:
        head.append(tail)
    return [part for part in head if part]


def _collapse_repeated_paragraphs(text: str, max_consecutive_repeats: int = 2) -> tuple[str, bool]:
    """
    Схлопывает подряд идущие одинаковые абзацы.
    Возвращает: (очищенный_текст, были_ли_удаления).
    """
    if not text:
        return "", False
    paragraphs = re.split(r"\n{2,}", str(text))
    output: list[str] = []
    removed = False
    last_norm = ""
    last_count = 0

    for paragraph in paragraphs:
        cleaned = paragraph.strip()
        if not cleaned:
            continue
        normalized = re.sub(r"\s+", " ", cleaned).strip().lower()
        normalized = re.sub(r"[\"'`*_~]+", "", normalized)
        normalized = re.sub(r"[^\w\s]+", " ", normalized, flags=re.UNICODE)
        normalized = re.sub(r"\s+", " ", normalized).strip()
        if normalized == last_norm:
            last_count += 1
        else:
            last_norm = normalized
            last_count = 1
        if last_count <= max_consecutive_repeats:
            output.append(cleaned)
        else:
            removed = True
    return ("\n\n".join(output).strip(), removed)


def _collapse_repeated_lines(text: str, max_consecutive_repeats: int = 2) -> tuple[str, bool]:
    """
    Схлопывает подряд идущие одинаковые строки (с мягкой нормализацией),
    чтобы гасить зацикливание в обычном абзацном тексте без пустых строк.
    """
    if not text:
        return "", False

    lines = str(text).splitlines()
    output: list[str] = []
    removed = False
    last_norm = ""
    last_count = 0

    for raw in lines:
        line = str(raw or "")
        if not line.strip():
            output.append(line)
            last_norm = ""
            last_count = 0
            continue
        normalized = line.strip().lower()
        normalized = re.sub(r"[\"'`*_~]+", "", normalized)
        normalized = re.sub(r"[^\w\s]+", " ", normalized, flags=re.UNICODE)
        normalized = re.sub(r"\s+", " ", normalized)
        if normalized == last_norm:
            last_count += 1
        else:
            last_norm = normalized
            last_count = 1
        if last_count <= max_consecutive_repeats:
            output.append(line)
        else:
            removed = True

    payload = "\n".join(output).strip()
    payload = re.sub(r"\n{3,}", "\n\n", payload)
    return payload, removed


def _dedupe_repeated_long_paragraphs(
    text: str,
    min_normalized_len: int = 140,
    max_occurrences: int = 1,
) -> tuple[str, bool]:
    """
    Убирает «склеенные» длинные дубли абзацев даже если они не подряд.
    Нужен для кейсов, когда модель повторяет большой блок через 1-2 вставки.
    """
    if not text:
        return "", False

    paragraphs = [p.strip() for p in re.split(r"\n{2,}", str(text)) if p.strip()]
    if not paragraphs:
        return str(text).strip(), False

    seen_counts: dict[str, int] = {}
    output: list[str] = []
    removed = False
    safe_max_occurrences = max(1, int(max_occurrences))

    for paragraph in paragraphs:
        normalized = re.sub(r"\s+", " ", paragraph).strip().lower()
        normalized = re.sub(r"[\"'`*_~]+", "", normalized)
        normalized = re.sub(r"[^\w\s]+", " ", normalized, flags=re.UNICODE)
        normalized = re.sub(r"\s+", " ", normalized).strip()
        if len(normalized) < max(20, int(min_normalized_len)):
            output.append(paragraph)
            continue

        count = seen_counts.get(normalized, 0) + 1
        seen_counts[normalized] = count
        if count <= safe_max_occurrences:
            output.append(paragraph)
        else:
            removed = True

    return "\n\n".join(output).strip(), removed


def _build_vision_route_fact_line(meta: dict) -> str:
    """
    Формирует человекочитаемый факт о реальном маршруте vision.
    Эта строка добавляется кодом (не моделью), чтобы исключить «выкручивание».
    """
    if not isinstance(meta, dict):
        return ""

    route = str(meta.get("route") or "").strip().lower()
    model = str(meta.get("model") or "").strip()
    fallback_used = bool(meta.get("fallback_used"))
    error = str(meta.get("error") or "").strip()

    if route == "local_lm_studio":
        return f"ℹ️ Факт vision: локально через LM Studio (`{model or '-'}`)."
    if route == "cloud_gemini":
        if fallback_used:
            return (
                f"ℹ️ Факт vision: cloud через Gemini (`{model or '-'}`), "
                "после неуспешной попытки локального vision."
            )
        return f"ℹ️ Факт vision: cloud через Gemini (`{model or '-'}`)."
    if route == "error":
        return f"ℹ️ Факт vision: ошибка vision-контура (`{error or 'unknown_error'}`)."
    return ""


def _enforce_vision_route_consistency(text: str, vision_meta: dict) -> tuple[str, bool]:
    """
    Гарантирует, что итоговый ответ не противоречит фактическому маршруту vision.
    Если маршрут cloud, удаляем ложные блоки "полностью локально" и добавляем корректировку.
    """
    payload = str(text or "").strip()
    if not payload:
        return "", False
    if not isinstance(vision_meta, dict):
        return payload, False

    route = str(vision_meta.get("route") or "").strip().lower()
    if route != "cloud_gemini":
        return payload, False

    contradiction_patterns = (
        r"полностью\s+перешли\s+к\s+локальн",
        r"все\s+операции.*локальн",
        r"всегда\s+локальн",
        r"полностью\s+отказались\s+от\s+использования\s+внешних\s+облачных",
        r"никакие.*не\s+переда[ею]т[сc]я?\s+в\s+интернет",
    )

    paragraphs = [p.strip() for p in re.split(r"\n{2,}", payload) if p.strip()]
    if not paragraphs:
        return payload, False

    kept: list[str] = []
    removed = False
    for paragraph in paragraphs:
        normalized = re.sub(r"\s+", " ", paragraph).strip().lower()
        if any(re.search(pattern, normalized) for pattern in contradiction_patterns):
            removed = True
            continue
        kept.append(paragraph)

    if not kept:
        kept = [payload]
    cleaned = "\n\n".join(kept).strip()

    model = str(vision_meta.get("model") or "-").strip()
    correction = (
        f"⚠️ Коррекция факта: этот vision-запрос выполнен через cloud (`{model}`), "
        "поэтому утверждения про «полностью локально» для этого ответа неприменимы."
    )
    if correction not in cleaned:
        cleaned = f"{correction}\n\n{cleaned}".strip()
    return cleaned, removed


def _cap_numbered_list_items(text: str, max_items: int = 20) -> tuple[str, bool]:
    """
    Ограничивает слишком длинные нумерованные списки.
    Возвращает: (очищенный_текст, был_ли_trim).
    """
    if not text or max_items <= 0:
        return text or "", False

    lines = str(text).splitlines()
    result: list[str] = []
    numbered_count = 0
    trimmed = False
    pattern = re.compile(r"^\s*\d+[\.\)]\s+")

    for line in lines:
        if pattern.match(line):
            numbered_count += 1
            if numbered_count > max_items:
                trimmed = True
                continue
        result.append(line)

    if trimmed:
        result.append("")
        result.append(
            f"⚠️ Список был ограничен до {max_items} пунктов, чтобы избежать зацикливания ответа."
        )
    return ("\n".join(result).strip(), trimmed)


def _prune_repetitive_numbered_items(
    text: str,
    max_same_body: int = 2,
) -> tuple[str, bool]:
    """
    Убирает дублирующиеся пункты нумерованного списка с одинаковым текстом пункта.

    Пример:
    - "31. Используйте любую возможность для эвакуации"
    - "36. Используйте любую возможность для эвакуации"

    Второй и последующие дубли удаляются.
    """
    if not text:
        return "", False

    lines = str(text).splitlines()
    numbered_pattern = re.compile(r"^\s*(\d+[\.\)])\s+(.*\S)\s*$")
    body_seen: dict[str, int] = {}
    cleaned_lines: list[str] = []
    removed = False

    for line in lines:
        match = numbered_pattern.match(line)
        if not match:
            cleaned_lines.append(line)
            continue

        body = match.group(2).strip().lower()
        body = re.sub(r"\s+", " ", body)
        body = re.sub(r"[\"'`*_~]+", "", body)
        count = int(body_seen.get(body, 0)) + 1
        body_seen[body] = count
        if count > max_same_body:
            removed = True
            continue
        cleaned_lines.append(line)

    return ("\n".join(cleaned_lines).strip(), removed)


def _drop_service_busy_phrases(text: str) -> tuple[str, bool]:
    """
    Удаляет из ответа служебные «очередные» и технические фразы,
    которые не должны попадать пользователю.
    """
    if not text:
        return "", False

    blocked_patterns = (
        "обрабатываю предыдущий запрос",
        "отправь следующее сообщение через пару секунд",
        "подожди пару секунд и повтори",
        "добавил в очередь обработки",
        "позиция:",
    )

    output_lines: list[str] = []
    removed = False
    for line in str(text).splitlines():
        low = line.strip().lower()
        if any(pattern in low for pattern in blocked_patterns):
            removed = True
            continue
        output_lines.append(line)

    normalized = "\n".join(output_lines).strip()
    normalized = re.sub(r"\n{3,}", "\n\n", normalized)
    return normalized, removed


def _drop_tool_artifact_blocks(text: str) -> tuple[str, bool]:
    """
    Удаляет утечки внутреннего tool/scaffold вывода модели:
    - begin/end_of_box, NO_REPLY, HEARTBEAT_OK
    - JSON-схемы sessions_send/action/parameters
    - дампы AGENTS.md и повторяющиеся блоки Default Channel.
    """
    payload = str(text or "").strip()
    if not payload:
        return "", False

    original = payload
    # Удаляем маркеры box, сохраняя потенциально полезный текст.
    payload = payload.replace("<|begin_of_box|>", "")
    payload = payload.replace("<|end_of_box|>", "")

    blocked_fragments = (
        "begin_of_box",
        "end_of_box",
        "no_reply",
        "heartbeat_ok",
        "i will now call the",
        "memory_get",
        "memory_search",
        "sessions_spawn",
        "sessions_send",
        "\"action\": \"sessions_send\"",
        "\"action\":\"sessions_send\"",
        "\"sessionkey\"",
        "\"default channel",
        "default channel id",
        "## /users/",
        "# agents.md - workspace agents",
        "## agent list",
        "### default agents",
        "</tool_call>",
    )
    noisy_line_patterns = (
        r"^\s*\"?(name|description|icon|color|sound|volume|timeout|type|id)\"?\s*:\s*\"?whatsapp\"?",
        r"^\s*-\s*\"default channel",
    )

    filtered_lines: list[str] = []
    removed = False
    for line in payload.splitlines():
        low = line.strip().lower()
        if low in {"```", "```json", "```text", "```yaml"}:
            removed = True
            continue
        if any(fragment in low for fragment in blocked_fragments):
            removed = True
            continue
        if any(re.search(pattern, low) for pattern in noisy_line_patterns):
            removed = True
            continue
        filtered_lines.append(line)

    payload = "\n".join(filtered_lines)
    payload = re.sub(r"\n{3,}", "\n\n", payload).strip()
    if not payload:
        return "", original != ""
    return payload, (removed or payload != original)


def _looks_like_internal_dump(text: str) -> bool:
    """
    Эвристика для распознавания «протекшего» внутреннего вывода:
    schema-дампы, системные markdown-блоки, тех. теги и однотипные JSON-строки.
    """
    payload = str(text or "").strip()
    if not payload:
        return False

    low = payload.lower()
    suspicious_markers = (
        "begin_of_box",
        "end_of_box",
        "sessions_send",
        "sessionkey",
        "default channel",
        "agents.md",
        "## agent list",
        "no_reply",
        "heartbeat_ok",
    )
    marker_hits = sum(1 for marker in suspicious_markers if marker in low)

    jsonish_line_count = 0
    for line in payload.splitlines():
        stripped = line.strip()
        if not stripped:
            continue
        if re.match(r'^\s*"?[A-Za-z_][A-Za-z0-9_ ]*"?\s*:\s*', stripped):
            jsonish_line_count += 1

    if marker_hits >= 2:
        return True
    if marker_hits >= 1 and jsonish_line_count >= 10:
        return True
    if jsonish_line_count >= 24:
        return True
    return False


def _clamp_auto_reply_text(text: str, *, is_private: bool) -> tuple[str, bool]:
    """
    Ограничивает длину ответа для каналов автоответа, чтобы избежать
    огромных «простыней» и зацикленного вывода.
    """
    payload = str(text or "").strip()
    if not payload:
        return "", False

    max_chars = (
        AUTO_REPLY_MAX_RESPONSE_CHARS_PRIVATE
        if is_private
        else AUTO_REPLY_MAX_RESPONSE_CHARS
    )
    if len(payload) <= max_chars:
        return payload, False
    trimmed = payload[: max(200, max_chars)].rstrip()
    return f"{trimmed}\n\n…(ответ сокращен автоматически)", True


def _drop_english_scaffold_when_russian_expected(
    text: str,
    prefer_russian: bool,
    min_paragraph_len: int = 180,
) -> tuple[str, bool]:
    """
    Удаляет длинные англоязычные scaffold-блоки, если ответ должен быть на русском.
    Сценарий: модель выдала английский черновик + русский ответ в одном сообщении.
    """
    payload = str(text or "").strip()
    if not payload or not prefer_russian:
        return payload, False

    paragraphs = [p.strip() for p in re.split(r"\n{2,}", payload) if p.strip()]
    if not paragraphs:
        return payload, False

    def _letters_stats(value: str) -> tuple[int, int]:
        latin = len(re.findall(r"[A-Za-z]", value))
        cyr = len(re.findall(r"[А-Яа-яЁё]", value))
        return latin, cyr

    has_substantial_russian = any(
        len(p) >= 80 and _letters_stats(p)[1] >= 20 for p in paragraphs
    )
    if not has_substantial_russian:
        return payload, False

    removed = False
    kept: list[str] = []
    for paragraph in paragraphs:
        latin, cyr = _letters_stats(paragraph)
        is_long = len(paragraph) >= max(80, int(min_paragraph_len))
        latin_ratio = float(latin) / float(max(1, latin + cyr))
        if is_long and latin >= 80 and cyr <= 18 and latin_ratio >= 0.78:
            removed = True
            continue
        kept.append(paragraph)

    if not removed or not kept:
        return payload, False
    return "\n\n".join(kept).strip(), True


def _is_service_busy_artifact_text(text: str) -> bool:
    """
    Определяет, что строка является служебным артефактом очереди/ожидания,
    который не должен попадать обратно в модельный контекст.
    """
    payload = str(text or "").strip().lower()
    if not payload:
        return False
    markers = (
        "обрабатываю предыдущий запрос",
        "отправь следующее сообщение через пару секунд",
        "подожди пару секунд и повтори",
        "добавил в очередь обработки",
    )
    return any(marker in payload for marker in markers)


def _drop_service_busy_context_items(context: list) -> tuple[list, int]:
    """
    Удаляет из контекста элементы с техфразами очереди (любой роли),
    чтобы модель не копировала устаревшие служебные ответы.
    """
    cleaned: list = []
    dropped = 0
    for item in context or []:
        text = str((item or {}).get("text", "") or "")
        if _is_service_busy_artifact_text(text):
            dropped += 1
            continue
        cleaned.append(item)
    return cleaned, dropped


def _build_stream_preview(text: str, max_chars: int = 3600) -> str:
    """
    Готовит безопасный фрагмент для live-обновления в Telegram.
    Показываем хвост длинного ответа, чтобы не упираться в лимит edit_text.
    """
    if not text:
        return "..."
    payload = str(text)
    if len(payload) <= max_chars:
        return payload
    tail = payload[-max_chars:]
    return f"…\n{tail}"


def _split_text_chunks_for_telegram(text: str, max_len: int = 3900) -> list[str]:
    """
    Делит длинный текст на безопасные куски для Telegram.
    Предпочитает границы абзацев/строк, чтобы не рубить мысль посередине.
    """
    payload = str(text or "")
    limit = int(max(500, max_len))
    if len(payload) <= limit:
        return [payload]

    chunks: list[str] = []
    rest = payload
    while len(rest) > limit:
        window = rest[:limit]
        cut = window.rfind("\n\n")
        if cut < int(limit * 0.45):
            cut = window.rfind("\n")
        if cut < int(limit * 0.35):
            cut = window.rfind(". ")
        if cut < int(limit * 0.25):
            cut = limit
        part = rest[:cut].rstrip()
        if not part:
            part = rest[:limit]
            cut = len(part)
        chunks.append(part)
        rest = rest[cut:].lstrip()
    if rest:
        chunks.append(rest)
    return chunks


def _should_emit_stream_edit(previous_preview: str, next_preview: str, min_delta_chars: int = 60) -> bool:
    """
    Решает, стоит ли отправлять очередной edit_text во время стриминга.
    Уменьшает «шум» и риск FloodWait на длинных генерациях.
    """
    prev = str(previous_preview or "")
    nxt = str(next_preview or "")
    if not nxt:
        return False
    if not prev:
        return True
    if prev == nxt:
        return False
    return abs(len(nxt) - len(prev)) >= int(max(1, min_delta_chars))


def _looks_incomplete_response(text: str) -> bool:
    """
    Эвристика «обрезанного» ответа: модель обещала структуру/продолжение,
    но завершилась на вводной фразе или на явном незавершённом конце.
    """
    payload = str(text or "").strip()
    if not payload:
        return False

    low = payload.lower()
    if len(payload) < 60:
        return False

    numbered_items = re.search(r"(?m)^\s*\d+[\.\)]\s+", payload) is not None
    promised_plan = any(
        marker in low
        for marker in (
            "пошаговый план",
            "план действий",
            "вот план",
            "step-by-step",
            "вот шаги",
        )
    )
    unfinished_tail = payload.endswith((":", ",", "—", "-", "…", "..."))
    no_final_punctuation = payload[-1] not in ".!?)»\"'"

    if promised_plan and not numbered_items:
        return True
    if unfinished_tail:
        return True
    if no_final_punctuation and len(payload) >= 220:
        return True
    return False


def _filter_context_for_group_author(
    context: list,
    current_author_id: int,
    is_private: bool,
    is_owner_sender: bool,
    enabled: bool = True,
) -> tuple[list, bool]:
    """
    Для групповых чатов с участниками (не owner) оставляем в user-контексте
    только сообщения текущего автора, чтобы снизить перенос «чужой личности».

    Assistant/system/tool контекст сохраняется.
    """
    if not enabled or is_private or is_owner_sender or not current_author_id:
        return context, False

    marker = f"author_id={int(current_author_id)}"
    filtered: list = []
    trimmed = False
    for item in context or []:
        role = str((item or {}).get("role", "user")).strip().lower()
        if role != "user":
            filtered.append(item)
            continue
        # Предпочитаем явную метку автора в payload (более надежно, чем парсить текст).
        try:
            item_author_id = int((item or {}).get("author_id") or 0)
        except Exception:
            item_author_id = 0
        if item_author_id:
            if item_author_id == int(current_author_id):
                filtered.append(item)
            else:
                trimmed = True
            continue
        text = str((item or {}).get("text", "") or "")
        if marker in text:
            filtered.append(item)
        else:
            trimmed = True
    return filtered, trimmed


async def _safe_stream_edit_text(reply_msg: Message, text: str) -> None:
    """
    Безопасное edit_text для стриминга:
    1) пробуем markdown-саниtизированный вариант,
    2) при провале пробуем plain text без parse_mode.
    """
    safe_text = sanitize_markdown_for_telegram(text)
    try:
        await reply_msg.edit_text(safe_text)
        return
    except Exception as markdown_error:
        plain = _to_plain_stream_text(safe_text)
        try:
            await reply_msg.edit_text(plain, parse_mode=None)
            return
        except Exception as plain_error:
            logger.debug(
                "Stream edit_text skipped after markdown/plain fallback failures",
                markdown_error=str(markdown_error),
                plain_error=str(plain_error),
            )


def _message_content_hint(msg: Message) -> str:
    """Возвращает короткий текстовый дескриптор любого типа сообщения."""
    text = _sanitize_model_output(msg.text or msg.caption or "")
    if text:
        return text
    if msg.voice:
        return "[VOICE] Голосовое сообщение"
    if msg.audio:
        title = ""
        if msg.audio and getattr(msg.audio, "title", None):
            title = f" ({msg.audio.title})"
        return f"[AUDIO] Аудио{title}"
    if msg.sticker:
        emoji = getattr(msg.sticker, "emoji", "") or ""
        return f"[STICKER] {emoji}".strip()
    if msg.animation:
        return "[GIF] Анимация"
    if msg.video:
        return "[VIDEO] Видео"
    if msg.photo:
        return "[PHOTO] Изображение"
    if msg.document:
        name = getattr(msg.document, "file_name", "") or ""
        return f"[DOCUMENT] {name}".strip()
    if msg.poll:
        question = getattr(msg.poll, "question", "") or ""
        return f"[POLL] {question}".strip()
    media_type = getattr(getattr(msg, "media", None), "value", "")
    if media_type:
        return f"[{str(media_type).upper()}] Медиа-сообщение"
    return ""


async def set_message_reaction(client, chat_id: int, message_id: int, emoji: str):
    """Ставит реакцию (emoji) на сообщение."""
    try:
        # В Pyrogram v2+ send_reaction принимает emoji как строку
        await client.send_reaction(chat_id, message_id, emoji)
    except Exception as e:
        logger.debug(f"Reaction failed: {e}")


async def _await_if_needed(value):
    """Ожидает значение только если оно awaitable."""
    if inspect.isawaitable(value):
        return await value
    return value


def _extract_text_from_media_payload(payload: Any) -> str:
    """
    Нормализует ответ мультимодальных интеграций (perceptor/openclaw) в строку.
    Поддерживает string/dict/list форматы.
    """
    if payload is None:
        return ""
    if isinstance(payload, str):
        return payload.strip()
    if isinstance(payload, dict):
        for key in ("text", "description", "transcript", "result", "answer"):
            value = payload.get(key)
            if isinstance(value, str) and value.strip():
                return value.strip()
        content = payload.get("content")
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict):
                    text = item.get("text")
                    if isinstance(text, str) and text.strip():
                        return text.strip()
                elif isinstance(item, str) and item.strip():
                    return item.strip()
    if isinstance(payload, list):
        for item in payload:
            extracted = _extract_text_from_media_payload(item)
            if extracted:
                return extracted
    return str(payload).strip()


async def _process_auto_reply(client, message: Message, deps: dict):
    """
    Умный автоответчик v3 (Omni-channel + Reactions + Multimodal).
    """
    security = deps["security"]
    rate_limiter = deps["rate_limiter"]
    memory = deps["memory"]
    router = deps["router"]
    config_manager = deps.get("config_manager")
    perceptor = deps.get("perceptor")
    openclaw = deps.get("openclaw_client")
    summarizer = deps.get("summarizer")
    ai_runtime: AIRuntimeControl | None = deps.get("ai_runtime")
    reaction_engine: ReactionLearningEngine | None = deps.get("reaction_engine")
    
    sender = message.from_user.username if message.from_user else "Unknown"
    is_owner_sender = bool(is_owner(message))
    logger.debug(
        "auto_reply: старт обработки",
        chat_id=message.chat.id,
        message_id=message.id,
        sender=sender,
        is_owner_sender=is_owner_sender,
        chat_type=str(getattr(message.chat.type, "name", message.chat.type)),
    )

    # 1. Проверка через SecurityManager
    role = security.get_user_role(sender, message.from_user.id if message.from_user else 0)
    logger.debug(
        "auto_reply: рассчитана роль отправителя",
        chat_id=message.chat.id,
        message_id=message.id,
        sender=sender,
        role=role,
    )
    
    if role == "blocked":
            logger.debug("auto_reply: пропуск blocked role", chat_id=message.chat.id, message_id=message.id, sender=sender)
            return

    if role == "stealth_restricted":
        logger.debug("auto_reply: пропуск stealth mode", chat_id=message.chat.id, message_id=message.id, sender=sender)
        return

    # 2. Логика срабатывания (Smart Reply v2.0)
    is_private = message.chat.type == enums.ChatType.PRIVATE
    is_reply_to_me = (
        message.reply_to_message and 
        message.reply_to_message.from_user and 
        message.reply_to_message.from_user.is_self
    )
    
    me = await client.get_me()
    is_mentioned = False
    text_content = _message_content_hint(message)
    
    if text_content:
        text_lower = text_content.lower()
        is_mentioned = (
            "краб" in text_lower or 
            (me.username and f"@{me.username.lower()}" in text_lower)
        )

    allow_group_replies = True
    if config_manager:
        allow_group_replies = config_manager.get("group_chat.allow_replies", True)

    should_reply = False
    if is_private:
        should_reply = True
    elif is_mentioned:
        should_reply = True
    elif is_reply_to_me and allow_group_replies:
        should_reply = True

    if not should_reply:
        logger.debug(
            "auto_reply: пропуск, не выполнены условия ответа",
            chat_id=message.chat.id,
            message_id=message.id,
            is_private=is_private,
            is_mentioned=is_mentioned,
            is_reply_to_me=bool(is_reply_to_me),
        )
        memory.save_message(
            message.chat.id,
            _build_user_memory_payload(
                message=message,
                sender=sender,
                text=text_content,
                is_owner_sender=is_owner_sender,
            ),
        )
        return
    if is_private:
        logger.info(
            "auto_reply: приватное сообщение принято в обработку",
            chat_id=message.chat.id,
            message_id=message.id,
            sender=sender,
            is_owner_sender=is_owner_sender,
        )

    # Антиспам
    has_rich_media = bool(
        message.photo or message.voice or message.audio or 
        message.sticker or message.animation or message.video or message.document
    )
    if not is_private and len(text_content) < 2 and not is_reply_to_me and not has_rich_media:
        logger.debug("auto_reply: пропуск anti-spam фильтр", chat_id=message.chat.id, message_id=message.id)
        return

    # Rate Limiting
    user_id = message.from_user.id if message.from_user else 0
    if not rate_limiter.is_allowed(user_id):
        logger.warning("auto_reply: пропуск rate limit", chat_id=message.chat.id, message_id=message.id, user_id=user_id)
        if is_private:
            try:
                await message.reply_text("⏳ Слишком много запросов подряд. Подожди немного и повтори.")
            except Exception:
                pass
        return

    # 2. Обработка мультимедиа (Vision / Voice / Video / Docs / Stickers)
    visual_context = ""
    transcribed_text = ""
    vision_route_fact_line = ""
    vision_route_meta: dict = {}
    is_voice_response_needed = _is_voice_reply_requested(text_content)
    temp_files = []

    try:
        # --- PHOTO (Vision) ---
        if message.photo:
            await client.send_chat_action(message.chat.id, action=enums.ChatAction.UPLOAD_PHOTO)
            photo_path = await message.download()
            temp_files.append(photo_path)
            if perceptor:
                vision_raw = await perceptor.analyze_image(
                    photo_path,
                    router,
                    prompt="Опиши это изображение подробно на русском языке.",
                )
                vision_result = _sanitize_model_output(
                    _extract_text_from_media_payload(vision_raw),
                    router,
                )
                if hasattr(perceptor, "get_last_vision_meta"):
                    try:
                        vision_route_meta = perceptor.get_last_vision_meta() or {}
                        vision_route_fact_line = _build_vision_route_fact_line(vision_route_meta)
                    except Exception:
                        vision_route_meta = {}
                        vision_route_fact_line = ""
            elif openclaw and hasattr(openclaw, "analyze_image"):
                vision_raw = await _await_if_needed(openclaw.analyze_image(photo_path))
                vision_result = _sanitize_model_output(
                    _extract_text_from_media_payload(vision_raw),
                    router,
                )
            else:
                await message.reply_text("❌ Vision module недоступен.")
                return
            if vision_result and not vision_result.startswith("Ошибка"):
                visual_context = f"[VISION ANALYSIS]: User sent a photo. Description: {vision_result}"
            else:
                visual_context = "[VISION ERROR]: Failed to analyze photo."

        # --- VOICE / AUDIO (STT) ---
        elif message.voice or message.audio:
            status_msg = None
            await client.send_chat_action(message.chat.id, action=enums.ChatAction.RECORD_AUDIO)
            try:
                status_msg = await message.reply_text("👂 Распознаю голос...")
            except Exception:
                status_msg = None
            audio_path = await message.download()
            temp_files.append(audio_path)
            if perceptor:
                transcription_raw = await perceptor.transcribe(audio_path, router)
            elif openclaw and hasattr(openclaw, "transcribe_audio"):
                transcription_raw = await _await_if_needed(openclaw.transcribe_audio(audio_path))
            else:
                await message.reply_text("❌ Voice module недоступен.")
                return
            transcribed_text = _sanitize_model_output(
                _extract_text_from_media_payload(transcription_raw),
                router,
            )
            if transcribed_text and not transcribed_text.startswith("Ошибка"):
                if message.voice:
                    is_voice_response_needed = True
                if status_msg:
                    try:
                        await status_msg.delete()
                    except Exception:
                        pass
            else:
                human_error = transcribed_text or "Не удалось распознать голосовое сообщение."
                if status_msg:
                    try:
                        await status_msg.edit_text(f"⚠️ {human_error[:450]}")
                    except Exception:
                        pass
                else:
                    await message.reply_text(f"⚠️ {human_error[:450]}")
                return

        # --- VIDEO / GIF (Deep Analysis) ---
        elif message.video or message.animation:
            if not perceptor:
                await message.reply_text("❌ Vision module недоступен.")
                return
            await client.send_chat_action(message.chat.id, action=enums.ChatAction.UPLOAD_VIDEO)
            notif = await message.reply_text("🎬 **Смотрю...**")
            media_path = await message.download()
            temp_files.append(media_path)
            # Для GIF/Video используем Gemini Video Analysis
            video_result = _sanitize_model_output(
                await perceptor.analyze_video(
                    media_path,
                    router,
                    prompt="Опиши очень кратко (1-2 предложения), что происходит на видео/гифке. Какой основной посыл или эмоция?",
                ),
                router,
            )
            if video_result and not video_result.startswith("Ошибка"):
                visual_context = f"[MEDIA ANALYSIS]: {video_result}"
                await notif.delete()
            else:
                await notif.edit_text(f"❌ Ошибка анализа: {video_result}")
                visual_context = "[MEDIA ERROR]: Failed to analyze video/gif."

        # --- DOCUMENT ---
        elif message.document:
            if not perceptor:
                await message.reply_text("❌ Document module недоступен.")
                return
            await client.send_chat_action(message.chat.id, action=enums.ChatAction.UPLOAD_DOCUMENT)
            notif = await message.reply_text("📄 **Читаю...**")
            doc_path = await message.download()
            temp_files.append(doc_path)
            doc_result = _sanitize_model_output(
                await perceptor.analyze_document(
                    doc_path,
                    router,
                    prompt="Сделай краткий обзор документа на русском.",
                ),
                router,
            )
            if doc_result and not doc_result.startswith("Ошибка"):
                visual_context = f"[DOCUMENT ANALYSIS]: {doc_result}"
                await notif.delete()
            else:
                await notif.edit_text(f"❌ Ошибка: {doc_result}")
                visual_context = "[DOCUMENT ERROR]: Failed to analyze document."

        # --- STICKER ---
        elif message.sticker:
            emoji = message.sticker.emoji or "🎨"
            visual_context = f"[USER SENT A STICKER: {emoji}]"
            # Для стикеров можно сразу поставить реакцию "глаза" или "сердце".
            await set_message_reaction(client, message.chat.id, message.id, "👀")

    except Exception as e:
        logger.error(f"Media processing error: {e}")
    finally:
        for p in temp_files:
            try:
                if os.path.exists(p): os.remove(p)
            except: pass

    # Context gathering
    reply_context = _build_reply_context(message)
    forward_context = _build_forward_context(
        message,
        enabled=bool(ai_runtime and ai_runtime.forward_context_enabled),
    )
    is_forwarded_input = _is_forwarded_message(message)

    # Final prompt
    final_prompt = f"{transcribed_text} (Voice Input)" if transcribed_text else text_content
    prefer_russian_response = _should_force_russian_reply(
        user_text=text_content,
        is_private=is_private,
        is_owner_sender=is_owner_sender,
        is_voice_response_needed=is_voice_response_needed,
    )
    author_context = _build_author_context(message, is_owner_sender=is_owner_sender)
    if author_context:
        final_prompt = f"{author_context}\n\n{final_prompt}"
    if visual_context:
        final_prompt = f"{visual_context}\n\nUser Says: {final_prompt}"
    if vision_route_fact_line:
        final_prompt = (
            f"[VISION ROUTE FACT]: {vision_route_fact_line}\n"
            "Это технический факт выполнения запроса. Не искажай его и не утверждай противоположное.\n\n"
            f"{final_prompt}"
        )
    if reply_context:
        final_prompt = f"{reply_context}\n\n{final_prompt}"
    if forward_context:
        forward_guard = (
            "Ниже пересланный контент. Отвечай строго по нему.\n"
            "Не продолжай старую тему из предыдущих сообщений, если пользователь этого явно не просил.\n"
            "Если в пересланном тексте нет явного вопроса/задачи — коротко уточни, что именно сделать:"
            " суммаризировать, проанализировать, ответить на него или извлечь факты."
        )
        final_prompt = f"{forward_context}\n\n{forward_guard}\n\n{final_prompt}"
    if reaction_engine and ai_runtime and ai_runtime.chat_mood_enabled:
        mood_line = reaction_engine.build_mood_context_line(message.chat.id)
        if mood_line:
            final_prompt = f"{mood_line}\n\n{final_prompt}"
    if prefer_russian_response:
        final_prompt = (
            "Отвечай строго на русском языке, если пользователь явно не попросил другой язык.\n"
            "Не вставляй длинные англоязычные блоки и не смешивай языки в одном ответе.\n"
            "Не придумывай результаты запуска команд/обновлений; если чего-то не выполнял — скажи это прямо.\n\n"
            f"{final_prompt}"
        )
    if is_voice_response_needed:
        final_prompt = (
            "Ответь строго на русском языке, дружелюбно и естественно.\n"
            "Без служебных фраз, без статусов, без мета-комментариев в скобках.\n"
            "Если это сказка/история — дай цельный литературный текст.\n\n"
            f"{final_prompt}"
        )

    # Sync & Save
    try:
        await asyncio.wait_for(
            memory.sync_telegram_history(client, message.chat.id, limit=30),
            timeout=max(2.0, float(AUTO_REPLY_HISTORY_SYNC_TIMEOUT_SECONDS)),
        )
    except asyncio.TimeoutError:
        logger.warning(
            "auto_reply: sync_telegram_history timeout, продолжаю без полной синхронизации",
            chat_id=message.chat.id,
            message_id=message.id,
            timeout_seconds=float(AUTO_REPLY_HISTORY_SYNC_TIMEOUT_SECONDS),
        )
    except Exception as sync_exc:
        logger.warning(
            "auto_reply: sync_telegram_history failed, продолжаю с локальным контекстом",
            chat_id=message.chat.id,
            message_id=message.id,
            error=str(sync_exc),
        )
    chat_type_value = str(getattr(message.chat.type, "name", message.chat.type)).lower()
    memory.save_message(
        message.chat.id,
        _build_user_memory_payload(
            message=message,
            sender=sender,
            text=final_prompt,
            is_owner_sender=is_owner_sender,
        ),
    )
    
    if summarizer and AUTO_SUMMARY_ENABLED:
        asyncio.create_task(summarizer.auto_summarize(message.chat.id))

    # Routing
    context = memory.get_token_aware_context(message.chat.id, max_tokens=AUTO_REPLY_CONTEXT_TOKENS)
    if is_forwarded_input:
        # Для форвардов не тащим длинный хвост старого диалога, чтобы не «залипать» в прошлую тему.
        context = []
    user_context_before = sum(
        1 for item in (context or []) if str((item or {}).get("role", "user")).strip().lower() == "user"
    )
    context, group_author_context_trimmed = _filter_context_for_group_author(
        context=context,
        current_author_id=user_id,
        is_private=is_private,
        is_owner_sender=is_owner_sender,
        enabled=bool(ai_runtime and ai_runtime.group_author_isolation_enabled),
    )
    context, dropped_service_context_items = _drop_service_busy_context_items(context)
    user_context_after = sum(
        1 for item in (context or []) if str((item or {}).get("role", "user")).strip().lower() == "user"
    )
    dropped_user_context_items = max(0, int(user_context_before - user_context_after))
    if ai_runtime:
        ai_runtime.set_context_snapshot(
            message.chat.id,
            {
                "chat_id": int(message.chat.id),
                "message_id": int(message.id),
                "context_messages": len(context or []),
                "prompt_length_chars": len(final_prompt or ""),
                "has_forward_context": bool(forward_context),
                "is_forwarded_input": bool(is_forwarded_input),
                "has_reply_context": bool(reply_context),
                "group_author_isolation_enabled": bool(ai_runtime.group_author_isolation_enabled),
                "continue_on_incomplete_enabled": bool(ai_runtime.continue_on_incomplete_enabled),
                "group_author_context_trimmed": bool(group_author_context_trimmed),
                "group_author_context_user_messages_before": int(user_context_before),
                "group_author_context_user_messages_after": int(user_context_after),
                "group_author_context_dropped_user_messages": int(dropped_user_context_items),
                "service_artifact_context_items_dropped": int(dropped_service_context_items),
                "continue_on_incomplete_triggered": False,
                "continue_on_incomplete_applied": False,
                "updated_at": int(time.time()),
            },
        )
    
    # Typing indicator
    await client.send_chat_action(message.chat.id, action=enums.ChatAction.TYPING)
    reply_msg = await message.reply_text("🤔 **Думаю...**")
    logger.debug("auto_reply: отправлен thinking placeholder", chat_id=message.chat.id, message_id=message.id)
    
    full_response = ""
    last_update = 0
    last_preview_sent = ""

    async def _iter_router_parts():
        """
        Унифицированный стрим-адаптер:
        1) route_stream (текущий API),
        2) route_query_stream (legacy API),
        3) route_query (single-shot fallback).
        """
        route_kwargs = {
            "prompt": final_prompt,
            "task_type": "chat",
            "context": context,
            "chat_type": message.chat.type.name.lower(),
            "is_owner": is_owner_sender,
        }

        route_stream = getattr(router, "route_stream", None)
        if callable(route_stream):
            try:
                stream_candidate = route_stream(**route_kwargs)
                if hasattr(stream_candidate, "__aiter__"):
                    streamed_any = False
                    async for part in stream_candidate:
                        streamed_any = True
                        yield part
                    if streamed_any:
                        return
            except Exception as stream_exc:
                logger.debug("auto_reply: route_stream fallback", error=str(stream_exc))

        route_query_stream = getattr(router, "route_query_stream", None)
        if callable(route_query_stream):
            try:
                stream_candidate = route_query_stream(**route_kwargs)
                if hasattr(stream_candidate, "__aiter__"):
                    streamed_any = False
                    async for part in stream_candidate:
                        streamed_any = True
                        yield part
                    if streamed_any:
                        return
            except Exception as stream_exc:
                logger.debug("auto_reply: route_query_stream fallback", error=str(stream_exc))

        route_query = getattr(router, "route_query", None)
        if callable(route_query):
            try:
                single = await _await_if_needed(route_query(**route_kwargs))
                single_text = str(single or "").strip()
                if single_text:
                    yield single_text
            except Exception as query_exc:
                logger.debug("auto_reply: route_query fallback failed", error=str(query_exc))
    
    async def run_streaming():
        nonlocal full_response, last_update, last_preview_sent
        try:
            async for part in _iter_router_parts():
                full_response += part
                curr_t = time.time()
                # Плавное обновление превью, чтобы пользователь видел прогресс в реальном времени.
                edit_interval = max(0.5, float(AUTO_REPLY_STREAM_EDIT_INTERVAL_SECONDS))
                if curr_t - last_update > edit_interval:
                    preview = _build_stream_preview(full_response, max_chars=3600)
                    candidate = preview + " ▌"
                    if _should_emit_stream_edit(last_preview_sent, candidate, min_delta_chars=80):
                        await _safe_stream_edit_text(reply_msg, candidate)
                        last_preview_sent = candidate
                        last_update = curr_t
        except Exception as e:
            logger.error(f"Streaming error occurred: {e}")
            # Если у нас уже есть какой-то текст, мы не пробрасываем ошибку дальше,
            # чтобы пользователь получил хотя бы часть ответа.
            if not full_response:
                raise e
            else:
                 full_response += f"\n\n⚠️ [Стрим прерван: {e}]"

    try:
        await asyncio.wait_for(run_streaming(), timeout=AUTO_REPLY_TIMEOUT_SECONDS)
    except asyncio.TimeoutError:
        logger.warning(f"Timeout reaching model for chat {message.chat.id}")
        if not full_response:
             await reply_msg.edit_text("⌛ **Время ожидания истекло.** Попробуйте еще раз.")
             return
    except Exception as e:
        logger.error(f"Auto-reply critical failure: {e}")
        if not full_response:
            await reply_msg.edit_text(f"❌ Ошибка: {e}")
            return

    if (
        full_response
        and bool(ai_runtime and ai_runtime.continue_on_incomplete_enabled)
        and _looks_incomplete_response(full_response)
    ):
        if ai_runtime:
            current = ai_runtime.get_context_snapshot(message.chat.id)
            current["continue_on_incomplete_triggered"] = True
            current["updated_at"] = int(time.time())
            ai_runtime.set_context_snapshot(message.chat.id, current)
        try:
            await client.send_chat_action(message.chat.id, action=enums.ChatAction.TYPING)
            continuation_prompt = (
                "Продолжи предыдущий ответ с места обрыва. "
                "Не повторяй уже написанное. "
                "Если был обещан план/список — допиши его полностью и завершённо."
            )
            continuation_context = list(context or []) + [
                {"role": "assistant", "text": str(full_response)},
            ]
            continuation = ""
            async for part in router.route_stream(
                prompt=continuation_prompt,
                task_type="chat",
                context=continuation_context,
                chat_type=message.chat.type.name.lower(),
                is_owner=is_owner_sender,
            ):
                continuation += part
            continuation = _sanitize_model_output(continuation, router)
            if continuation:
                full_response = f"{full_response.rstrip()}\n\n{continuation.lstrip()}"
                if ai_runtime:
                    current = ai_runtime.get_context_snapshot(message.chat.id)
                    current["continue_on_incomplete_applied"] = True
                    current["updated_at"] = int(time.time())
                    ai_runtime.set_context_snapshot(message.chat.id, current)
        except Exception as continue_exc:
            logger.debug(
                "Автодопродолжение ответа пропущено после ошибки",
                chat_id=message.chat.id,
                error=str(continue_exc),
            )

    persisted_response_text = _sanitize_model_output(full_response, router)
    if full_response:
        logger.debug(
            "auto_reply: получен ответ модели",
            chat_id=message.chat.id,
            message_id=message.id,
            response_chars=len(full_response or ""),
        )
        clean_display_text = _sanitize_model_output(full_response, router)
        clean_display_text, removed_service_phrases = _drop_service_busy_phrases(clean_display_text)
        clean_display_text, removed_tool_artifacts = _drop_tool_artifact_blocks(clean_display_text)
        clean_display_text, removed_english_scaffold = _drop_english_scaffold_when_russian_expected(
            clean_display_text,
            prefer_russian=prefer_russian_response,
            min_paragraph_len=160,
        )
        clean_display_text, removed_repeated_lines = _collapse_repeated_lines(
            clean_display_text,
            max_consecutive_repeats=2,
        )
        clean_display_text, removed_repeats = _collapse_repeated_paragraphs(
            clean_display_text,
            max_consecutive_repeats=2,
        )
        clean_display_text, removed_nonconsecutive_repeats = _dedupe_repeated_long_paragraphs(
            clean_display_text,
            min_normalized_len=140,
            max_occurrences=1,
        )
        clean_display_text, corrected_vision_consistency = _enforce_vision_route_consistency(
            clean_display_text,
            vision_route_meta,
        )
        clean_display_text, trimmed_numbered = _cap_numbered_list_items(
            clean_display_text,
            max_items=AUTO_REPLY_MAX_NUMBERED_LIST_ITEMS,
        )
        clean_display_text, removed_numbered_duplicates = _prune_repetitive_numbered_items(
            clean_display_text,
            max_same_body=2,
        )
        if removed_repeats:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автоочистка: убраны повторяющиеся фрагменты ответа."
            ).strip()
        if removed_repeated_lines:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автоочистка: убраны повторяющиеся строки ответа."
            ).strip()
        if removed_nonconsecutive_repeats:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автоочистка: убраны повторяющиеся длинные блоки ответа."
            ).strip()
        if corrected_vision_consistency:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автокоррекция: удалены фразы, противоречащие фактическому vision-маршруту."
            ).strip()
        if removed_service_phrases:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автоочистка: удалены служебные строки очереди/ожидания."
            ).strip()
        if removed_tool_artifacts:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автоочистка: удалены внутренние служебные блоки модели."
            ).strip()
        if removed_english_scaffold:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автоочистка: убраны длинные англоязычные вставки (сохранён русский вариант ответа)."
            ).strip()
        if removed_numbered_duplicates:
            clean_display_text = (
                f"{clean_display_text}\n\n⚠️ Автоочистка: убраны дубли пунктов в нумерованном списке."
            ).strip()
        if _looks_like_internal_dump(clean_display_text):
            clean_display_text = (
                "⚠️ Поймал внутренний служебный вывод модели и скрыл его.\n"
                "Попробуй повторить запрос короче, без пересылки слишком длинного технического контента."
            )
        clean_display_text, runtime_error_rewritten = _normalize_runtime_error_message_for_user(
            clean_display_text,
            router,
        )
        if runtime_error_rewritten:
            logger.warning(
                "auto_reply: runtime ошибка нормализована в user-facing fallback",
                chat_id=message.chat.id,
                message_id=message.id,
            )
        clean_display_text, response_trimmed = _clamp_auto_reply_text(
            clean_display_text,
            is_private=is_private,
        )
        if response_trimmed:
            logger.warning(
                "auto_reply: ответ ограничен по длине",
                chat_id=message.chat.id,
                message_id=message.id,
                limit_private=AUTO_REPLY_MAX_RESPONSE_CHARS_PRIVATE,
                limit_public=AUTO_REPLY_MAX_RESPONSE_CHARS,
            )
        if trimmed_numbered:
            logger.warning("Ответ был ограничен по длине нумерованного списка", chat_id=message.chat.id)
        if not clean_display_text:
            clean_display_text = "⚠️ Ответ очищен от служебных артефактов. Повтори запрос, если нужен полный ответ."
        if vision_route_fact_line and vision_route_fact_line not in clean_display_text:
            clean_display_text = f"{vision_route_fact_line}\n\n{clean_display_text}".strip()
        
        # Интеллектуальная реакция: если ответ начинается с эмодзи, ставим его как реакцию
        import re
        emoji_match = re.match(r"^([\U00010000-\U0010ffff])", clean_display_text)
        if emoji_match:
            await set_message_reaction(client, message.chat.id, message.id, emoji_match.group(1))
        
        # Отправка ответа
        MAX_LEN = 3900
        chunks = _split_text_chunks_for_telegram(clean_display_text, max_len=MAX_LEN)
        truncated_for_telegram = len(chunks) > 1
        chunks_sent = 1
        if len(chunks) > 1:
            await _safe_stream_edit_text(reply_msg, chunks[0])
            for idx, chunk in enumerate(chunks[1:], start=2):
                suffix = f"\n\n— Часть {idx}/{len(chunks)} —"
                safe_chunk = chunk
                if len(safe_chunk) + len(suffix) <= MAX_LEN:
                    safe_chunk = f"{safe_chunk}{suffix}"
                await message.reply_text(safe_chunk, parse_mode=None)
            chunks_sent = len(chunks)
        else:
            await _safe_stream_edit_text(reply_msg, clean_display_text)
        persisted_response_text = clean_display_text

        # Привязка ответа к маршруту для weak reaction feedback.
        if reaction_engine:
            try:
                last_route = router.get_last_route() if hasattr(router, "get_last_route") else {}
                if isinstance(last_route, dict) and last_route:
                    reaction_engine.bind_assistant_message(
                        chat_id=message.chat.id,
                        message_id=reply_msg.id,
                        route=last_route,
                    )
            except Exception as bind_exc:
                logger.debug("Не удалось привязать ответ для reaction learning", error=str(bind_exc))
        
        # TTS Implementation (Perceptor-first, OpenClaw fallback).
        if is_voice_response_needed and (perceptor or (openclaw and hasattr(openclaw, "generate_speech"))):
            error_keywords = [
                "извини",
                "не могу",
                "ошибка",
                "не удалось",
                "llm error",
                "not_found",
                "status\": \"not_found\"",
            ]
            if not any(kw in clean_display_text[:100].lower() for kw in error_keywords):
                logger.info(f"🎤 Requesting TTS for chat {message.chat.id}")
                await client.send_chat_action(message.chat.id, action=enums.ChatAction.RECORD_AUDIO)
                
                try:
                    tts_text = _prepare_tts_text(clean_display_text)
                    if not tts_text:
                        logger.warning("⚠️ TTS skipped: empty prepared text", chat_id=message.chat.id)
                        return
                    tts_chunks = _split_tts_chunks(tts_text, max_chars=1100, max_chunks=6)
                    if not tts_chunks:
                        logger.warning("⚠️ TTS skipped: no chunks after split", chat_id=message.chat.id)
                        return

                    sent_parts = 0
                    total_parts = len(tts_chunks)
                    for idx, chunk in enumerate(tts_chunks, start=1):
                        if perceptor:
                            tts_file = await perceptor.speak(chunk)
                        else:
                            tts_file = await _await_if_needed(openclaw.generate_speech(chunk))
                        if not (tts_file and os.path.exists(tts_file)):
                            logger.warning(
                                "⚠️ TTS failed to generate file for chunk",
                                chat_id=message.chat.id,
                                chunk_index=idx,
                                chunk_total=total_parts,
                            )
                            continue

                        caption = (
                            "🗣️ **Voice Reply**"
                            if total_parts == 1
                            else f"🗣️ **Voice Reply {idx}/{total_parts}**"
                        )
                        await message.reply_voice(tts_file, caption=caption)
                        sent_parts += 1
                        try:
                            os.remove(tts_file)
                        except Exception:
                            pass

                    if sent_parts == 0:
                        await message.reply_text("🗣️ *[Ошибка озвучки: не удалось сгенерировать аудио]*")
                    else:
                        logger.info(
                            "✅ Voice reply sent",
                            chat_id=message.chat.id,
                            parts_sent=sent_parts,
                            parts_total=total_parts,
                        )
                except Exception as tts_exc:
                    logger.error(f"❌ TTS Error in ai.py: {tts_exc}")
                    await message.reply_text(f"🗣️ *[Ошибка TTS: {str(tts_exc)[:100]}]*")
            else:
                logger.info("🔇 Skipping TTS for error message/refusal.")
    else:
        await reply_msg.edit_text("❌ Пустой ответ.")

    # Save Assistant Message
    is_runtime_error_resp = False
    if hasattr(router, "_is_runtime_error_message"):
        is_runtime_error_resp = router._is_runtime_error_message(persisted_response_text)
    elif persisted_response_text.startswith("❌ "):
        is_runtime_error_resp = True

    if not is_runtime_error_resp:
        memory.save_message(
            message.chat.id,
            {
                "role": "assistant",
                "text": persisted_response_text,
                "chat_type": chat_type_value,
                "reply_to_author_id": int(user_id or 0),
            },
        )
    else:
        logger.warning(
            "auto_reply: пропущен save_message (обнаружено сообщение об ошибке)",
            chat_id=message.chat.id
        )

    if ai_runtime:
        current = ai_runtime.get_context_snapshot(message.chat.id)
        current["response_length_chars"] = len(persisted_response_text or "")
        current["telegram_truncated"] = bool(locals().get("truncated_for_telegram", False))
        current["telegram_chunks_sent"] = int(locals().get("chunks_sent", 1))
        current["updated_at"] = int(time.time())
        ai_runtime.set_context_snapshot(message.chat.id, current)

    # Optional: авто-реакция Краба на запрос пользователя.
    if reaction_engine and ai_runtime and ai_runtime.auto_reactions_enabled:
        try:
            if reaction_engine.can_send_auto_reaction(message.chat.id):
                reaction_emoji = reaction_engine.choose_auto_reaction(persisted_response_text, message.chat.id)
                await set_message_reaction(client, message.chat.id, message.id, reaction_emoji)
        except Exception as react_exc:
            logger.debug("Auto reaction skipped", error=str(react_exc))



def register_handlers(app, deps: dict):
    """Регистрирует AI-обработчики."""
    router = deps["router"]
    memory = deps["memory"]
    security = deps["security"]
    agent = deps["agent"]
    rate_limiter = deps["rate_limiter"]
    safe_handler = deps["safe_handler"]
    queue_manager = ChatWorkQueue(
        max_per_chat=AUTO_REPLY_QUEUE_MAX_PER_CHAT,
        max_retries=AUTO_REPLY_QUEUE_MAX_RETRIES,
    )
    reaction_engine = ReactionLearningEngine(
        store_path=os.getenv("REACTION_FEEDBACK_PATH", "artifacts/reaction_feedback.json"),
        enabled=REACTION_LEARNING_ENABLED,
        weight=REACTION_LEARNING_WEIGHT,
        mood_enabled=CHAT_MOOD_ENABLED,
        auto_reactions_enabled=AUTO_REACTIONS_ENABLED,
        auto_reaction_rate_seconds=_timeout_from_env("AUTO_REACTIONS_MIN_INTERVAL_SECONDS", 6),
        mood_window=_timeout_from_env("CHAT_MOOD_WINDOW", 120),
    )
    ai_runtime = AIRuntimeControl(queue_manager=queue_manager, reaction_engine=reaction_engine, router=router)
    deps["ai_runtime"] = ai_runtime
    deps["reaction_engine"] = reaction_engine
    deps["chat_work_queue"] = queue_manager
    forward_burst_buffers: dict[str, dict] = {}

    @app.on_raw_update()
    async def reaction_learning_raw_handler(client, update, users, chats):
        """
        Обработка raw updates реакций на сообщения.
        Используется как weak-signal для адаптации модели/тона.
        """
        if not isinstance(update, raw.types.UpdateMessageReactions):
            return
        if not ai_runtime.reaction_learning_enabled:
            return
        try:
            chat_id = int(pyro_utils.get_peer_id(update.peer))
            message_id = int(update.msg_id)
            reactions = getattr(update, "reactions", None)
            if not reactions:
                return

            recent = getattr(reactions, "recent_reactions", None) or []
            for item in recent:
                reaction_obj = getattr(item, "reaction", None)
                emoji = ""
                if isinstance(reaction_obj, raw.types.ReactionEmoji):
                    emoji = str(getattr(reaction_obj, "emoticon", "") or "")
                elif isinstance(reaction_obj, raw.types.ReactionCustomEmoji):
                    emoji = f"custom:{getattr(reaction_obj, 'document_id', '')}"
                if not emoji:
                    continue
                actor_id = int(pyro_utils.get_peer_id(getattr(item, "peer_id", None))) if getattr(item, "peer_id", None) else 0
                reaction_engine.register_reaction(
                    chat_id=chat_id,
                    message_id=message_id,
                    actor_id=actor_id,
                    emoji=emoji,
                    action="added",
                    router=router,
                )
        except Exception as exc:
            logger.debug("Reaction raw update parse skipped", error=str(exc))

    def _extract_prompt_and_confirm_flag(message_text: str) -> tuple[str, bool]:
        """
        Разбирает команду и выделяет:
        - пользовательский prompt,
        - флаг подтверждения дорогого прогона (`--confirm-expensive` / `--confirm` / `confirm`).
        """
        raw = message_text or ""
        try:
            argv = shlex.split(raw)
        except ValueError:
            argv = raw.split()

        if len(argv) < 2:
            return "", False

        confirm_expensive = False
        payload_tokens: list[str] = []
        for token in argv[1:]:
            normalized = token.strip().lower()
            if normalized in {"--confirm-expensive", "--confirm", "confirm"}:
                confirm_expensive = True
                continue
            payload_tokens.append(token)

        prompt = " ".join(payload_tokens).strip()
        return prompt, confirm_expensive

    async def _danger_audit(message: Message, action: str, status: str, details: str = ""):
        """Логирует опасные действия в Saved Messages и владельцу для аудита."""
        sender = message.from_user.username if message.from_user else "unknown"
        chat_title = message.chat.title or "private"
        chat_id = message.chat.id
        payload = (
            f"🛡️ **Danger Audit**\n"
            f"- action: `{action}`\n"
            f"- status: `{status}`\n"
            f"- sender: `@{sender}`\n"
            f"- chat: `{chat_title}` (`{chat_id}`)\n"
        )
        if details:
            payload += f"- details: `{details[:800]}`\n"
        try:
            await app.send_message("me", payload)
        except Exception:
            pass
        try:
            await app.send_message("@p0lrd", payload)
        except Exception:
            pass

    # --- !think: Reasoning Mode ---
    @app.on_message(filters.command("think", prefixes="!"))
    @safe_handler
    async def think_command(client, message: Message):
        """Reasoning Mode: !think <запрос>"""
        prompt, confirm_expensive = _extract_prompt_and_confirm_flag(message.text or "")
        if not prompt:
            await message.reply_text(
                "🧠 О чем мне подумать? `!think Как работает квантовый компьютер?`\n"
                "Для критичных задач: добавь `--confirm-expensive`."
            )
            return

        # notification = await message.reply_text("🧠 **Размышляю...** (Reasoning Mode)") # Убираем лишнее

        context = memory.get_token_aware_context(message.chat.id, max_tokens=10000)

        full_response = ""
        last_update = 0
        last_preview_sent = ""
        
        reply_msg = await message.reply_text("🤔 **Размышляю...**")

        try:
            stream_used = False
            route_stream = getattr(router, "route_stream", None)
            if callable(route_stream):
                try:
                    stream_candidate = route_stream(
                        prompt=prompt,
                        task_type="reasoning",
                        context=context,
                        chat_type=message.chat.type.name.lower(),
                        is_owner=is_owner(message),
                        confirm_expensive=confirm_expensive,
                    )
                    if hasattr(stream_candidate, "__aiter__"):
                        stream_used = True
                        async for chunk in stream_candidate:
                            full_response += chunk
                            curr_t = time.time()
                            if curr_t - last_update > 2.0:
                                preview = _build_stream_preview(full_response, max_chars=3600)
                                candidate = preview + " ▌"
                                if _should_emit_stream_edit(last_preview_sent, candidate, min_delta_chars=120):
                                    await _safe_stream_edit_text(reply_msg, candidate)
                                    last_preview_sent = candidate
                                    last_update = curr_t
                        if not full_response:
                            stream_used = False
                except Exception as stream_exc:
                    logger.debug("think_command: fallback на route_query после stream ошибки", error=str(stream_exc))
                    stream_used = False

            if not stream_used:
                # Legacy fallback для тестов/моков без корректного async stream API.
                full_response = await router.route_query(
                    prompt=prompt,
                    task_type="reasoning",
                    context=context,
                    chat_type=message.chat.type.name.lower(),
                    is_owner=is_owner(message),
                    confirm_expensive=confirm_expensive,
                )

            await reply_msg.edit_text(_sanitize_model_output(full_response, router))
        except asyncio.TimeoutError: # Moved timeout handling here
            full_response = (
                f"⏳ Размышление заняло слишком много времени (>{THINK_TIMEOUT_SECONDS}с). "
                "Попробуй упростить запрос."
            )
        memory.save_message(message.chat.id, {"role": "assistant", "text": _sanitize_model_output(full_response, router)})

    # --- !smart: Агентный цикл (Phase 6) ---
    @app.on_message(filters.command("smart", prefixes="!"))
    @safe_handler
    async def smart_command(client, message: Message):
        """Agent Workflow: !smart <задача>"""
        if not security.can_execute_command(
            message.from_user.username, message.from_user.id, "user"
        ):
            return

        prompt, confirm_expensive = _extract_prompt_and_confirm_flag(message.text or "")
        if not prompt:
            await message.reply_text(
                "🧠 Опиши сложную задачу: "
                "`!smart Разработай план переезда в другую страну`"
            )
            return

        # Confirm-step для потенциально дорогих критичных сценариев.
        require_confirm = bool(getattr(router, "require_confirm_expensive", False))
        profile = (
            router.classify_task_profile(prompt, "reasoning")
            if hasattr(router, "classify_task_profile")
            else "chat"
        )
        is_critical = profile in {"security", "infra", "review"}
        if require_confirm and is_critical and not confirm_expensive:
            await message.reply_text(
                "⚠️ Для критичной задачи нужен confirm-step.\n"
                "Повтори с `!smart --confirm-expensive <задача>`."
            )
            return

        notification = await message.reply_text("🕵️ **Agent:** Инициализирую воркфлоу...")

        result = await agent.solve_complex_task(prompt, message.chat.id)

        await notification.edit_text(result)
        memory.save_message(message.chat.id, {"role": "assistant", "text": result})

    @app.on_message(filters.command("bg", prefixes="!"))
    @safe_handler
    async def bg_command(client, message: Message):
        """Background Task: !bg <задача>"""
        if not is_authorized(message): return

        if len(message.command) < 2:
            await message.reply_text("⏳ Опиши фоновую задачу: `!bg проведи глубокое исследование по X`")
            return

        prompt = message.text.split(" ", 1)[1]
        task_queue = deps["task_queue"]
        
        # Создаем корутину для выполнения
        coro = agent.solve_complex_task(prompt, message.chat.id)
        
        task_id = await task_queue.enqueue(f"Agent solve: {prompt[:30]}", message.chat.id, coro)
        
        await message.reply_text(f"🚀 Задача запущена в фоне!\nID: `{task_id}`\nЯ пришлю уведомление, когда закончу.")

    # --- !swarm: Swarm Intelligence (Phase 10) ---
    @app.on_message(filters.command("swarm", prefixes="!"))
    @safe_handler
    async def swarm_command(client, message: Message):
        """Swarm Intelligence: !swarm <запрос>"""
        if not is_authorized(message): return
        
        if len(message.command) < 2:
            await message.reply_text("🐝 Опиши задачу для Роя: `!swarm проанализируй рынок и поищи новости`")
            return

        query = message.text.split(" ", 1)[1]
        notification = await message.reply_text("🐝 **Swarm Intelligence:** Активация агентов...")

        tools = deps["tools"]
        # Вызываем автономное решение (включая консилиум если есть триггер)
        result = await tools.swarm.autonomous_decision(query)
        
        if result is None:
             # Fallback на обычный ответ если рой не знает что делать
             result = await router.route_query(
                 prompt=query, 
                 task_type='chat',
                 chat_type=message.chat.type.name.lower(),
                 is_owner=is_owner(message)
             )

        await notification.edit_text(result)
        memory.save_message(message.chat.id, {"role": "assistant", "text": result})

    # --- !code: Генерация кода ---
    @app.on_message(filters.command("code", prefixes="!"))
    @safe_handler
    async def code_command(client, message: Message):
        """Генерация кода: !code <описание>"""
        prompt, confirm_expensive, raw_code_mode = _extract_code_prompt_flags(message.text or "")
        if not prompt:
            await message.reply_text(
                "💻 Опиши задачу: `!code Напиши FastAPI сервер с эндпоинтом /health`\n"
                "Флаги: `--confirm-expensive`, `--raw-code`"
            )
            return

        notification = await message.reply_text("💻 **Генерирую код...**")

        if raw_code_mode:
            code_prompt = (
                f"Напиши код по запросу: {prompt}\n\n"
                "Формат: только код с комментариями, без лишних объяснений. "
                "Язык программирования — определи из контекста."
            )
        else:
            code_prompt = _build_safe_code_prompt(
                prompt=prompt,
                strict_mode=_is_critical_code_request(prompt),
            )

        response = await router.route_query(
            prompt=code_prompt,
            task_type="coding",
            chat_type=message.chat.type.name.lower(),
            is_owner=is_owner(message),
            confirm_expensive=confirm_expensive,
        )

        await notification.edit_text(response)

    # --- !learn / !remember: Обучение RAG ---
    @app.on_message(filters.command(["learn", "remember"], prefixes="!"))
    @safe_handler
    async def learn_command(client, message: Message):
        """Обучение: !learn <запрос или файл или ссылка>"""
        browser_agent = deps.get("browser_agent")
        openclaw = deps.get("openclaw_client")
        
        # 1. Если есть файл
        if message.document:
            file_name = message.document.file_name.lower()
            if not (file_name.endswith(('.txt', '.pdf', '.md'))):
                await message.reply_text("❌ Поддерживаются только .txt, .pdf и .md")
                return
            
            notif = await message.reply_text(f"📄 Читаю файл `{file_name}`...")
            path = await message.download()
            
            content = ""
            if file_name.endswith('.pdf'):
                try:
                    import PyPDF2
                    with open(path, 'rb') as f:
                        reader = PyPDF2.PdfReader(f)
                        content = "\n".join([page.extract_text() for page in reader.pages])
                except Exception as e:
                    content = f"Error reading PDF: {e}"
            else:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            
            os.remove(path)
            
            if len(content) < 10:
                await notif.edit_text("❌ Файл пуст или не читается.")
                return
            
            doc_id = router.rag.add_document(
                text=content,
                metadata={"source": "file", "filename": file_name},
                category="document"
            )
            await notif.edit_text(f"🧠 **Файл изучен!**\nID: `{doc_id}`\nСимволов: {len(content)}")
            return

        # 2. Если есть ссылка
        if len(message.command) > 1 and message.command[1].startswith('http'):
            url = message.command[1]
            notif = await message.reply_text(f"🌐 Изучаю ссылку: `{url}`...")
            content_text = ""
            title = url

            # OpenClaw-first: web_fetch, локальный браузер только fallback.
            if openclaw:
                fetched = await openclaw.invoke_tool("web_fetch", {"url": url})
                if not fetched.get("error"):
                    try:
                        content_text = fetched.get("content", [{}])[0].get("text", "")[:20000]
                        title = fetched.get("details", {}).get("title", title)
                    except Exception:
                        content_text = ""

            if not content_text and browser_agent:
                res = await browser_agent.browse(url)
                if "error" not in res:
                    content_text = res.get("content", "")
                    title = res.get("title", title)

            if not content_text:
                await notif.edit_text("❌ Не удалось получить содержимое страницы.")
                return

            doc_id = router.rag.add_document(
                text=content_text,
                metadata={"source": "web", "url": url, "title": title},
                category="web"
            )
            await notif.edit_text(f"🧠 **Ссылка изучена!**\nЗаголовок: `{title}`\nID: `{doc_id}`")
            return

        # 3. Обычный текст
        if len(message.command) < 2:
            await message.reply_text("🧠 Чему научить? `!learn Python был создан Гвидо ван Россумом` или отправь файл/ссылку.")
            return

        fact = message.text.split(" ", 1)[1]
        doc_id = router.rag.add_document(
            text=fact,
            metadata={
                "source": "user_learn",
                "user": message.from_user.username if message.from_user else "unknown",
                "chat_id": str(message.chat.id),
            },
            category="learning",
        )
        await message.reply_text(f"🧠 **Сохранено в память.** ID: `{doc_id}`")

    @app.on_message(filters.command("clone", prefixes="!"))
    @safe_handler
    async def clone_command(client, message: Message):
        """Persona Cloning: !clone [name] (Owner Only)"""
        if not is_owner(message):
            return
        
        name = message.command[1] if len(message.command) > 1 else "Digital Twin"
        notif = await message.reply_text(f"👯 **Инициализирую клонирование личности `{name}`...**")
        
        # 1. Сбор данных из RAG (сообщения пользователя)
        await notif.edit_text("🔎 **Шаг 1/3:** Собираю образцы твоего стиля из памяти...")
        query = f"сообщения от @{message.from_user.username}"
        samples = router.rag.query(query, n_results=15, category="learning")
        
        if not samples or len(samples) < 50:
            # Fallback: пробуем искать в общей категории
            samples = router.rag.query(query, n_results=15)

        if not samples or len(samples) < 50:
             await notif.edit_text("❌ **Ошибка:** Недостаточно данных в памяти для анализа стиля. Пообщайся со мной побольше!")
             return

        # 2. Анализ стиля через LLM
        await notif.edit_text("📊 **Шаг 2/3:** Анализирую паттерны речи и лингвистический профиль...")
        analysis_prompt = (
            f"Проанализируй стиль общения пользователя на основе этих примеров:\n\n{samples}\n\n"
            "Твоя задача: Составить краткий 'System Prompt' (на русском), который позволит другой LLM "
            f"имитировать этого пользователя. Назови его '{name}'. "
            "Учти: тональность, любимые слова, использование эмодзи, длину предложений, уровень формальности. "
            "Ответь ТОЛЬКО текстом промпта, начинающимся с 'Ты — цифровой двойник...'"
        )
        
        custom_prompt = await router.route_query(
            prompt=analysis_prompt,
            task_type="chat",
            is_owner=True
        )

        # 3. Регистрация личности
        await notif.edit_text("💾 **Шаг 3/3:** Сохраняю новую личность в ядро...")
        persona_manager = deps["persona_manager"]
        pid = f"clone_{name.lower().replace(' ', '_')}"
        persona_manager.add_custom_persona(
            pid=pid,
            name=f"Клон: {name}",
            prompt=custom_prompt,
            desc=f"Цифровой двойник, созданный на основе анализа @{message.from_user.username}"
        )
        
        await notif.edit_text(
            f"✅ **Клонирование завершено!**\n\n"
            f"🆔 ID: `{pid}`\n"
            f"🎭 Имя: `Клон: {name}`\n\n"
            f"Чтобы активировать, введи: `!persona set {pid}`"
        )

    # --- !rag: Статистика и поиск по базе знаний ---
    @app.on_message(filters.command(["rag", "search"], prefixes="!"))
    @safe_handler
    async def rag_command(client, message: Message):
        """Инфо и поиск по RAG: !rag [запрос]"""
        if len(message.command) < 2:
            report = router.rag.format_stats_report()
            await message.reply_text(report)
            return

        query = message.text.split(" ", 1)[1]
        results = router.rag.query_with_scores(query, n_results=3)
        
        if not results:
            await message.reply_text("🔎 Ничего не найдено.")
            return
        
        resp = f"🔎 **Результаты поиска по запросу: `{query}`**\n\n"
        for i, res in enumerate(results, 1):
            expired = "⚠️ (Устарело)" if res['expired'] else ""
            resp += f"{i}. [{res['category']}] Score: {res['score']} {expired}\n"
            resp += f"_{res['text'][:200]}..._\n\n"
        
        await message.reply_text(resp)

    # --- !forget: Очистить историю чата ---
    @app.on_message(filters.command("forget", prefixes="!"))
    @safe_handler
    async def forget_command(client, message: Message):
        """Очистка истории текущего чата."""
        if not is_authorized(message): return
        
        memory.clear_history(message.chat.id)
        await message.reply_text("🧹 **Память чата очищена.**")

    # --- !vision: Runtime-настройки локального vision ---
    @app.on_message(filters.command("vision", prefixes="!"))
    @safe_handler
    async def vision_command(client, message: Message):
        """Управление локальным vision-контуром: !vision local on|off|status|model <id>."""
        if not is_authorized(message):
            return

        perceptor = deps.get("perceptor")
        if not perceptor:
            await message.reply_text("❌ Perceptor не инициализирован.")
            return

        config_manager = deps.get("config_manager")
        parts = (message.text or "").split()
        args = parts[1:] if len(parts) > 1 else []
        action = (args[0].strip().lower() if args else "status")

        def _status_text() -> str:
            enabled = bool(getattr(perceptor, "local_vision_enabled", False))
            pinned_model = str(getattr(perceptor, "local_vision_model", "") or "").strip()
            resolved_model = ""
            if hasattr(perceptor, "_resolve_local_vision_model"):
                try:
                    resolved_model = str(perceptor._resolve_local_vision_model(router) or "").strip()
                except Exception:
                    resolved_model = pinned_model
            gemini_model = str(getattr(perceptor, "vision_model", "") or "").strip()
            timeout_sec = int(float(getattr(perceptor, "local_vision_timeout_seconds", 90)))
            max_tokens = int(getattr(perceptor, "local_vision_max_tokens", 1200))
            last_meta = {}
            if hasattr(perceptor, "get_last_vision_meta"):
                try:
                    candidate_meta = perceptor.get_last_vision_meta()
                    if isinstance(candidate_meta, dict):
                        last_meta = candidate_meta
                except Exception:
                    last_meta = {}
            last_route = str(last_meta.get("route") or "-").strip()
            last_model = str(last_meta.get("model") or "-").strip()
            last_fallback = bool(last_meta.get("fallback_used"))
            last_error = str(last_meta.get("error") or "").strip()
            last_line = (
                f"• Last vision route: `{last_route}`\n"
                f"• Last vision model: `{last_model}`\n"
                f"• Last fallback used: `{'YES' if last_fallback else 'NO'}`\n"
            )
            if last_error:
                last_line += f"• Last vision error: `{last_error[:140]}`\n"
            return (
                "**👁️ Vision Runtime:**\n\n"
                f"• Local vision: `{'ON' if enabled else 'OFF'}`\n"
                f"• Local model (pinned): `{pinned_model or '-'}`\n"
                f"• Local model (resolved): `{resolved_model or '-'}`\n"
                f"• Local timeout: `{timeout_sec}s`\n"
                f"• Local max tokens: `{max_tokens}`\n"
                f"• Gemini fallback model: `{gemini_model or '-'}`\n"
                f"{last_line}\n"
                "Команды:\n"
                "`!vision status`\n"
                "`!vision local on`\n"
                "`!vision local off`\n"
                "`!vision model <lm_studio_model_id>`"
            )

        if action in {"status", "show"}:
            await message.reply_text(_status_text())
            return

        if action == "local":
            if len(args) < 2 or args[1].strip().lower() not in {"on", "off"}:
                await message.reply_text("⚠️ Формат: `!vision local on` или `!vision local off`")
                return
            enabled = args[1].strip().lower() == "on"
            perceptor.local_vision_enabled = enabled
            os.environ["LOCAL_VISION_ENABLED"] = "1" if enabled else "0"
            if config_manager:
                try:
                    config_manager.set("LOCAL_VISION_ENABLED", "1" if enabled else "0")
                except Exception:
                    pass
            await message.reply_text(
                f"✅ Local vision: `{'ON' if enabled else 'OFF'}`\n"
                "_Изменение применено runtime. Для постоянного режима уже записано в config (если доступен)._"
            )
            return

        if action == "model":
            if len(args) < 2:
                await message.reply_text("⚠️ Формат: `!vision model <lm_studio_model_id>`")
                return
            model_id = " ".join(args[1:]).strip()
            perceptor.local_vision_model = model_id
            os.environ["LOCAL_VISION_MODEL"] = model_id
            if config_manager:
                try:
                    config_manager.set("LOCAL_VISION_MODEL", model_id)
                except Exception:
                    pass
            await message.reply_text(
                f"✅ Local vision model закреплён: `{model_id}`\n"
                "_Совет: проверь точный id через `!model scan`._"
            )
            return

        await message.reply_text("⚠️ Формат: `!vision status|local on|local off|model <lm_studio_model_id>`")
        return

    # --- !img / !draw: Генерация изображений ---
    @app.on_message(filters.command(["img", "draw"], prefixes="!"))
    @safe_handler
    async def img_command(client, message: Message):
        """Генерация изображения: !img <описание> (local/cloud + выбор модели)."""
        if not is_authorized(message): return

        image_gen = deps.get("image_gen")
        if not image_gen:
            await message.reply_text("❌ Ошибка: Image Manager не инициализирован.")
            return

        try:
            tokens = shlex.split(message.text or "")
        except ValueError:
            tokens = (message.text or "").split()

        args = tokens[1:] if len(tokens) > 1 else []
        if not args:
            await message.reply_text(
                "🎨 Использование:\n"
                "`!img <промпт>`\n"
                "`!img --model <alias> <промпт>`\n"
                "`!img --local <промпт>` или `!img --cloud <промпт>`\n"
                "`!img models` — список генераторов\n"
                "`!img cost [alias]` — ориентировочная стоимость\n"
                "`!img health` — проверить local/cloud backend\n"
                "`!img default show|local <alias>|cloud <alias>|mode local|cloud|auto`"
            )
            return

        head = args[0].strip().lower()
        if head == "health":
            rows = await image_gen.list_models()
            local_ok = any(r.get("channel") == "local" and r.get("available") for r in rows)
            cloud_ok = any(r.get("channel") == "cloud" and r.get("available") for r in rows)
            defaults = image_gen.get_defaults() if hasattr(image_gen, "get_defaults") else {}
            await message.reply_text(
                "**🩺 Image Health:**\n\n"
                f"• Local backend: {'🟢' if local_ok else '🔴'}\n"
                f"• Cloud backend: {'🟢' if cloud_ok else '🔴'}\n"
                f"• Default local: `{defaults.get('default_local_alias', '-')}`\n"
                f"• Default cloud: `{defaults.get('default_cloud_alias', '-')}`\n"
                f"• Prefer local: `{defaults.get('prefer_local', '-')}`"
            )
            return

        if head == "default":
            if not hasattr(image_gen, "set_default_alias") or not hasattr(image_gen, "set_prefer_mode"):
                await message.reply_text("⚠️ В этой версии image manager нет runtime-настроек дефолтов.")
                return

            if len(args) < 2 or args[1].strip().lower() == "show":
                defaults = image_gen.get_defaults() if hasattr(image_gen, "get_defaults") else {}
                await message.reply_text(
                    "**🎯 Image Defaults:**\n\n"
                    f"• Local: `{defaults.get('default_local_alias', '-')}`\n"
                    f"• Cloud: `{defaults.get('default_cloud_alias', '-')}`\n"
                    f"• Prefer local: `{defaults.get('prefer_local', '-')}`"
                )
                return

            action = args[1].strip().lower()
            config_manager = deps.get("config_manager")

            if action in {"local", "cloud"}:
                if len(args) < 3:
                    await message.reply_text("⚠️ Формат: `!img default local <alias>` или `!img default cloud <alias>`")
                    return
                alias = args[2].strip()
                result = image_gen.set_default_alias(action, alias)
                if not result.get("ok"):
                    await message.reply_text(f"❌ {result.get('error')}")
                    return
                # Сохраняем и в config.yaml, чтобы переживало рестарт.
                if config_manager:
                    key = "IMAGE_DEFAULT_LOCAL_MODEL" if action == "local" else "IMAGE_DEFAULT_CLOUD_MODEL"
                    try:
                        config_manager.set(key, alias)
                    except Exception:
                        pass
                await message.reply_text(
                    f"✅ Default `{action}` model закреплён: `{alias}`\n"
                    f"Теперь: local=`{result.get('default_local_alias')}`, cloud=`{result.get('default_cloud_alias')}`"
                )
                return

            if action == "mode":
                if len(args) < 3:
                    await message.reply_text("⚠️ Формат: `!img default mode local|cloud|auto`")
                    return
                mode = args[2].strip().lower()
                result = image_gen.set_prefer_mode(mode)
                if not result.get("ok"):
                    await message.reply_text(f"❌ {result.get('error')}")
                    return
                if config_manager:
                    prefer_local = "1" if result.get("prefer_local") else "0"
                    try:
                        config_manager.set("IMAGE_PREFER_LOCAL", prefer_local)
                    except Exception:
                        pass
                await message.reply_text(
                    f"✅ Image mode: `{mode}` | prefer_local=`{result.get('prefer_local')}`"
                )
                return

            await message.reply_text("⚠️ Формат: `!img default show|local <alias>|cloud <alias>|mode local|cloud|auto`")
            return

        if head in {"models", "list"}:
            if not hasattr(image_gen, "list_models"):
                await message.reply_text("⚠️ В этой версии image manager нет каталога моделей.")
                return
            rows = await image_gen.list_models()
            lines = ["**🎨 Image Models:**", ""]
            defaults = image_gen.get_defaults() if hasattr(image_gen, "get_defaults") else {}
            def_local = defaults.get("default_local_alias")
            def_cloud = defaults.get("default_cloud_alias")
            for row in rows:
                icon = "🟢" if row.get("available") else "🔴"
                cost = row.get("cost_per_image_usd")
                cost_text = f"~${cost}/img" if cost is not None else "n/a"
                reason = f" ({row.get('reason')})" if row.get("reason") else ""
                alias = row.get("alias")
                marks = []
                if alias == def_local:
                    marks.append("default-local")
                if alias == def_cloud:
                    marks.append("default-cloud")
                marker = f" [{' | '.join(marks)}]" if marks else ""
                lines.append(
                    f"{icon} `{alias}`{marker} — {row.get('title')} | {row.get('channel')}/{row.get('provider')} | {cost_text}{reason}"
                )
            lines.append("\n_Выбор модели:_ `!img --model <alias> <промпт>`")
            lines.append("_Дефолты:_ `!img default show|local <alias>|cloud <alias>|mode local|cloud|auto`")
            await message.reply_text("\n".join(lines))
            return

        if head == "cost":
            if not hasattr(image_gen, "estimate_cost"):
                await message.reply_text("⚠️ В этой версии image manager нет калькулятора стоимости.")
                return
            if len(args) >= 2:
                aliases = [args[1]]
            else:
                aliases = list(getattr(image_gen, "model_specs", {}).keys())
            lines = ["**💸 Image Cost (ориентировочно):**", ""]
            for alias in aliases:
                info = image_gen.estimate_cost(alias, images=1)
                if not info.get("ok"):
                    lines.append(f"- `{alias}`: ❌ {info.get('error')}")
                    continue
                unit = info.get("unit_cost_usd")
                if unit is None:
                    lines.append(f"- `{alias}`: n/a")
                else:
                    lines.append(f"- `{alias}`: ~`${unit}` за изображение")
            await message.reply_text("\n".join(lines))
            return

        model_alias = None
        prefer_local = None
        aspect_ratio = "1:1"
        prompt_tokens: list[str] = []
        idx = 0
        while idx < len(args):
            token = args[idx]
            lowered = token.strip().lower()
            if lowered in {"--model", "-m"} and idx + 1 < len(args):
                model_alias = args[idx + 1].strip()
                idx += 2
                continue
            if lowered == "--local":
                prefer_local = True
                idx += 1
                continue
            if lowered == "--cloud":
                prefer_local = False
                idx += 1
                continue
            if lowered in {"--ar", "--aspect"} and idx + 1 < len(args):
                aspect_ratio = args[idx + 1].strip()
                idx += 2
                continue
            prompt_tokens.append(token)
            idx += 1

        prompt = " ".join(prompt_tokens).strip()
        if not prompt:
            await message.reply_text("❌ Введи описание картинки: `!img котик в космосе`")
            return

        notification = await message.reply_text("🎨 **Генерирую изображение...**")

        if hasattr(image_gen, "generate_with_meta"):
            result = await image_gen.generate_with_meta(
                prompt=prompt,
                aspect_ratio=aspect_ratio,
                model_alias=model_alias,
                prefer_local=prefer_local,
            )
            image_path = result.get("path")
        else:
            result = {"ok": False, "error": "legacy_image_manager"}
            image_path = await image_gen.generate(prompt, aspect_ratio=aspect_ratio)
            if image_path:
                result = {
                    "ok": True,
                    "path": image_path,
                    "model_alias": model_alias or "legacy",
                    "channel": "cloud",
                    "provider": "legacy",
                    "model_id": "legacy",
                    "cost_estimate_usd": None,
                }

        if result.get("ok") and image_path and os.path.exists(image_path):
            await notification.delete()
            cost = result.get("cost_estimate_usd")
            cost_text = f"~`${cost}`" if cost is not None else "n/a"
            caption = (
                f"🎨 **Запрос:** `{prompt}`\\n"
                f"Model: `{result.get('model_alias', '-')}`\\n"
                f"Channel: `{result.get('channel', '-')}` | Provider: `{result.get('provider', '-')}`\\n"
                f"Cost est.: {cost_text}"
            )
            await message.reply_photo(photo=image_path, caption=caption)
            os.remove(image_path)
            return

        details = result.get("details")
        details_text = f"\n{details}" if details else ""
        await notification.edit_text(
            "❌ Не удалось сгенерировать изображение.\\n"
            f"Причина: `{result.get('error', 'unknown')}`{details_text}\\n"
            "_Проверь `!img models` и настройки ключей/workflow._"
        )

    # --- !exec: Python REPL (Owner only, опасная команда) ---
    @app.on_message(filters.command("exec", prefixes="!"))
    @safe_handler
    async def exec_command(client, message: Message):
        """Python REPL: !exec <code> (Owner Only)"""
        if not is_superuser(message):
            logger.warning(
                f"⛔ Unauthorized exec attempt from @{message.from_user.username}"
            )
            return

        if message.chat.type != enums.ChatType.PRIVATE:
            await message.reply_text("⛔ `!exec` разрешен только в личных сообщениях.")
            await _danger_audit(message, "exec", "blocked", "non-private-chat")
            return

        if len(message.command) < 2:
            await message.reply_text("🐍 Введи Python код: `!exec print('hello')`")
            return

        code = message.text.split(" ", 1)[1]
        notification = await message.reply_text("🐍 **Выполняю...**")

        # Перехват stdout
        old_stdout = sys.stdout
        sys.stdout = buffer = StringIO()
        # Контент для REPL (пробрасываем внутренности для отладки)
        exec_globals = {
            "client": client,
            "ctx": client,
            "message": message,
            "msg": message,
            "deps": deps,
            "router": router,
            "mr": router,
            "lms": router,
            "sys": sys,
            "os": os,
            "asyncio": asyncio,
            "logger": logger,
            "traceback": traceback,
        }
        
        try:
            exec(code, exec_globals)  # noqa: S102
            output = buffer.getvalue() or "✅ Выполнено (нет вывода)"
        except Exception as e:
            output = f"❌ {type(e).__name__}: {e}\n{traceback.format_exc()[-500:]}"
        finally:
            sys.stdout = old_stdout

        if len(output) > 4000:
            output = output[:3900] + "\n...[Truncated]..."

        # Очищаем вывод от вложенных бэктиков, которые ломают markdown
        safe_output = strip_backticks_from_content(output)
        await notification.edit_text(f"🐍 **Результат:**\n\n```\n{safe_output}\n```")
        await _danger_audit(message, "exec", "ok", code[:300])

    async def _enqueue_direct_auto_reply_task(client, message: Message) -> None:
        """
        Постановка одной конкретной задачи в очередь/прямой ран.
        Используется и напрямую, и после burst-склейки форвардов.
        """
        chat_id = int(message.chat.id)
        message_id = int(message.id)
        if _is_duplicate_message(chat_id, message_id):
            logger.debug("Skipping duplicate update in auto-reply enqueue", chat_id=chat_id, message_id=message_id)
            return

        async def _runner():
            await _process_auto_reply(client, message, deps)

        if not ai_runtime.queue_enabled:
            logger.debug(
                "auto_reply_logic: очередь выключена, выполняю запрос сразу",
                chat_id=chat_id,
                message_id=message_id,
            )
            await _runner()
            return

        queued_task = ChatQueuedTask(
            chat_id=chat_id,
            message_id=message_id,
            received_at=time.time(),
            priority=0,
            runner=_runner,
        )
        accepted, queue_size = queue_manager.enqueue(queued_task)
        if not accepted:
            if message.chat.type == enums.ChatType.PRIVATE:
                await message.reply_text(
                    "⚠️ Очередь переполнена для этого чата. Подожди пару секунд и повтори."
                )
            return

        queue_manager.ensure_worker(chat_id)
        queue_log_fn = logger.info if message.chat.type == enums.ChatType.PRIVATE else logger.debug
        queue_log_fn(
            "auto_reply_logic: задача поставлена в очередь",
            chat_id=chat_id,
            message_id=message_id,
            queue_size=queue_size,
        )
        now = time.time()
        last_notice = _LAST_BUSY_NOTICE_TS.get(chat_id, 0.0)
        if (
            bool(ai_runtime and ai_runtime.queue_notify_position_enabled)
            and message.chat.type == enums.ChatType.PRIVATE
            and queue_size > 1
            and (now - last_notice) >= AUTO_REPLY_BUSY_NOTICE_SECONDS
        ):
            _LAST_BUSY_NOTICE_TS[chat_id] = now
            try:
                await message.reply_text(f"🧾 Добавил в очередь обработки (позиция: {queue_size}).")
            except Exception:
                pass

    async def _flush_forward_burst(client, burst_key: str) -> None:
        """
        Флашит накопленную пачку форвардов одним заданием.
        """
        await asyncio.sleep(AUTO_REPLY_FORWARD_BURST_WINDOW_SECONDS)
        payload = forward_burst_buffers.pop(burst_key, None)
        if not payload:
            return

        messages = list(payload.get("messages") or [])
        if not messages:
            return
        tail = messages[-max(1, int(AUTO_REPLY_FORWARD_BURST_MAX_ITEMS)) :]
        anchor_message = tail[-1]
        if len(tail) > 1:
            context_text = _compose_forward_burst_context(
                tail[:-1],
                max_items=AUTO_REPLY_FORWARD_BURST_MAX_ITEMS,
            )
            if context_text:
                context_key = f"{int(anchor_message.chat.id)}:{int(anchor_message.id)}"
                _FORWARD_BURST_CONTEXT_MAP[context_key] = context_text
        await _enqueue_direct_auto_reply_task(client, anchor_message)

    async def _enqueue_auto_reply_task(client, message: Message) -> None:
        """
        Унифицированная постановка auto-reply задачи в очередь.
        Для пересланной «пачки» включает короткое окно склейки.
        """
        if _is_forwarded_message(message):
            sender_id = int(getattr(getattr(message, "from_user", None), "id", 0) or 0)
            burst_key = f"{int(message.chat.id)}:{sender_id}"
            state = forward_burst_buffers.get(burst_key)
            if not state:
                state = {"messages": [], "task": None}
                forward_burst_buffers[burst_key] = state
            added = _append_forward_to_burst_state(
                state,
                message,
                max_items=AUTO_REPLY_FORWARD_BURST_MAX_ITEMS,
            )
            if not added:
                logger.debug(
                    "auto_reply_logic: дубликат форварда пропущен в burst-буфере",
                    chat_id=message.chat.id,
                    message_id=message.id,
                    burst_key=burst_key,
                    burst_size=len(state.get("messages") or []),
                )
                return
            pending_task = state.get("task")
            if pending_task and not pending_task.done():
                pending_task.cancel()
            state["task"] = asyncio.create_task(_flush_forward_burst(client, burst_key))
            logger.debug(
                "auto_reply_logic: форвард добавлен в burst-буфер",
                chat_id=message.chat.id,
                message_id=message.id,
                burst_key=burst_key,
                burst_size=len(state["messages"]),
                window_sec=AUTO_REPLY_FORWARD_BURST_WINDOW_SECONDS,
            )
            return

        await _enqueue_direct_auto_reply_task(client, message)

    # --- Авто-ответ (самый последний, ловит текст + медиа) ---
    @app.on_message(
        (
            filters.text
            | filters.photo
            | filters.voice
            | filters.audio
            | filters.sticker
            | filters.animation
            | filters.video
            | filters.document
        )
        & ~filters.me
        & ~filters.bot
        ,
        group=9,
    )
    @safe_handler
    async def auto_reply_logic(client, message: Message):
        """
        Умный автоответчик v2 (Omni-channel).
        Делегирует исполнение в _process_auto_reply.
        """
        text = str(getattr(message, "text", "") or "").strip()
        if text.startswith("!"):
            # Команды должны обрабатываться только command-handler'ами.
            return

        if message.chat.type == enums.ChatType.PRIVATE:
            logger.info(
                "auto_reply_logic: получено приватное входящее сообщение",
                chat_id=message.chat.id,
                message_id=message.id,
                sender=getattr(getattr(message, "from_user", None), "username", None),
            )
        await _enqueue_auto_reply_task(client, message)

    @app.on_message(
        filters.private & ~filters.me & ~filters.bot,
        group=8,
    )
    @safe_handler
    async def auto_reply_private_failsafe_logic(client, message: Message):
        """
        Failsafe для личных чатов.
        Если по каким-то причинам основной фильтр не сработал, этот обработчик
        гарантированно подхватит входящее и отправит в общий pipeline.
        """
        text = str(getattr(message, "text", "") or "").strip()
        if text.startswith("!"):
            return

        logger.info(
            "auto_reply_failsafe: получено входящее в личке",
            chat_id=message.chat.id,
            message_id=message.id,
            sender=getattr(getattr(message, "from_user", None), "username", None),
        )
        await _enqueue_auto_reply_task(client, message)

    @app.on_message(
        (
            filters.text
            | filters.photo
            | filters.voice
            | filters.audio
            | filters.sticker
            | filters.animation
            | filters.video
            | filters.document
        )
        & filters.me
        & ~filters.bot
        ,
        group=9,
    )
    @safe_handler
    async def auto_reply_self_private_logic(client, message: Message):
        """
        Автоответ в «чате с собой» (self private).
        Нужен для сценария, когда владелец пишет в личку самому аккаунту
        и ожидает обычный AI-ответ без префикса команды.
        """
        if not AUTO_REPLY_SELF_PRIVATE_ENABLED:
            return
        if not _is_self_private_message(message):
            return

        text = str(getattr(message, "text", "") or "").strip()
        if text.startswith("!"):
            # Команды обрабатываются отдельными command-handler'ами.
            return

        # Защита от зацикливания: сообщения-ответы, созданные самим ботом,
        # всегда являются reply и не должны повторно идти в auto-reply.
        if getattr(message, "reply_to_message", None):
            reply_from = getattr(message.reply_to_message, "from_user", None)
            if reply_from and bool(getattr(reply_from, "is_self", False)):
                return

        chat_id = int(message.chat.id)
        message_id = int(message.id)
        if _is_duplicate_message(chat_id, message_id):
            return

        async def _runner():
            await _process_auto_reply(client, message, deps)

        if not ai_runtime.queue_enabled:
            await _runner()
            return

        queued_task = ChatQueuedTask(
            chat_id=chat_id,
            message_id=message_id,
            received_at=time.time(),
            priority=0,
            runner=_runner,
        )
        accepted, queue_size = queue_manager.enqueue(queued_task)
        if not accepted:
            await message.reply_text("⚠️ Очередь переполнена для self-чата. Подожди пару секунд и повтори.")
            return

        queue_manager.ensure_worker(chat_id)
        now = time.time()
        last_notice = _LAST_BUSY_NOTICE_TS.get(chat_id, 0.0)
        if (
            bool(ai_runtime and ai_runtime.queue_notify_position_enabled)
            and queue_size > 1
            and (now - last_notice) >= AUTO_REPLY_BUSY_NOTICE_SECONDS
        ):
            _LAST_BUSY_NOTICE_TS[chat_id] = now
            try:
                await message.reply_text(f"🧾 Добавил в очередь обработки (позиция: {queue_size}).")
            except Exception:
                pass
