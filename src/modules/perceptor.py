# -*- coding: utf-8 -*-
"""Multimodal Module

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14nwK0xjjNETf0WZaRHpDGGjPzDs4reBf
"""

# -*- coding: utf-8 -*-
"""
Perceptor Module (Eyes & Ears).
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –∞—É–¥–∏–æ –∏ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
–£–º–µ–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å HEIC -> JPG –Ω–∞ –ª–µ—Ç—É.
"""

import os
import asyncio
import logging
import time
import uuid
import base64
import mimetypes
import edge_tts
from typing import Dict, Any, Optional
from io import BytesIO
from PIL import Image
from pillow_heif import register_heif_opener
from pathlib import Path
import aiohttp

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É HEIC –¥–ª—è Pillow
register_heif_opener()

logger = logging.getLogger(__name__)

# Gemini SDK (New v1.0+)
try:
    from google import genai
    from google.genai import types
    _GENAI_AVAILABLE = True
except ImportError:
    _GENAI_AVAILABLE = False
    genai = None

class Perceptor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # Whisper-–º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –≤–º–µ—Å—Ç–æ —Ö–∞—Ä–¥–∫–æ–¥–∞
        self.whisper_model = config.get("WHISPER_MODEL", os.getenv("WHISPER_MODEL", "mlx-community/whisper-large-v3-turbo"))
        # Vision-–º–æ–¥–µ–ª—å –∏–∑ .env (—É–±—Ä–∞–ª–∏ —Ö–∞—Ä–¥–∫–æ–¥ gemini-2.0-flash)
        self.vision_model = os.getenv("GEMINI_VISION_MODEL", "gemini-2.0-flash")
        # –õ–æ–∫–∞–ª—å–Ω—ã–π vision —á–µ—Ä–µ–∑ LM Studio (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).
        self.local_vision_enabled = str(
            os.getenv("LOCAL_VISION_ENABLED", str(config.get("LOCAL_VISION_ENABLED", "0")))
        ).strip().lower() in {"1", "true", "yes", "on"}
        self.local_vision_model = str(
            os.getenv("LOCAL_VISION_MODEL", str(config.get("LOCAL_VISION_MODEL", "")))
        ).strip()
        self.local_vision_timeout_seconds = float(
            os.getenv(
                "LOCAL_VISION_TIMEOUT_SECONDS",
                str(config.get("LOCAL_VISION_TIMEOUT_SECONDS", "90")),
            )
        )
        self.local_vision_max_tokens = int(
            os.getenv(
                "LOCAL_VISION_MAX_TOKENS",
                str(config.get("LOCAL_VISION_MAX_TOKENS", "1200")),
            )
        )
        
        self.gemini_key = config.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
        # MLX Whisper –º–æ–∂–µ—Ç –ø–∞–¥–∞—Ç—å –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö (Metal/AGX assert),
        # –ø–æ—ç—Ç–æ–º—É —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤—Å–µ STT-–∑–∞–¥–∞—á–∏ –æ–¥–Ω–∏–º –ª–æ–∫–æ–º.
        self._transcribe_lock = asyncio.Lock()
        
        logger.info(f"üëÇ Perceptor initialized. Audio: {self.whisper_model}, Vision: {self.vision_model}")
        
        # Warmup MLX
        self._warmup_audio()

    def _warmup_audio(self):
        """–ü—Ä–æ–≥—Ä–µ–≤ Neural Engine –¥–ª—è MLX Whisper."""
        try:
            import mlx_whisper
            import numpy as np
            logger.info("üî• Warming up Neural Engine (MLX)...")
            # –ü—Ä–æ–≥—Ä–µ–≤ —Ç–∏—à–∏–Ω–æ–π
            mlx_whisper.transcribe(np.zeros(16000, dtype=np.float32), path_or_hf_repo=self.whisper_model)
            logger.info("‚úÖ MLX Audio Model Ready.")
        except Exception as e:
            logger.warning(f"MLX Warmup Skipped: {e}")

    async def transcribe(self, file_path: str, router) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é MLX Whisper (Local).
        """
        try:
            async with self._transcribe_lock:
                logger.info(f"üé§ Transcribing: {file_path}")
                import mlx_whisper

                start_time = time.time()

                # –ü—Ä–æ–º–ø—Ç-–ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è Whisper
                punctuation_prompt = "–ü—Ä–∏–≤–µ—Ç, —è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π, –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º."

                # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ executor, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop (MLX —Ç—è–∂–µ–ª—ã–π)
                # –•–æ—Ç—è mlx_whisper –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω, –ª—É—á—à–µ –ø–µ—Ä–µ—Å—Ç—Ä–∞—Ö–æ–≤–∞—Ç—å—Å—è
                result = await asyncio.to_thread(
                    mlx_whisper.transcribe,
                    file_path,
                    path_or_hf_repo=self.whisper_model,
                    initial_prompt=punctuation_prompt,
                    language="ru",       # –§–æ—Ä—Å–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π
                    temperature=0.0,     # –£–±–∏—Ä–∞–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
                    verbose=False
                )

                text = result.get("text", "").strip()
                duration = time.time() - start_time

                logger.info(f"‚úÖ Transcribed in {duration:.2f}s: {text[:50]}...")
                return text

        except Exception as e:
            logger.error(f"‚ùå Local Transcription Failed: {e}")
            return f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {e}"

    async def analyze_image(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HEIC.
        """
        converted_path = file_path
        start_time = time.time()

        try:
            logger.info(f"üì∏ Starting Vision Analysis: {file_path}")
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if str(file_path).lower().endswith(".heic"):
                img = Image.open(file_path)
                converted_path = f"{file_path}.jpg"
                img.save(converted_path, format="JPEG")
                logger.info(f"Converted HEIC to JPG: {converted_path}")

            # 2. Vision Request via Gemini SDK
            if self.local_vision_enabled:
                local_result = await self._analyze_image_local_lm_studio(
                    file_path=converted_path,
                    router=router,
                    prompt=prompt,
                )
                if local_result.get("ok"):
                    duration = time.time() - start_time
                    logger.info(
                        "‚úÖ Local Vision (LM Studio) success in %.2fs (model=%s)",
                        duration,
                        local_result.get("model", "-"),
                    )
                    return str(local_result.get("text", "")).strip()
                logger.warning("‚ö†Ô∏è Local Vision failed, fallback to Gemini: %s", local_result.get("error"))

            if not _GENAI_AVAILABLE:
                logger.error("‚ùå Google GenAI SDK not found.")
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."
                
            api_key = (router.gemini_key if hasattr(router, 'gemini_key') else None) or self.gemini_key
            if not api_key:
                logger.error("‚ùå Gemini API Key missing.")
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API."

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
            client = genai.Client(api_key=api_key)
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ PIL
            img_pil = Image.open(converted_path)
            
            logger.info(f"üì° Sending Vision Request to {self.vision_model}...")
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø–æ—Ç–æ–∫–µ, —Ç–∞–∫ –∫–∞–∫ SDK –±–ª–æ–∫–∏—Ä—É—é—â–∏–π
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, img_pil]
            )
            
            if response and response.text:
                 duration = time.time() - start_time
                 logger.info(f"‚úÖ Vision Success in {duration:.2f}s")
                 return response.text
            
            logger.warning("‚ö†Ô∏è Gemini returned empty text.")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç)."

        except Exception as e:
            logger.error(f"‚ùå Vision error: {e}", exc_info=True)
            return f"–Ø –æ—Å–ª–µ–ø, –ü–æ. –û—à–∏–±–∫–∞: {e}"
        finally:
            # –ß–∏—Å—Ç–∏–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω —Å–æ–∑–¥–∞–≤–∞–ª—Å—è
            if converted_path != file_path and os.path.exists(converted_path):
                try:
                    os.remove(converted_path)
                except Exception:
                    pass

    async def analyze_visual(self, file_path: str, prompt: str) -> str:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è —Å–∫—Ä–∏–Ω—à–æ—Ç—ã).
        """
        start_time = time.time()
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API."

            client = genai.Client(api_key=api_key)
            img_pil = Image.open(file_path)
            
            logger.info(f"üì° Sending Visual Analysis request ({self.vision_model})...")
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, img_pil]
            )
            
            duration = time.time() - start_time
            if response and response.text:
                logger.info(f"‚úÖ Visual Analysis success in {duration:.2f}s")
                return response.text
            
            return "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."

        except Exception as e:
            logger.error(f"‚ùå Visual analysis error: {e}", exc_info=True)
            return f"–û—à–∏–±–∫–∞ –∑—Ä–µ–Ω–∏—è: {e}"

    def _resolve_local_vision_model(self, router) -> str:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é vision-–º–æ–¥–µ–ª—å:
        1) LOCAL_VISION_MODEL
        2) active_local_model —Ä–æ—É—Ç–µ—Ä–∞
        3) LOCAL_PREFERRED_MODEL —Ä–æ—É—Ç–µ—Ä–∞
        """
        if self.local_vision_model:
            return self.local_vision_model

        active_model = str(getattr(router, "active_local_model", "") or "").strip()
        if active_model:
            return active_model

        preferred_model = str(getattr(router, "local_preferred_model", "") or "").strip()
        if preferred_model:
            return preferred_model

        return ""

    def _build_image_data_url(self, file_path: str) -> str:
        """–ö–æ–¥–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ data-url –¥–ª—è OpenAI-compatible API LM Studio."""
        mime_type = mimetypes.guess_type(file_path)[0] or "image/jpeg"
        image_bytes = Path(file_path).read_bytes()
        encoded = base64.b64encode(image_bytes).decode("ascii")
        return f"data:{mime_type};base64,{encoded}"

    def _extract_lm_studio_vision_text(self, payload: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ message.content."""
        choices = payload.get("choices") if isinstance(payload, dict) else None
        if not isinstance(choices, list) or not choices:
            return ""
        message = choices[0].get("message", {}) if isinstance(choices[0], dict) else {}
        content = message.get("content")
        if isinstance(content, str):
            return content.strip()
        if isinstance(content, list):
            parts: list[str] = []
            for item in content:
                if not isinstance(item, dict):
                    continue
                if item.get("type") == "text":
                    text = str(item.get("text", "")).strip()
                    if text:
                        parts.append(text)
            return "\n".join(parts).strip()
        return ""

    async def _analyze_image_local_lm_studio(self, file_path: str, router, prompt: str) -> Dict[str, Any]:
        """
        –õ–æ–∫–∞–ª—å–Ω—ã–π vision-–∑–∞–ø—Ä–æ—Å –≤ LM Studio —á–µ—Ä–µ–∑ OpenAI-compatible /chat/completions.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {"ok": bool, "text": "...", "error": "...", "model": "..."}.
        """
        model_name = self._resolve_local_vision_model(router)
        if not model_name:
            return {"ok": False, "error": "local_vision_model_not_set", "model": ""}

        lm_base = str(getattr(router, "lm_studio_url", "") or "").rstrip("/")
        if not lm_base:
            lm_base = str(os.getenv("LM_STUDIO_URL", "http://127.0.0.1:1234/v1")).rstrip("/")
            if "/v1" not in lm_base:
                lm_base = f"{lm_base}/v1"
        chat_url = f"{lm_base}/chat/completions"

        data_url = self._build_image_data_url(file_path)
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": data_url}},
                    ],
                }
            ],
            "temperature": 0.2,
            "max_tokens": max(200, int(self.local_vision_max_tokens)),
        }

        timeout = aiohttp.ClientTimeout(total=max(10, int(self.local_vision_timeout_seconds)))
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(chat_url, json=payload) as resp:
                    if resp.status != 200:
                        body = await resp.text()
                        return {
                            "ok": False,
                            "error": f"lmstudio_http_{resp.status}:{body[:240]}",
                            "model": model_name,
                        }
                    data = await resp.json()
        except Exception as exc:
            return {"ok": False, "error": f"lmstudio_vision_request_failed:{exc}", "model": model_name}

        text = self._extract_lm_studio_vision_text(data)
        if not text:
            return {"ok": False, "error": "lmstudio_vision_empty_response", "model": model_name}
        return {"ok": True, "text": text, "model": model_name}

    async def analyze_video(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ-—Å–æ–æ–±—â–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –∫—Ä—É–∂–∫–∏) —á–µ—Ä–µ–∑ Gemini 2.0 Flash.
        """
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = router.gemini_key or self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ."

            client = genai.Client(api_key=api_key)
            
            logger.info(f"üéûÔ∏è Uploading video to Gemini: {file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            video_file = await asyncio.to_thread(
                client.files.upload,
                path=file_path
            )
            
            # –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            while True:
                # Polling file status
                # Using to_thread because client methods might be blocking
                file_info = await asyncio.to_thread(client.files.get, name=video_file.name)
                
                # Check status (Assuming 'ACTIVE' or 'PROCESSING')
                # In new SDK, state is an enum or string.
                state = str(file_info.state)
                
                if "ACTIVE" in state:
                    break
                elif "FAILED" in state:
                    raise Exception("Google Video Processing failed.")
                
                logger.info(f"Video processing... {state}")
                await asyncio.sleep(2)

            logger.info(f"‚úÖ Video processing complete: {video_file.name}")
            
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, video_file]
            )
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            await asyncio.to_thread(client.files.delete, name=video_file.name)
            
            return response.text if response else "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø–æ –≤–∏–¥–µ–æ."

        except Exception as e:
            logger.error(f"Video analysis error: {e}")
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤–∏–¥–µ–æ: {e}"

    async def analyze_document(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç (PDF) —á–µ—Ä–µ–∑ Gemini Native Document Understanding.
        """
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = router.gemini_key or self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞."

            client = genai.Client(api_key=api_key)
            
            logger.info(f"üìÑ Uploading document to Gemini: {file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            doc_file = await asyncio.to_thread(
                client.files.upload,
                path=file_path
            )
            
            # –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            while True:
                file_info = await asyncio.to_thread(client.files.get, name=doc_file.name)
                state = str(file_info.state)
                
                if "ACTIVE" in state:
                    break
                elif "FAILED" in state:
                    raise Exception("Google Document Processing failed.")
                
                logger.info(f"Document processing... {state}")
                await asyncio.sleep(2)

            logger.info(f"‚úÖ Document processing complete: {doc_file.name}")
            
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, doc_file]
            )
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –∏–∑ –æ–±–ª–∞–∫–∞
            await asyncio.to_thread(client.files.delete, name=doc_file.name)
            
            return response.text if response else "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É."

        except Exception as e:
            logger.error(f"Document analysis error: {e}")
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}"

    def _clean_text_for_tts(self, text: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–µ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –æ–∑–≤—É—á–∫–æ–π.
        """
        import re
        # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ –∫–æ—Ä–æ–±–æ–∫ –∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
        text = re.sub(r'<\|begin_of_box\|>|<\|end_of_box\|>', '', text)
        text = re.sub(r'<\|thought\|>.*?</\|thought\|>', '', text, flags=re.DOTALL)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\(\)]', '', text)
        text = re.sub(r'http\S+', '', text)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    async def speak(self, text: str, voice: str = "ru-RU-SvetlanaNeural", method: str = "auto") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (TTS).
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç edge-tts (–±–µ—Å–ø–ª–∞—Ç–Ω–æ) –∏ OpenAI (–ø–ª–∞—Ç–Ω–æ, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ .ogg —Ñ–∞–π–ª—É.
        """
        file_id = str(uuid.uuid4())
        os.makedirs("artifacts/downloads", exist_ok=True)
        
        mp3_path = f"artifacts/downloads/{file_id}.mp3"
        ogg_path = f"artifacts/downloads/{file_id}.ogg"
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ –æ–∑–≤—É—á–∫–æ–π
        clean_text = self._clean_text_for_tts(text)
        if not clean_text:
            logger.warning("TTS text is empty after cleaning. Skipping speech synthesis.")
            return None

        # 1. –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞
        use_openai = False
        if method == "openai":
            use_openai = True
        elif method == "auto":
            # –í –±—É–¥—É—â–µ–º —Ç—É—Ç –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –≤—ã–±–æ—Ä–∞
            pass

        try:
            if use_openai:
                return await self._speak_openai(clean_text, mp3_path, ogg_path)
            
            # –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ edge-tts
            res = await self._speak_edge(clean_text, voice, mp3_path, ogg_path)
            if res:
                return res
            
            # Fallback –Ω–∞ openai –µ—Å–ª–∏ edge-tts —É–ø–∞–ª
            logger.warning("‚ö†Ô∏è edge-tts failed, falling back to OpenAI TTS...")
            return await self._speak_openai(clean_text, mp3_path, ogg_path)

        except Exception as e:
            logger.error(f"TTS Master Error: {e}")
            return None

    async def _speak_edge(self, text: str, voice: str, mp3_path: str, ogg_path: str) -> Optional[str]:
        """–û–∑–≤—É—á–∫–∞ —á–µ—Ä–µ–∑ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π edge-tts."""
        try:
            # –ï—Å–ª–∏ –≥–æ–ª–æ—Å –Ω–µ —É–∫–∞–∑–∞–Ω –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ—Ç macOS, –º–µ–Ω—è–µ–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π
            if voice in ["Milena", "Yuri", "Katya", "default"]:
                voice = "ru-RU-SvetlanaNeural"

            logger.info(f"üó£Ô∏è Speaking via edge-tts: {text[:40]}... (Voice: {voice})")
            communicate = edge_tts.Communicate(text, voice)
            await communicate.save(mp3_path)

            if not os.path.exists(mp3_path) or os.path.getsize(mp3_path) < 100:
                return None
            
            return await self._convert_to_ogg(mp3_path, ogg_path)
        except Exception as e:
            logger.error(f"edge-tts error: {e}")
            return None

    async def _speak_openai(self, text: str, mp3_path: str, ogg_path: str) -> Optional[str]:
        """–û–∑–≤—É—á–∫–∞ —á–µ—Ä–µ–∑ –ø–ª–∞—Ç–Ω—ã–π OpenAI API."""
        try:
            from openai import AsyncOpenAI
            api_key = os.getenv("OPENAI_API_KEY") or self.gemini_key # –ò–Ω–æ–≥–¥–∞ –∫–ª—é—á–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã –≤ —à–ª—é–∑–∞—Ö
            if not api_key or "sk-" not in api_key:
                logger.error("‚ùå OpenAI API Key missing for TTS fallback.")
                return None

            logger.info(f"üéôÔ∏è Speaking via OpenAI TTS: {text[:40]}...")
            client = AsyncOpenAI(api_key=api_key)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å tts-1 (–±—ã—Å—Ç—Ä–∞—è) –∏ –≥–æ–ª–æ—Å nova/shimmer
            response = await client.audio.speech.create(
                model="tts-1",
                voice="nova",
                input=text
            )
            await asyncio.to_thread(response.stream_to_file, mp3_path)
            
            return await self._convert_to_ogg(mp3_path, ogg_path)
        except Exception as e:
            logger.error(f"OpenAI TTS error: {e}")
            return None

    async def _convert_to_ogg(self, mp3_path: str, ogg_path: str) -> Optional[str]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è MP3 -> OGG Opus —á–µ—Ä–µ–∑ ffmpeg."""
        try:
            proc_ffmpeg = await asyncio.create_subprocess_exec(
                "ffmpeg", "-i", mp3_path, "-c:a", "libopus", "-b:a", "24k", "-vbr", "on", "-y", ogg_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await proc_ffmpeg.communicate()
            
            if os.path.exists(mp3_path):
                os.remove(mp3_path)
            
            if os.path.exists(ogg_path) and os.path.getsize(ogg_path) > 200:
                logger.info(f"‚úÖ TTS Generated: {ogg_path}")
                return ogg_path
            return None
        except Exception as e:
            logger.error(f"FFmpeg error: {e}")
            return None
