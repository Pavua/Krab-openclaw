# -*- coding: utf-8 -*-
"""Multimodal Module

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14nwK0xjjNETf0WZaRHpDGGjPzDs4reBf
"""

# -*- coding: utf-8 -*-
"""
Perceptor Module (Eyes & Ears).
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –∞—É–¥–∏–æ –∏ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
–£–º–µ–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å HEIC -> JPG –Ω–∞ –ª–µ—Ç—É.
"""

import os
import logging
from typing import Dict, Any
from io import BytesIO
from PIL import Image
from pillow_heif import register_heif_opener
from pathlib import Path

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É HEIC –¥–ª—è Pillow
register_heif_opener()

logger = logging.getLogger(__name__)

class Perceptor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # M4 Max Optimization: Use mlx-community/whisper-large-v3-turbo
        self.whisper_model = "mlx-community/whisper-large-v3-turbo"
        logger.info(f"üëÇ Perceptor initialized. Audio Model: {self.whisper_model}")
        
        # Warmup MLX
        self._warmup_audio()

    def _warmup_audio(self):
        """–ü—Ä–æ–≥—Ä–µ–≤ Neural Engine –¥–ª—è MLX Whisper."""
        try:
            import mlx_whisper
            import numpy as np
            logger.info("üî• Warming up Neural Engine (MLX)...")
            # –ü—Ä–æ–≥—Ä–µ–≤ —Ç–∏—à–∏–Ω–æ–π
            mlx_whisper.transcribe(np.zeros(16000, dtype=np.float32), path_or_hf_repo=self.whisper_model)
            logger.info("‚úÖ MLX Audio Model Ready.")
        except Exception as e:
            logger.warning(f"MLX Warmup Skipped: {e}")

    async def transcribe(self, file_path: str, router) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é MLX Whisper (Local).
        """
        try:
            logger.info(f"üé§ Transcribing: {file_path}")
            import mlx_whisper
            import time

            start_time = time.time()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞/–∏–Ω–∏—Ç–∞
            # –ü—Ä–æ–º–ø—Ç-–ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è Whisper (—É–ª—É—á—à–∞–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å)
            # –í–∑—è—Ç–æ –∏–∑ KrabEar v4.7 (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
            punctuation_prompt = "–ü—Ä–∏–≤–µ—Ç, —è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π, –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
            
            result = mlx_whisper.transcribe(
                file_path, 
                path_or_hf_repo=self.whisper_model,
                initial_prompt=punctuation_prompt,
                language="ru",       # –§–æ—Ä—Å–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π
                temperature=0.0,     # –£–±–∏—Ä–∞–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
                verbose=False
            )
            
            text = result.get("text", "").strip()
            duration = time.time() - start_time
            
            logger.info(f"‚úÖ Transcribed in {duration:.2f}s: {text[:50]}...")
            return text

        except Exception as e:
            logger.error(f"‚ùå Local Transcription Failed: {e}")
            return f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {e}"

    async def analyze_image(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HEIC.
        """
        converted_path = file_path

        try:
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if str(file_path).lower().endswith(".heic"):
                img = Image.open(file_path)
                converted_path = f"{file_path}.jpg"
                img.save(converted_path, format="JPEG")
                logger.info(f"Converted HEIC to JPG: {converted_path}")

            # 2. –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ –†–æ—É—Ç–µ—Ä (Vision Request)
            # Vision –∑–∞–¥–∞—á–∏ –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ä–µ—à–∞–µ—Ç Cloud Gemini (Pro Vision) –∏–ª–∏ LLaVA (Local)
            # –ü–µ—Ä–µ–¥–∞–µ–º –≤ router –ø—É—Ç—å –∫ –∫–∞—Ä—Ç–∏–Ω–∫–µ

            # –í–ù–ò–ú–ê–ù–ò–ï: –ó–¥–µ—Å—å router –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å –ø—Ä–∏–Ω–∏–º–∞—Ç—å image_path.
            # –ú—ã –≤—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ router.route_query —Å —Ç–∏–ø–æ–º 'vision'
            # (–≠—Ç–æ—Ç —Ç–∏–ø –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ router, –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º chat –∑–∞–≥–ª—É—à–∫—É)

            # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Gemini –Ω–∞–ø—Ä—è–º—É—é (—Ç–∞–∫ –∫–∞–∫ Router –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ—Ä–∞–±–æ—Ç–∫–∏)
            import google.generativeai as genai

            if not router.gemini_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API."

            genai.configure(api_key=router.gemini_key)
            model = genai.GenerativeModel('gemini-2.0-flash') # Vision model

            cookie_picture = Image.open(converted_path)
            response = model.generate_content([prompt, cookie_picture])
            return response.text

        except Exception as e:
            logger.error(f"Vision error: {e}")
            return f"–Ø –æ—Å–ª–µ–ø, –ü–æ. –û—à–∏–±–∫–∞: {e}"
        finally:
            # –ß–∏—Å—Ç–∏–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω —Å–æ–∑–¥–∞–≤–∞–ª—Å—è
            if converted_path != file_path and os.path.exists(converted_path):
                os.remove(converted_path)

    async def speak(self, text: str, voice: str = "Milena") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (TTS) —á–µ—Ä–µ–∑ macOS 'say' + ffmpeg.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ .ogg —Ñ–∞–π–ª—É.
        """
        import asyncio
        import uuid
        
        file_id = str(uuid.uuid4())
        # Ensure dir exists
        os.makedirs("artifacts/downloads", exist_ok=True)
        
        aiff_path = f"artifacts/downloads/{file_id}.aiff"
        ogg_path = f"artifacts/downloads/{file_id}.ogg"
        
        try:
            logger.info(f"üó£Ô∏è Speaking: {text[:30]}... (Voice: {voice})")
            
            # 1. Generate AIFF
            proc_say = await asyncio.create_subprocess_exec(
                "say", "-v", voice, "-o", aiff_path, text,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await proc_say.communicate()

            if proc_say.returncode != 0:
                logger.error(f"TTS (say) failed: {stderr.decode().strip()}")
                return None
            
            if not os.path.exists(aiff_path):
                logger.error(f"TTS (say) completed but file missing: {aiff_path}")
                return None
            
            # 2. Convert to OGG (Voice Note format)
            proc_ffmpeg = await asyncio.create_subprocess_exec(
                "ffmpeg", "-i", aiff_path, "-c:a", "libopus", "-b:a", "24k", "-y", ogg_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await proc_ffmpeg.communicate()
            
            if proc_ffmpeg.returncode != 0:
                 logger.error(f"ffmpeg conversion failed: {stderr.decode().strip()}")
                 # Don't return None yet, maybe AIFF is useful? No, Telegram needs OGG/MP3 for voice usually.
                 return None

            if os.path.exists(aiff_path):
                 os.remove(aiff_path)
            
            if os.path.exists(ogg_path):
                logger.info(f"‚úÖ TTS Generated: {ogg_path}")
                return ogg_path
            return None
            
        except Exception as e:
            logger.error(f"TTS Error: {e}")
            return None