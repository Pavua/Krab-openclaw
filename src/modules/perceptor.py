# -*- coding: utf-8 -*-
"""Multimodal Module

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14nwK0xjjNETf0WZaRHpDGGjPzDs4reBf
"""

# -*- coding: utf-8 -*-
"""
Perceptor Module (Eyes & Ears).
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –∞—É–¥–∏–æ –∏ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
–£–º–µ–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å HEIC -> JPG –Ω–∞ –ª–µ—Ç—É.
"""

import os
import asyncio
import logging
import time
import uuid
import base64
import mimetypes
import re
import json
import threading
import subprocess
import sys
import edge_tts
from typing import Dict, Any, Optional
from io import BytesIO
from PIL import Image
from pillow_heif import register_heif_opener
from pathlib import Path
import aiohttp

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É HEIC –¥–ª—è Pillow
register_heif_opener()

logger = logging.getLogger(__name__)

# Gemini SDK (New v1.0+)
try:
    from google import genai
    from google.genai import types
    _GENAI_AVAILABLE = True
except ImportError:
    _GENAI_AVAILABLE = False
    genai = None

class Perceptor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # Whisper-–º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –≤–º–µ—Å—Ç–æ —Ö–∞—Ä–¥–∫–æ–¥–∞
        self.whisper_model = config.get("WHISPER_MODEL", os.getenv("WHISPER_MODEL", "mlx-community/whisper-large-v3-turbo"))
        # Vision-–º–æ–¥–µ–ª—å –∏–∑ .env (—É–±—Ä–∞–ª–∏ —Ö–∞—Ä–¥–∫–æ–¥ gemini-2.0-flash)
        self.vision_model = os.getenv("GEMINI_VISION_MODEL", "gemini-2.0-flash")
        # –õ–æ–∫–∞–ª—å–Ω—ã–π vision —á–µ—Ä–µ–∑ LM Studio (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).
        self.local_vision_enabled = str(
            os.getenv("LOCAL_VISION_ENABLED", str(config.get("LOCAL_VISION_ENABLED", "0")))
        ).strip().lower() in {"1", "true", "yes", "on"}
        self.local_vision_model = str(
            os.getenv("LOCAL_VISION_MODEL", str(config.get("LOCAL_VISION_MODEL", "")))
        ).strip()
        self.local_vision_timeout_seconds = float(
            os.getenv(
                "LOCAL_VISION_TIMEOUT_SECONDS",
                str(config.get("LOCAL_VISION_TIMEOUT_SECONDS", "90")),
            )
        )
        self.local_vision_max_tokens = int(
            os.getenv(
                "LOCAL_VISION_MAX_TOKENS",
                str(config.get("LOCAL_VISION_MAX_TOKENS", "1200")),
            )
        )
        
        self.gemini_key = config.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
        self.stt_language = str(
            config.get("STT_LANGUAGE", os.getenv("STT_LANGUAGE", "ru"))
        ).strip() or "ru"
        self.stt_temperature = float(
            config.get("STT_TEMPERATURE", os.getenv("STT_TEMPERATURE", "0.0"))
        )
        self.stt_beam_size = int(
            config.get("STT_BEAM_SIZE", os.getenv("STT_BEAM_SIZE", "7"))
        )
        self.stt_best_of = int(
            config.get("STT_BEST_OF", os.getenv("STT_BEST_OF", "5"))
        )
        self.stt_patience = float(
            config.get("STT_PATIENCE", os.getenv("STT_PATIENCE", "1.0"))
        )
        self.stt_condition_on_previous_text = str(
            config.get(
                "STT_CONDITION_ON_PREVIOUS_TEXT",
                os.getenv("STT_CONDITION_ON_PREVIOUS_TEXT", "1"),
            )
        ).strip().lower() in {"1", "true", "yes", "on"}
        self.stt_no_speech_threshold = float(
            config.get(
                "STT_NO_SPEECH_THRESHOLD",
                os.getenv("STT_NO_SPEECH_THRESHOLD", "0.45"),
            )
        )
        self.stt_compression_ratio_threshold = float(
            config.get(
                "STT_COMPRESSION_RATIO_THRESHOLD",
                os.getenv("STT_COMPRESSION_RATIO_THRESHOLD", "2.4"),
            )
        )
        self.stt_hotwords = self._parse_hotwords(
            config.get("STT_HOTWORDS", os.getenv("STT_HOTWORDS", ""))
        )
        self.stt_replace_map = self._parse_replace_map(
            config.get("STT_REPLACE_JSON", os.getenv("STT_REPLACE_JSON", ""))
        )
        # MLX Whisper –º–æ–∂–µ—Ç –ø–∞–¥–∞—Ç—å –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö (Metal/AGX assert),
        # –ø–æ—ç—Ç–æ–º—É —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤—Å–µ STT-–∑–∞–¥–∞—á–∏ –æ–¥–Ω–∏–º –ª–æ–∫–æ–º.
        self._transcribe_lock = asyncio.Lock()
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞: –∑–∞–ø—É—Å–∫ STT –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ.
        # –ï—Å–ª–∏ MLX/Metal –ø–∞–¥–∞–µ—Ç (SIGABRT), —É–ø–∞–¥–µ—Ç —Ç–æ–ª—å–∫–æ –≤–æ—Ä–∫–µ—Ä, –∞ –Ω–µ –≤–µ—Å—å Krab.
        isolated_default = "1"
        if "PYTEST_CURRENT_TEST" in os.environ:
            # –í —Ç–µ—Å—Ç–∞—Ö –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Å—Ç–∞–≤–ª—è–µ–º in-process –ø—É—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –º–æ–∫–æ–≤.
            isolated_default = "0"
        self.stt_isolated_worker = str(
            config.get(
                "STT_ISOLATED_WORKER",
                os.getenv("STT_ISOLATED_WORKER", isolated_default),
            )
        ).strip().lower() in {"1", "true", "yes", "on"}
        self.stt_worker_timeout_seconds = int(
            config.get(
                "STT_WORKER_TIMEOUT_SECONDS",
                os.getenv("STT_WORKER_TIMEOUT_SECONDS", "240"),
            )
        )
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç vision –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.
        self.last_vision_meta: Dict[str, Any] = {
            "route": "unknown",
            "model": "",
            "fallback_used": False,
            "local_enabled": bool(self.local_vision_enabled),
            "error": "",
        }
        
        logger.info(f"üëÇ Perceptor initialized. Audio: {self.whisper_model}, Vision: {self.vision_model}")

        # Warmup MLX –≤ —Ñ–æ–Ω–µ.
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω, —Ç.–∫. –Ω–∞ —á–∞—Å—Ç–∏ macOS/MLX-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        # –ø—Ä–æ–≥—Ä–µ–≤ –º–æ–∂–µ—Ç –∞–≤–∞—Ä–∏–π–Ω–æ –∑–∞–≤–µ—Ä—à–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å (AGX assert) –¥–æ —Å—Ç–∞—Ä—Ç–∞ Web API.
        warmup_enabled = str(
            config.get("PERCEPTOR_AUDIO_WARMUP", os.getenv("PERCEPTOR_AUDIO_WARMUP", "0"))
        ).strip().lower() in {"1", "true", "yes", "on"}
        if warmup_enabled:
            threading.Thread(
                target=self._warmup_audio,
                name="perceptor-mlx-warmup",
                daemon=True,
            ).start()
        else:
            logger.info("Perceptor MLX warmup disabled by PERCEPTOR_AUDIO_WARMUP=0")

    def _warmup_audio(self):
        """–ü—Ä–æ–≥—Ä–µ–≤ Neural Engine –¥–ª—è MLX Whisper."""
        try:
            import mlx_whisper
            import numpy as np
            logger.info("üî• Warming up Neural Engine (MLX)...")
            # –ü—Ä–æ–≥—Ä–µ–≤ —Ç–∏—à–∏–Ω–æ–π
            mlx_whisper.transcribe(np.zeros(16000, dtype=np.float32), path_or_hf_repo=self.whisper_model)
            logger.info("‚úÖ MLX Audio Model Ready.")
        except Exception as e:
            logger.warning(f"MLX Warmup Skipped: {e}")

    async def transcribe(self, file_path: str, router) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é MLX Whisper (Local).
        """
        try:
            async with self._transcribe_lock:
                logger.info(f"üé§ Transcribing: {file_path}")

                start_time = time.time()

                if self.stt_isolated_worker:
                    result = await self._run_mlx_transcribe_isolated(file_path=file_path)
                else:
                    import mlx_whisper
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ executor, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop (MLX —Ç—è–∂–µ–ª—ã–π).
                    # –ß–∞—Å—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π mlx_whisper,
                    # –ø–æ—ç—Ç–æ–º—É —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å, –∑–∞—Ç–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π fallback.
                    result = await self._run_mlx_transcribe_with_fallback(
                        mlx_whisper=mlx_whisper,
                        file_path=file_path,
                    )

                worker_error = str(result.get("_worker_error", "")).strip()
                if worker_error:
                    raise RuntimeError(worker_error)

                raw_text = str(result.get("text", "")).strip()
                text = self._postprocess_transcript(raw_text)
                duration = time.time() - start_time

                logger.info(f"‚úÖ Transcribed in {duration:.2f}s: {text[:50]}...")
                return text

        except Exception as e:
            logger.error(f"‚ùå Local Transcription Failed: {e}")
            return f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {e}"

    def _parse_hotwords(self, raw: Any) -> list[str]:
        """–ü–∞—Ä—Å–∏—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è STT –∏–∑ —Å—Ç—Ä–æ–∫–∏ 'a,b,c'."""
        text = str(raw or "").strip()
        if not text:
            return []
        return [token.strip() for token in text.split(",") if token.strip()]

    def _parse_replace_map(self, raw: Any) -> dict[str, str]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å–ª–æ–≤–∞—Ä—å –∑–∞–º–µ–Ω —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ JSON-—Å—Ç—Ä–æ–∫–∏.
        –ü—Ä–∏–º–µ—Ä:
        {"–¥–∂–∏–º–µ–Ω–∏":"Gemini","–æ—É–ø–µ–Ω –∫–ª–æ":"OpenClaw"}
        """
        text = str(raw or "").strip()
        if not text:
            return {}
        try:
            data = json.loads(text)
        except Exception:
            logger.warning("STT_REPLACE_JSON is invalid JSON; skip custom replacements")
            return {}
        if not isinstance(data, dict):
            return {}
        cleaned: dict[str, str] = {}
        for key, value in data.items():
            src = str(key or "").strip()
            dst = str(value or "").strip()
            if src and dst:
                cleaned[src] = dst
        return cleaned

    def _build_stt_prompt(self) -> str:
        """–°–æ–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤."""
        base_prompt = (
            "–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–π —Ä—É—Å—Å–∫—É—é —Ä–µ—á—å —Ç–æ—á–Ω–æ, —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π, "
            "–∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏ –∏ –∞–±–∑–∞—Ü–∞–º–∏ –ø–æ —Å–º—ã—Å–ª—É. "
            "–ù–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤ –æ—Ç —Å–µ–±—è."
        )
        if self.stt_hotwords:
            hotwords_block = ", ".join(self.stt_hotwords[:40])
            return f"{base_prompt} –í–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {hotwords_block}."
        return base_prompt

    def _build_primary_stt_kwargs(self) -> dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞."""
        return {
            "initial_prompt": self._build_stt_prompt(),
            "language": self.stt_language,
            "temperature": self.stt_temperature,
            "beam_size": self.stt_beam_size,
            "best_of": self.stt_best_of,
            "patience": self.stt_patience,
            "condition_on_previous_text": self.stt_condition_on_previous_text,
            "no_speech_threshold": self.stt_no_speech_threshold,
            "compression_ratio_threshold": self.stt_compression_ratio_threshold,
            "verbose": False,
        }

    def _build_fallback_stt_kwargs(self) -> dict[str, Any]:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å –±–æ–ª–µ–µ —Å—Ç–∞—Ä—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏."""
        return {
            "initial_prompt": self._build_stt_prompt(),
            "language": self.stt_language,
            "temperature": self.stt_temperature,
            "verbose": False,
        }

    async def _run_mlx_transcribe_isolated(self, file_path: str) -> dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç MLX Whisper –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ.
        –≠—Ç–æ –∑–∞—â–∏—â–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å Krab –æ—Ç –∞–≤–∞—Ä–∏–π Metal/AGX (SIGABRT).
        """
        payload = {
            "primary_kwargs": self._build_primary_stt_kwargs(),
            "fallback_kwargs": self._build_fallback_stt_kwargs(),
        }
        cmd = [
            sys.executable,
            "-m",
            "src.modules.perceptor_stt_worker",
            file_path,
            self.whisper_model,
            json.dumps(payload, ensure_ascii=False),
        ]

        try:
            completed = await asyncio.to_thread(
                subprocess.run,
                cmd,
                capture_output=True,
                text=True,
                timeout=max(30, self.stt_worker_timeout_seconds),
                check=False,
            )
        except subprocess.TimeoutExpired:
            return {"_worker_error": f"stt_worker_timeout:{self.stt_worker_timeout_seconds}s"}
        except Exception as exc:
            return {"_worker_error": f"stt_worker_spawn_failed:{exc}"}

        if completed.returncode != 0:
            stderr = (completed.stderr or "").strip()
            stdout = (completed.stdout or "").strip()
            reason = stderr or stdout or f"exit_code={completed.returncode}"
            return {"_worker_error": f"stt_worker_failed:{reason}"}

        output = (completed.stdout or "").strip()
        if not output:
            return {"_worker_error": "stt_worker_failed:empty_stdout"}
        last_line = output.splitlines()[-1]
        try:
            payload_obj = json.loads(last_line)
        except Exception:
            return {"_worker_error": f"stt_worker_failed:invalid_json:{last_line[:200]}"}
        if not isinstance(payload_obj, dict):
            return {"_worker_error": "stt_worker_failed:invalid_payload_type"}
        if payload_obj.get("ok") is False:
            return {"_worker_error": str(payload_obj.get("error", "stt_worker_failed:unknown"))}
        return {"text": str(payload_obj.get("text", "")).strip()}

    async def _run_mlx_transcribe_with_fallback(self, mlx_whisper: Any, file_path: str) -> dict[str, Any]:
        """–ü—Ä–æ–±—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π STT-–ø—Ä–æ—Ñ–∏–ª—å, –ø—Ä–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Ç–∫–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –±–∞–∑–æ–≤—ã–π."""
        primary_kwargs = self._build_primary_stt_kwargs()
        try:
            return await asyncio.to_thread(
                mlx_whisper.transcribe,
                file_path,
                path_or_hf_repo=self.whisper_model,
                **primary_kwargs,
            )
        except TypeError as exc:
            logger.warning("mlx_whisper does not support full STT args, fallback to basic profile: %s", exc)
            return await asyncio.to_thread(
                mlx_whisper.transcribe,
                file_path,
                path_or_hf_repo=self.whisper_model,
                **self._build_fallback_stt_kwargs(),
            )

    def _apply_custom_replacements(self, text: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω STT_REPLACE_JSON).
        –ó–∞–º–µ–Ω—è–µ—Ç –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º —Å–ª–æ–≤ –≤ —Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–º —Ä–µ–∂–∏–º–µ.
        """
        if not self.stt_replace_map or not text:
            return text
        fixed = text
        for raw_src, raw_dst in self.stt_replace_map.items():
            src = re.escape(raw_src)
            fixed = re.sub(rf"(?<!\w){src}(?!\w)", raw_dst, fixed, flags=re.IGNORECASE)
        return fixed

    def _capitalize_sentences(self, text: str) -> str:
        """–ü–æ–¥–Ω–∏–º–∞–µ—Ç –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        if not text:
            return text
        chars = list(text)
        need_upper = True
        for idx, ch in enumerate(chars):
            if need_upper and ch.isalpha():
                chars[idx] = ch.upper()
                need_upper = False
            if ch in ".!?":
                need_upper = True
            elif not ch.isspace() and ch not in "\"'¬´¬ª()[]{}":
                # –í–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã –≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –Ω–µ —Ñ–æ—Ä—Å–∏—Ä—É–µ–º.
                need_upper = False
        return "".join(chars)

    def _postprocess_transcript(self, raw_text: str) -> str:
        """
        –î–µ—Ç–µ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:
        - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤/–ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏,
        - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∞—é—â–µ–≥–æ –∑–Ω–∞–∫–∞,
        - –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π,
        - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–º–µ–Ω—ã —Ç–µ—Ä–º–∏–Ω–æ–≤.
        """
        text = str(raw_text or "").strip()
        if not text:
            return ""

        # –î–ª—è –∫–æ–º–∞–Ω–¥–Ω—ã—Ö –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ (!, /) –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –Ω–µ –≤–º–µ—à–∏–≤–∞–µ–º.
        if text.startswith(("!", "/")):
            return text

        text = re.sub(r"\s+", " ", text)
        text = re.sub(r"\s+([,.;:!?])", r"\1", text)
        text = re.sub(r"([,.;:!?])(?=[^\s\"')\]¬ª}])", r"\1 ", text)
        text = re.sub(r"([!?.,])\1{2,}", r"\1", text)
        text = text.strip()

        if len(text.split()) >= 5 and text[-1] not in ".!?":
            text += "."

        text = self._capitalize_sentences(text)
        text = self._apply_custom_replacements(text)
        return text

    async def analyze_image(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HEIC.
        """
        converted_path = file_path
        start_time = time.time()
        self.last_vision_meta = {
            "route": "unknown",
            "model": "",
            "fallback_used": False,
            "local_enabled": bool(self.local_vision_enabled),
            "error": "",
        }

        try:
            logger.info(f"üì∏ Starting Vision Analysis: {file_path}")
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if str(file_path).lower().endswith(".heic"):
                img = Image.open(file_path)
                converted_path = f"{file_path}.jpg"
                img.save(converted_path, format="JPEG")
                logger.info(f"Converted HEIC to JPG: {converted_path}")

            # 2. Vision Request via Gemini SDK
            if self.local_vision_enabled:
                local_result = await self._analyze_image_local_lm_studio(
                    file_path=converted_path,
                    router=router,
                    prompt=prompt,
                )
                if local_result.get("ok"):
                    duration = time.time() - start_time
                    self.last_vision_meta = {
                        "route": "local_lm_studio",
                        "model": str(local_result.get("model") or ""),
                        "fallback_used": False,
                        "local_enabled": True,
                        "error": "",
                    }
                    logger.info(
                        "‚úÖ Local Vision (LM Studio) success in %.2fs (model=%s)",
                        duration,
                        local_result.get("model", "-"),
                    )
                    return str(local_result.get("text", "")).strip()
                self.last_vision_meta = {
                    "route": "cloud_gemini",
                    "model": str(self.vision_model or ""),
                    "fallback_used": True,
                    "local_enabled": True,
                    "error": str(local_result.get("error") or ""),
                }
                logger.warning("‚ö†Ô∏è Local Vision failed, fallback to Gemini: %s", local_result.get("error"))

            if not _GENAI_AVAILABLE:
                logger.error("‚ùå Google GenAI SDK not found.")
                self.last_vision_meta = {
                    "route": "error",
                    "model": "",
                    "fallback_used": bool(self.local_vision_enabled),
                    "local_enabled": bool(self.local_vision_enabled),
                    "error": "google_genai_sdk_missing",
                }
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."
                
            api_key = (router.gemini_key if hasattr(router, 'gemini_key') else None) or self.gemini_key
            if not api_key:
                logger.error("‚ùå Gemini API Key missing.")
                self.last_vision_meta = {
                    "route": "error",
                    "model": "",
                    "fallback_used": bool(self.local_vision_enabled),
                    "local_enabled": bool(self.local_vision_enabled),
                    "error": "gemini_api_key_missing",
                }
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API."

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
            client = genai.Client(api_key=api_key)
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ PIL
            img_pil = Image.open(converted_path)
            
            logger.info(f"üì° Sending Vision Request to {self.vision_model}...")
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø–æ—Ç–æ–∫–µ, —Ç–∞–∫ –∫–∞–∫ SDK –±–ª–æ–∫–∏—Ä—É—é—â–∏–π
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, img_pil]
            )
            
            if response and response.text:
                 duration = time.time() - start_time
                 self.last_vision_meta = {
                     "route": "cloud_gemini",
                     "model": str(self.vision_model or ""),
                     "fallback_used": bool(self.local_vision_enabled),
                     "local_enabled": bool(self.local_vision_enabled),
                     "error": "",
                 }
                 logger.info(f"‚úÖ Vision Success in {duration:.2f}s")
                 return response.text
            
            logger.warning("‚ö†Ô∏è Gemini returned empty text.")
            self.last_vision_meta = {
                "route": "cloud_gemini",
                "model": str(self.vision_model or ""),
                "fallback_used": bool(self.local_vision_enabled),
                "local_enabled": bool(self.local_vision_enabled),
                "error": "gemini_empty_response",
            }
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç)."

        except Exception as e:
            logger.error(f"‚ùå Vision error: {e}", exc_info=True)
            self.last_vision_meta = {
                "route": "error",
                "model": "",
                "fallback_used": bool(self.local_vision_enabled),
                "local_enabled": bool(self.local_vision_enabled),
                "error": str(e),
            }
            return f"–Ø –æ—Å–ª–µ–ø, –ü–æ. –û—à–∏–±–∫–∞: {e}"
        finally:
            # –ß–∏—Å—Ç–∏–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω —Å–æ–∑–¥–∞–≤–∞–ª—Å—è
            if converted_path != file_path and os.path.exists(converted_path):
                try:
                    os.remove(converted_path)
                except Exception:
                    pass

    def get_last_vision_meta(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ vision-–∑–∞–ø—Ä–æ—Å–∞."""
        return dict(self.last_vision_meta)

    async def analyze_visual(self, file_path: str, prompt: str) -> str:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è —Å–∫—Ä–∏–Ω—à–æ—Ç—ã).
        """
        start_time = time.time()
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API."

            client = genai.Client(api_key=api_key)
            img_pil = Image.open(file_path)
            
            logger.info(f"üì° Sending Visual Analysis request ({self.vision_model})...")
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, img_pil]
            )
            
            duration = time.time() - start_time
            if response and response.text:
                logger.info(f"‚úÖ Visual Analysis success in {duration:.2f}s")
                return response.text
            
            return "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."

        except Exception as e:
            logger.error(f"‚ùå Visual analysis error: {e}", exc_info=True)
            return f"–û—à–∏–±–∫–∞ –∑—Ä–µ–Ω–∏—è: {e}"

    def _resolve_local_vision_model(self, router) -> str:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é vision-–º–æ–¥–µ–ª—å:
        1) LOCAL_VISION_MODEL
        2) active_local_model —Ä–æ—É—Ç–µ—Ä–∞
        3) LOCAL_PREFERRED_MODEL —Ä–æ—É—Ç–µ—Ä–∞
        """
        if self.local_vision_model:
            return self.local_vision_model

        active_model = str(getattr(router, "active_local_model", "") or "").strip()
        if active_model:
            return active_model

        preferred_model = str(getattr(router, "local_preferred_model", "") or "").strip()
        if preferred_model:
            return preferred_model

        return ""

    def _resolve_lm_studio_http_base(self, router) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–π HTTP URL LM Studio –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ endpoint'–æ–≤.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º—ã:
        - http://host:1234
        - http://host:1234/v1
        - ws://host:1234/v1/chat/completions
        """
        lm_raw = str(getattr(router, "lm_studio_url", "") or "").strip()
        if not lm_raw:
            lm_raw = str(os.getenv("LM_STUDIO_URL", "http://127.0.0.1:1234/v1")).strip()
        lm_raw = lm_raw.rstrip("/")
        if lm_raw.startswith("ws://"):
            lm_raw = "http://" + lm_raw[len("ws://") :]
        elif lm_raw.startswith("wss://"):
            lm_raw = "https://" + lm_raw[len("wss://") :]

        suffixes = (
            "/v1/chat/completions",
            "/chat/completions",
            "/api/v1",
            "/v1",
        )
        lowered = lm_raw.lower()
        for suffix in suffixes:
            if lowered.endswith(suffix):
                lm_raw = lm_raw[: -len(suffix)]
                lowered = lm_raw.lower()
        return lm_raw.rstrip("/")

    def _extract_model_entries(self, payload: Any) -> list[dict[str, Any]]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ—Ç–≤–µ—Ç –∫–∞—Ç–∞–ª–æ–≥–∞ –º–æ–¥–µ–ª–µ–π LM Studio –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π."""
        if isinstance(payload, list):
            return [item for item in payload if isinstance(item, dict)]
        if not isinstance(payload, dict):
            return []

        for key in ("models", "data", "items", "result"):
            candidate = payload.get(key)
            if isinstance(candidate, list):
                return [item for item in candidate if isinstance(item, dict)]
        return []

    async def _collect_lm_studio_models(self, router) -> list[dict[str, Any]]:
        """
        –ß–∏—Ç–∞–µ—Ç –∫–∞—Ç–∞–ª–æ–≥ –º–æ–¥–µ–ª–µ–π LM Studio.
        –ï—Å–ª–∏ endpoint –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω/—Ñ–æ—Ä–º–∞—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
        """
        base = self._resolve_lm_studio_http_base(router)
        if not base:
            return []
        endpoints = [
            f"{base}/api/v1/models",
            f"{base}/v1/models",
        ]
        timeout = aiohttp.ClientTimeout(total=6)
        for endpoint in endpoints:
            try:
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(endpoint) as resp:
                        if resp.status != 200:
                            continue
                        payload = await resp.json(content_type=None)
                        entries = self._extract_model_entries(payload)
                        if entries:
                            return entries
            except Exception:
                continue
        return []

    def _find_model_entry(self, entries: list[dict[str, Any]], model_name: str) -> Optional[dict[str, Any]]:
        """–ò—â–µ—Ç –∑–∞–ø–∏—Å—å –º–æ–¥–µ–ª–∏ –ø–æ key/id/name/model –≤ –∫–∞—Ç–∞–ª–æ–≥–µ LM Studio."""
        target = str(model_name or "").strip().lower()
        if not target:
            return None
        for entry in entries:
            for key in ("key", "id", "model", "name"):
                candidate = str(entry.get(key) or "").strip().lower()
                if not candidate:
                    continue
                if candidate == target:
                    return entry
        for entry in entries:
            for key in ("key", "id", "model", "name"):
                candidate = str(entry.get(key) or "").strip().lower()
                if not candidate:
                    continue
                if target in candidate or candidate in target:
                    return entry
        return None

    def _infer_vision_support_from_entry(
        self,
        entry: Optional[dict[str, Any]],
        model_name: str,
    ) -> tuple[Optional[bool], str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É vision.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (True|False|None, reason), –≥–¥–µ None = –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å.
        """
        lower_name = str(model_name or "").strip().lower()
        explicit_true_tokens = {"vision", "image", "image_input", "multimodal", "vlm", "mm"}
        explicit_text_tokens = {"text", "text_only", "text-only"}

        # 1) –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–º–µ–Ω–∏ (–ø–æ–ª–µ–∑–Ω–∞, –∫–æ–≥–¥–∞ –∫–∞—Ç–∞–ª–æ–≥ –±–µ–¥–Ω—ã–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º).
        if any(token in lower_name for token in ("glm-4.6v", "glm-4v", "vision", "vlm", "llava", "qwen2-vl", "internvl", "minicpm-v", "phi-3.5-vision")):
            return True, "name_hint"

        if not isinstance(entry, dict):
            return None, "catalog_entry_missing"

        for key in ("vision", "supports_vision", "image_input", "multimodal"):
            if key in entry and isinstance(entry.get(key), bool):
                return bool(entry.get(key)), f"field:{key}"

        capabilities = entry.get("capabilities")
        if isinstance(capabilities, dict):
            for key in ("vision", "image", "image_input", "multimodal"):
                if key in capabilities and isinstance(capabilities.get(key), bool):
                    return bool(capabilities.get(key)), f"capability:{key}"
            normalized_keys = {str(k).strip().lower() for k in capabilities.keys()}
            if normalized_keys.intersection(explicit_true_tokens):
                return True, "capability_keys"

        def _normalize_tokens(value: Any) -> set[str]:
            if isinstance(value, str):
                return {token for token in re.split(r"[\s,;/|]+", value.strip().lower()) if token}
            if isinstance(value, list):
                tokens: set[str] = set()
                for item in value:
                    if isinstance(item, str):
                        tokens.update(_normalize_tokens(item))
                    elif isinstance(item, dict):
                        for item_key in ("name", "id", "type", "kind"):
                            raw = item.get(item_key)
                            if isinstance(raw, str):
                                tokens.update(_normalize_tokens(raw))
                return tokens
            return set()

        modality_tokens = _normalize_tokens(entry.get("modalities")) | _normalize_tokens(entry.get("modality"))
        capability_tokens = _normalize_tokens(capabilities)
        domain_tokens = _normalize_tokens(entry.get("domain")) | _normalize_tokens(entry.get("type"))
        all_tokens = modality_tokens | capability_tokens | domain_tokens

        if all_tokens.intersection(explicit_true_tokens):
            return True, "token_hint"
        if all_tokens and all_tokens.issubset(explicit_text_tokens):
            return False, "text_only_tokens"

        return None, "unknown_capabilities"

    async def _check_local_vision_support(self, router, model_name: str) -> dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –≤—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å local vision.
        supported:
        - True  -> –º–æ–¥–µ–ª—å —è–≤–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision
        - False -> –º–æ–¥–µ–ª—å —è–≤–Ω–æ text-only
        - None  -> –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å (–ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–ø—ã—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞)
        """
        entries = await self._collect_lm_studio_models(router)
        entry = self._find_model_entry(entries, model_name)
        supported, reason = self._infer_vision_support_from_entry(entry, model_name)
        return {
            "supported": supported,
            "reason": reason,
            "model": model_name,
            "catalog_entries": len(entries),
            "entry_found": bool(entry),
        }

    def _build_image_data_url(self, file_path: str) -> str:
        """–ö–æ–¥–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ data-url –¥–ª—è OpenAI-compatible API LM Studio."""
        mime_type = mimetypes.guess_type(file_path)[0] or "image/jpeg"
        image_bytes = Path(file_path).read_bytes()
        encoded = base64.b64encode(image_bytes).decode("ascii")
        return f"data:{mime_type};base64,{encoded}"

    def _extract_lm_studio_vision_text(self, payload: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ message.content."""
        choices = payload.get("choices") if isinstance(payload, dict) else None
        if not isinstance(choices, list) or not choices:
            return ""
        message = choices[0].get("message", {}) if isinstance(choices[0], dict) else {}
        content = message.get("content")
        if isinstance(content, str):
            return content.strip()
        if isinstance(content, list):
            parts: list[str] = []
            for item in content:
                if not isinstance(item, dict):
                    continue
                if item.get("type") == "text":
                    text = str(item.get("text", "")).strip()
                    if text:
                        parts.append(text)
            return "\n".join(parts).strip()
        return ""

    async def _analyze_image_local_lm_studio(self, file_path: str, router, prompt: str) -> Dict[str, Any]:
        """
        –õ–æ–∫–∞–ª—å–Ω—ã–π vision-–∑–∞–ø—Ä–æ—Å –≤ LM Studio —á–µ—Ä–µ–∑ OpenAI-compatible /chat/completions.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {"ok": bool, "text": "...", "error": "...", "model": "..."}.
        """
        model_name = self._resolve_local_vision_model(router)
        if not model_name:
            return {"ok": False, "error": "local_vision_model_not_set", "model": ""}

        precheck = await self._check_local_vision_support(router, model_name)
        if precheck.get("supported") is False:
            return {
                "ok": False,
                "error": f"local_model_not_vision_capability:{precheck.get('reason', 'text_only')}",
                "model": model_name,
                "precheck": precheck,
            }

        lm_base = self._resolve_lm_studio_http_base(router)
        if not lm_base:
            return {"ok": False, "error": "lmstudio_base_url_missing", "model": model_name}
        chat_url = f"{lm_base}/v1/chat/completions"

        data_url = self._build_image_data_url(file_path)
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": data_url}},
                    ],
                }
            ],
            "temperature": 0.2,
            "max_tokens": max(200, int(self.local_vision_max_tokens)),
        }

        timeout = aiohttp.ClientTimeout(total=max(10, int(self.local_vision_timeout_seconds)))
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(chat_url, json=payload) as resp:
                    if resp.status != 200:
                        body = await resp.text()
                        return {
                            "ok": False,
                            "error": f"lmstudio_http_{resp.status}:{body[:240]}",
                            "model": model_name,
                            "precheck": precheck,
                        }
                    data = await resp.json()
        except Exception as exc:
            return {
                "ok": False,
                "error": f"lmstudio_vision_request_failed:{exc}",
                "model": model_name,
                "precheck": precheck,
            }

        text = self._extract_lm_studio_vision_text(data)
        if not text:
            return {
                "ok": False,
                "error": "lmstudio_vision_empty_response",
                "model": model_name,
                "precheck": precheck,
            }
        return {"ok": True, "text": text, "model": model_name, "precheck": precheck}

    async def analyze_video(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ-—Å–æ–æ–±—â–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –∫—Ä—É–∂–∫–∏) —á–µ—Ä–µ–∑ Gemini 2.0 Flash.
        """
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = router.gemini_key or self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ."

            client = genai.Client(api_key=api_key)
            
            logger.info(f"üéûÔ∏è Uploading video to Gemini: {file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            video_file = await asyncio.to_thread(
                client.files.upload,
                path=file_path
            )
            
            # –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            while True:
                # Polling file status
                # Using to_thread because client methods might be blocking
                file_info = await asyncio.to_thread(client.files.get, name=video_file.name)
                
                # Check status (Assuming 'ACTIVE' or 'PROCESSING')
                # In new SDK, state is an enum or string.
                state = str(file_info.state)
                
                if "ACTIVE" in state:
                    break
                elif "FAILED" in state:
                    raise Exception("Google Video Processing failed.")
                
                logger.info(f"Video processing... {state}")
                await asyncio.sleep(2)

            logger.info(f"‚úÖ Video processing complete: {video_file.name}")
            
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, video_file]
            )
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            await asyncio.to_thread(client.files.delete, name=video_file.name)
            
            return response.text if response else "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø–æ –≤–∏–¥–µ–æ."

        except Exception as e:
            logger.error(f"Video analysis error: {e}")
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤–∏–¥–µ–æ: {e}"

    async def analyze_document(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç (PDF) —á–µ—Ä–µ–∑ Gemini Native Document Understanding.
        """
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = router.gemini_key or self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞."

            client = genai.Client(api_key=api_key)
            
            logger.info(f"üìÑ Uploading document to Gemini: {file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            doc_file = await asyncio.to_thread(
                client.files.upload,
                path=file_path
            )
            
            # –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            while True:
                file_info = await asyncio.to_thread(client.files.get, name=doc_file.name)
                state = str(file_info.state)
                
                if "ACTIVE" in state:
                    break
                elif "FAILED" in state:
                    raise Exception("Google Document Processing failed.")
                
                logger.info(f"Document processing... {state}")
                await asyncio.sleep(2)

            logger.info(f"‚úÖ Document processing complete: {doc_file.name}")
            
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, doc_file]
            )
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –∏–∑ –æ–±–ª–∞–∫–∞
            await asyncio.to_thread(client.files.delete, name=doc_file.name)
            
            return response.text if response else "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É."

        except Exception as e:
            logger.error(f"Document analysis error: {e}")
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}"

    def _clean_text_for_tts(self, text: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–µ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –æ–∑–≤—É—á–∫–æ–π.
        """
        import re
        # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ –∫–æ—Ä–æ–±–æ–∫ –∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
        text = re.sub(r'<\|begin_of_box\|>|<\|end_of_box\|>', '', text)
        text = re.sub(r'<\|thought\|>.*?</\|thought\|>', '', text, flags=re.DOTALL)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\(\)]', '', text)
        text = re.sub(r'http\S+', '', text)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    async def speak(self, text: str, voice: str = "ru-RU-SvetlanaNeural", method: str = "auto") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (TTS).
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç edge-tts (–±–µ—Å–ø–ª–∞—Ç–Ω–æ) –∏ OpenAI (–ø–ª–∞—Ç–Ω–æ, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ .ogg —Ñ–∞–π–ª—É.
        """
        file_id = str(uuid.uuid4())
        os.makedirs("artifacts/downloads", exist_ok=True)
        
        mp3_path = f"artifacts/downloads/{file_id}.mp3"
        ogg_path = f"artifacts/downloads/{file_id}.ogg"
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ –æ–∑–≤—É—á–∫–æ–π
        clean_text = self._clean_text_for_tts(text)
        if not clean_text:
            logger.warning("TTS text is empty after cleaning. Skipping speech synthesis.")
            return None

        # 1. –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞
        use_openai = False
        if method == "openai":
            use_openai = True
        elif method == "auto":
            # –í –±—É–¥—É—â–µ–º —Ç—É—Ç –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –≤—ã–±–æ—Ä–∞
            pass

        try:
            if use_openai:
                return await self._speak_openai(clean_text, mp3_path, ogg_path)
            
            # –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ edge-tts
            res = await self._speak_edge(clean_text, voice, mp3_path, ogg_path)
            if res:
                return res
            
            # Fallback –Ω–∞ openai –µ—Å–ª–∏ edge-tts —É–ø–∞–ª
            logger.warning("‚ö†Ô∏è edge-tts failed, falling back to OpenAI TTS...")
            return await self._speak_openai(clean_text, mp3_path, ogg_path)

        except Exception as e:
            logger.error(f"TTS Master Error: {e}")
            return None

    async def _speak_edge(self, text: str, voice: str, mp3_path: str, ogg_path: str) -> Optional[str]:
        """–û–∑–≤—É—á–∫–∞ —á–µ—Ä–µ–∑ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π edge-tts."""
        try:
            # –ï—Å–ª–∏ –≥–æ–ª–æ—Å –Ω–µ —É–∫–∞–∑–∞–Ω –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ—Ç macOS, –º–µ–Ω—è–µ–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π
            if voice in ["Milena", "Yuri", "Katya", "default"]:
                voice = "ru-RU-SvetlanaNeural"

            logger.info(f"üó£Ô∏è Speaking via edge-tts: {text[:40]}... (Voice: {voice})")
            communicate = edge_tts.Communicate(text, voice)
            await communicate.save(mp3_path)

            if not os.path.exists(mp3_path) or os.path.getsize(mp3_path) < 100:
                return None
            
            return await self._convert_to_ogg(mp3_path, ogg_path)
        except Exception as e:
            logger.error(f"edge-tts error: {e}")
            return None

    async def _speak_openai(self, text: str, mp3_path: str, ogg_path: str) -> Optional[str]:
        """–û–∑–≤—É—á–∫–∞ —á–µ—Ä–µ–∑ –ø–ª–∞—Ç–Ω—ã–π OpenAI API."""
        try:
            from openai import AsyncOpenAI
            api_key = os.getenv("OPENAI_API_KEY") or self.gemini_key # –ò–Ω–æ–≥–¥–∞ –∫–ª—é—á–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã –≤ —à–ª—é–∑–∞—Ö
            if not api_key or "sk-" not in api_key:
                logger.error("‚ùå OpenAI API Key missing for TTS fallback.")
                return None

            logger.info(f"üéôÔ∏è Speaking via OpenAI TTS: {text[:40]}...")
            client = AsyncOpenAI(api_key=api_key)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å tts-1 (–±—ã—Å—Ç—Ä–∞—è) –∏ –≥–æ–ª–æ—Å nova/shimmer
            response = await client.audio.speech.create(
                model="tts-1",
                voice="nova",
                input=text
            )
            await asyncio.to_thread(response.stream_to_file, mp3_path)
            
            return await self._convert_to_ogg(mp3_path, ogg_path)
        except Exception as e:
            logger.error(f"OpenAI TTS error: {e}")
            return None

    async def _convert_to_ogg(self, mp3_path: str, ogg_path: str) -> Optional[str]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è MP3 -> OGG Opus —á–µ—Ä–µ–∑ ffmpeg."""
        try:
            proc_ffmpeg = await asyncio.create_subprocess_exec(
                "ffmpeg", "-i", mp3_path, "-c:a", "libopus", "-b:a", "24k", "-vbr", "on", "-y", ogg_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await proc_ffmpeg.communicate()
            
            if os.path.exists(mp3_path):
                os.remove(mp3_path)
            
            if os.path.exists(ogg_path) and os.path.getsize(ogg_path) > 200:
                logger.info(f"‚úÖ TTS Generated: {ogg_path}")
                return ogg_path
            return None
        except Exception as e:
            logger.error(f"FFmpeg error: {e}")
            return None
