# -*- coding: utf-8 -*-
"""Multimodal Module

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14nwK0xjjNETf0WZaRHpDGGjPzDs4reBf
"""

# -*- coding: utf-8 -*-
"""
Perceptor Module (Eyes & Ears).
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –∞—É–¥–∏–æ –∏ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
–£–º–µ–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å HEIC -> JPG –Ω–∞ –ª–µ—Ç—É.
"""

import os
import asyncio
import logging
import time
import uuid
import edge_tts
from typing import Dict, Any
from io import BytesIO
from PIL import Image
from pillow_heif import register_heif_opener
from pathlib import Path

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É HEIC –¥–ª—è Pillow
register_heif_opener()

logger = logging.getLogger(__name__)

# Gemini SDK (New v1.0+)
try:
    from google import genai
    from google.genai import types
    _GENAI_AVAILABLE = True
except ImportError:
    _GENAI_AVAILABLE = False
    genai = None

class Perceptor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # Whisper-–º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –≤–º–µ—Å—Ç–æ —Ö–∞—Ä–¥–∫–æ–¥–∞
        self.whisper_model = config.get("WHISPER_MODEL", os.getenv("WHISPER_MODEL", "mlx-community/whisper-large-v3-turbo"))
        # Vision-–º–æ–¥–µ–ª—å –∏–∑ .env (—É–±—Ä–∞–ª–∏ —Ö–∞—Ä–¥–∫–æ–¥ gemini-2.0-flash)
        self.vision_model = os.getenv("GEMINI_VISION_MODEL", "gemini-2.0-flash")
        
        self.gemini_key = config.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
        
        logger.info(f"üëÇ Perceptor initialized. Audio: {self.whisper_model}, Vision: {self.vision_model}")
        
        # Warmup MLX
        self._warmup_audio()

    def _warmup_audio(self):
        """–ü—Ä–æ–≥—Ä–µ–≤ Neural Engine –¥–ª—è MLX Whisper."""
        try:
            import mlx_whisper
            import numpy as np
            logger.info("üî• Warming up Neural Engine (MLX)...")
            # –ü—Ä–æ–≥—Ä–µ–≤ —Ç–∏—à–∏–Ω–æ–π
            mlx_whisper.transcribe(np.zeros(16000, dtype=np.float32), path_or_hf_repo=self.whisper_model)
            logger.info("‚úÖ MLX Audio Model Ready.")
        except Exception as e:
            logger.warning(f"MLX Warmup Skipped: {e}")

    async def transcribe(self, file_path: str, router) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é MLX Whisper (Local).
        """
        try:
            logger.info(f"üé§ Transcribing: {file_path}")
            import mlx_whisper
            
            start_time = time.time()
            
            # –ü—Ä–æ–º–ø—Ç-–ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è Whisper
            punctuation_prompt = "–ü—Ä–∏–≤–µ—Ç, —è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π, –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ executor, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop (MLX —Ç—è–∂–µ–ª—ã–π)
            # –•–æ—Ç—è mlx_whisper –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω, –ª—É—á—à–µ –ø–µ—Ä–µ—Å—Ç—Ä–∞—Ö–æ–≤–∞—Ç—å—Å—è
            result = await asyncio.to_thread(
                mlx_whisper.transcribe,
                file_path, 
                path_or_hf_repo=self.whisper_model,
                initial_prompt=punctuation_prompt,
                language="ru",       # –§–æ—Ä—Å–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–π
                temperature=0.0,     # –£–±–∏—Ä–∞–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
                verbose=False
            )
            
            text = result.get("text", "").strip()
            duration = time.time() - start_time
            
            logger.info(f"‚úÖ Transcribed in {duration:.2f}s: {text[:50]}...")
            return text

        except Exception as e:
            logger.error(f"‚ùå Local Transcription Failed: {e}")
            return f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {e}"

    async def analyze_image(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HEIC.
        """
        converted_path = file_path
        start_time = time.time()

        try:
            logger.info(f"üì∏ Starting Vision Analysis: {file_path}")
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if str(file_path).lower().endswith(".heic"):
                img = Image.open(file_path)
                converted_path = f"{file_path}.jpg"
                img.save(converted_path, format="JPEG")
                logger.info(f"Converted HEIC to JPG: {converted_path}")

            # 2. Vision Request via Gemini SDK
            if not _GENAI_AVAILABLE:
                logger.error("‚ùå Google GenAI SDK not found.")
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."
                
            api_key = (router.gemini_key if hasattr(router, 'gemini_key') else None) or self.gemini_key
            if not api_key:
                logger.error("‚ùå Gemini API Key missing.")
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API."

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
            client = genai.Client(api_key=api_key)
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ PIL
            img_pil = Image.open(converted_path)
            
            logger.info(f"üì° Sending Vision Request to {self.vision_model}...")
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø–æ—Ç–æ–∫–µ, —Ç–∞–∫ –∫–∞–∫ SDK –±–ª–æ–∫–∏—Ä—É—é—â–∏–π
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, img_pil]
            )
            
            if response and response.text:
                 duration = time.time() - start_time
                 logger.info(f"‚úÖ Vision Success in {duration:.2f}s")
                 return response.text
            
            logger.warning("‚ö†Ô∏è Gemini returned empty text.")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç)."

        except Exception as e:
            logger.error(f"‚ùå Vision error: {e}", exc_info=True)
            return f"–Ø –æ—Å–ª–µ–ø, –ü–æ. –û—à–∏–±–∫–∞: {e}"
        finally:
            # –ß–∏—Å—Ç–∏–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω —Å–æ–∑–¥–∞–≤–∞–ª—Å—è
            if converted_path != file_path and os.path.exists(converted_path):
                try:
                    os.remove(converted_path)
                except Exception:
                    pass

    async def analyze_visual(self, file_path: str, prompt: str) -> str:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è —Å–∫—Ä–∏–Ω—à–æ—Ç—ã).
        """
        start_time = time.time()
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API."

            client = genai.Client(api_key=api_key)
            img_pil = Image.open(file_path)
            
            logger.info(f"üì° Sending Visual Analysis request ({self.vision_model})...")
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, img_pil]
            )
            
            duration = time.time() - start_time
            if response and response.text:
                logger.info(f"‚úÖ Visual Analysis success in {duration:.2f}s")
                return response.text
            
            return "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."

        except Exception as e:
            logger.error(f"‚ùå Visual analysis error: {e}", exc_info=True)
            return f"–û—à–∏–±–∫–∞ –∑—Ä–µ–Ω–∏—è: {e}"

    async def analyze_video(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ-—Å–æ–æ–±—â–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –∫—Ä—É–∂–∫–∏) —á–µ—Ä–µ–∑ Gemini 2.0 Flash.
        """
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = router.gemini_key or self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ."

            client = genai.Client(api_key=api_key)
            
            logger.info(f"üéûÔ∏è Uploading video to Gemini: {file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            video_file = await asyncio.to_thread(
                client.files.upload,
                path=file_path
            )
            
            # –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            while True:
                # Polling file status
                # Using to_thread because client methods might be blocking
                file_info = await asyncio.to_thread(client.files.get, name=video_file.name)
                
                # Check status (Assuming 'ACTIVE' or 'PROCESSING')
                # In new SDK, state is an enum or string.
                state = str(file_info.state)
                
                if "ACTIVE" in state:
                    break
                elif "FAILED" in state:
                    raise Exception("Google Video Processing failed.")
                
                logger.info(f"Video processing... {state}")
                await asyncio.sleep(2)

            logger.info(f"‚úÖ Video processing complete: {video_file.name}")
            
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, video_file]
            )
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
            await asyncio.to_thread(client.files.delete, name=video_file.name)
            
            return response.text if response else "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø–æ –≤–∏–¥–µ–æ."

        except Exception as e:
            logger.error(f"Video analysis error: {e}")
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤–∏–¥–µ–æ: {e}"

    async def analyze_document(self, file_path: str, router, prompt: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç (PDF) —á–µ—Ä–µ–∑ Gemini Native Document Understanding.
        """
        try:
            if not _GENAI_AVAILABLE:
                return "–û—à–∏–±–∫–∞: Google GenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."

            api_key = router.gemini_key or self.gemini_key
            if not api_key:
                return "–û—à–∏–±–∫–∞: –ù–µ—Ç –∫–ª—é—á–∞ Gemini API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞."

            client = genai.Client(api_key=api_key)
            
            logger.info(f"üìÑ Uploading document to Gemini: {file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            doc_file = await asyncio.to_thread(
                client.files.upload,
                path=file_path
            )
            
            # –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            while True:
                file_info = await asyncio.to_thread(client.files.get, name=doc_file.name)
                state = str(file_info.state)
                
                if "ACTIVE" in state:
                    break
                elif "FAILED" in state:
                    raise Exception("Google Document Processing failed.")
                
                logger.info(f"Document processing... {state}")
                await asyncio.sleep(2)

            logger.info(f"‚úÖ Document processing complete: {doc_file.name}")
            
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=self.vision_model,
                contents=[prompt, doc_file]
            )
            
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –∏–∑ –æ–±–ª–∞–∫–∞
            await asyncio.to_thread(client.files.delete, name=doc_file.name)
            
            return response.text if response else "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É."

        except Exception as e:
            logger.error(f"Document analysis error: {e}")
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}"

    def _clean_text_for_tts(self, text: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–µ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –æ–∑–≤—É—á–∫–æ–π.
        """
        import re
        # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ –∫–æ—Ä–æ–±–æ–∫ –∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
        text = re.sub(r'<\|begin_of_box\|>|<\|end_of_box\|>', '', text)
        text = re.sub(r'<\|thought\|>.*?</\|thought\|>', '', text, flags=re.DOTALL)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\(\)]', '', text)
        text = re.sub(r'http\S+', '', text)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    async def speak(self, text: str, voice: str = "ru-RU-SvetlanaNeural") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (TTS) —á–µ—Ä–µ–∑ edge-tts + ffmpeg.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ .ogg —Ñ–∞–π–ª—É.
        """
        file_id = str(uuid.uuid4())
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs("artifacts/downloads", exist_ok=True)
        
        mp3_path = f"artifacts/downloads/{file_id}.mp3"
        ogg_path = f"artifacts/downloads/{file_id}.ogg"
        
        # –ï—Å–ª–∏ –≥–æ–ª–æ—Å –Ω–µ —É–∫–∞–∑–∞–Ω –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ—Ç macOS, –º–µ–Ω—è–µ–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π
        if voice in ["Milena", "Yuri", "Katya", "default"]:
            voice = "ru-RU-SvetlanaNeural"

        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ –æ–∑–≤—É—á–∫–æ–π
        clean_text = self._clean_text_for_tts(text)
        if not clean_text:
            logger.warning("TTS text is empty after cleaning. Skipping speech synthesis.")
            return None

        try:
            logger.info(f"üó£Ô∏è Speaking via edge-tts: {clean_text[:40]}... (Voice: {voice})")
            
            # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è MP3 —á–µ—Ä–µ–∑ edge-tts
            communicate = edge_tts.Communicate(clean_text, voice)
            await communicate.save(mp3_path)

            if not os.path.exists(mp3_path) or os.path.getsize(mp3_path) < 100:
                logger.error(f"TTS (edge-tts) failed: file missing or too small {mp3_path}")
                return None
            
            # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ OGG (Opus) –¥–ª—è Telegram (Voice Note format)
            proc_ffmpeg = await asyncio.create_subprocess_exec(
                "ffmpeg", "-i", mp3_path, "-c:a", "libopus", "-b:a", "24k", "-vbr", "on", "-y", ogg_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await proc_ffmpeg.communicate()
            
            if proc_ffmpeg.returncode != 0:
                 logger.error(f"ffmpeg conversion failed: {stderr.decode().strip()}")
                 return None

            if os.path.exists(mp3_path):
                 os.remove(mp3_path)
            
            if os.path.exists(ogg_path):
                # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ OGG
                if os.path.getsize(ogg_path) < 200:
                    logger.error(f"Generated OGG is too small ({os.path.getsize(ogg_path)} bytes), likely silent.")
                    os.remove(ogg_path)
                    return None
                    
                logger.info(f"‚úÖ TTS Generated: {ogg_path} ({os.path.getsize(ogg_path)} bytes)")
                return ogg_path
            return None
            
        except Exception as e:
            logger.error(f"TTS Error (edge-tts): {e}")
            if os.path.exists(mp3_path): os.remove(mp3_path)
            return None