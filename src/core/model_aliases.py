# -*- coding: utf-8 -*-
"""
Model alias resolution and presets (extracted from old handlers/commands.py).
Used by the web panel and command handlers for human-friendly model names.
"""
from __future__ import annotations

MODEL_FRIENDLY_ALIASES: dict[str, str] = {
    # Gemini
    "gemini-flash": "google/gemini-2.5-flash",
    "gemini-2.5-flash": "google/gemini-2.5-flash",
    "gemini-flash-latest": "google/gemini-2.5-flash",
    "gemini-3-pro": "google/gemini-3-pro-preview",
    "gemini-3-pro-latest": "google/gemini-3-pro-preview",
    "gemini-pro": "google/gemini-3-pro-preview",
    "gemini-pro-latest": "google/gemini-3-pro-preview",
    "gemini-2.5-pro": "google/gemini-2.5-pro",
    # OpenAI
    "gpt-4o-mini": "openai/gpt-4o-mini",
    "gpt-4.1-mini": "openai/gpt-4.1-mini",
    "gpt-5-mini": "openai/gpt-5-mini",
    "gpt-5": "openai/gpt-5",
    "gpt-5-chat": "openai/gpt-5-chat-latest",
    "gpt-5-codex": "openai/gpt-5-codex",
    "o3": "openai/o3",
    "o4-mini": "openai/o4-mini",
}


def normalize_model_alias(raw_model_name: str) -> tuple[str, str]:
    """
    Normalizes human-friendly model aliases to canonical provider/model format.
    Returns (resolved_id, info_message).
    """
    original = str(raw_model_name or "").strip()
    if not original:
        return "", ""

    canonical = "-".join(original.lower().replace("_", "-").split())
    resolved = MODEL_FRIENDLY_ALIASES.get(canonical, original)

    if "/" not in resolved:
        lowered = resolved.lower()
        if lowered.startswith("gemini"):
            resolved = f"google/{resolved}"
        elif lowered.startswith(("gpt", "o1", "o3", "o4", "o5", "codex")):
            resolved = f"openai/{resolved}"

    if resolved == original:
        return resolved, ""
    return resolved, f"‚ÑπÔ∏è –ê–ª–∏–∞—Å: `{original}` ‚Üí `{resolved}`"


def parse_model_set_request(args: list[str], valid_slots: list[str]) -> dict[str, str | bool]:
    """
    Parses `!model set` arguments in canonical and legacy format.
    """
    slots_sorted = sorted({str(slot).strip().lower() for slot in valid_slots if str(slot).strip()})
    usage = (
        "‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–æ–º–∞–Ω–¥—ã:\n"
        "`!model set <slot> <model_id>`\n"
        "–ü—Ä–∏–º–µ—Ä: `!model set chat zai-org/glm-4.6v-flash`"
    )

    if len(args) < 3:
        return {"ok": False, "error": usage}

    candidate_slot = str(args[1]).strip().lower()
    if candidate_slot in slots_sorted:
        slot = candidate_slot
        model_raw = " ".join(args[2:]).strip()
    else:
        slot = "chat"
        model_raw = " ".join(args[1:]).strip()

    if not model_raw:
        return {"ok": False, "error": usage}

    model_id, alias_msg = normalize_model_alias(model_raw)
    return {
        "ok": True,
        "slot": slot,
        "model_id": model_id,
        "alias_msg": alias_msg,
        "is_legacy": candidate_slot not in slots_sorted,
    }


def render_model_presets_text() -> str:
    """Renders user-friendly model presets for quick switching."""
    return (
        "üß© **–ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–µ—Å–µ—Ç—ã –º–æ–¥–µ–ª–µ–π:**\n\n"
        "**Cloud (Gemini):**\n"
        "‚Ä¢ `!model set chat gemini-flash` ‚Üí `google/gemini-2.5-flash`\n"
        "‚Ä¢ `!model set pro gemini-3-pro-latest` ‚Üí `google/gemini-3-pro-preview`\n"
        "‚Ä¢ `!model set thinking gemini-2.5-pro` ‚Üí `google/gemini-2.5-pro`\n\n"
        "**Cloud (OpenAI):**\n"
        "‚Ä¢ `!model set chat gpt-4o-mini` ‚Üí `openai/gpt-4o-mini`\n"
        "‚Ä¢ `!model set chat gpt-5-mini` ‚Üí `openai/gpt-5-mini`\n"
        "‚Ä¢ `!model set coding gpt-5-codex` ‚Üí `openai/gpt-5-codex`\n\n"
        "**Local (LM Studio):**\n"
        "‚Ä¢ `!model set chat zai-org/glm-4.6v-flash`\n\n"
        "–ü–æ—Å–ª–µ —Å–º–µ–Ω—ã ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞:\n"
        "‚Ä¢ `!model`\n"
        "‚Ä¢ `!model preflight chat –¢–µ—Å—Ç –º–∞—Ä—à—Ä—É—Ç–∞`\n"
    )
