# -*- coding: utf-8 -*-
"""
Model Manager (Router) –¥–ª—è Krab v2.0.
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Cloud vs Local).
"""

import os
import aiohttp
import logging
from typing import Literal, Optional, Dict, Any, List
from src.core.rag_engine import RAGEngine

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
import structlog
logger = structlog.get_logger("ModelRouter")

class ModelRouter:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.lm_studio_url = config.get("LM_STUDIO_URL", "http://localhost:1234/v1")
        self.ollama_url = config.get("OLLAMA_URL", "http://localhost:11434/api")
        self.gemini_key = config.get("GEMINI_API_KEY")

        # –°—Ç–∞—Ç—É—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.is_local_available = False
        self.local_engine = None # 'lm-studio' or 'ollama'
        self.active_local_model = None
        
        # RAG Engine
        self.rag = RAGEngine()
        
        # Persona Manager (–Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py)
        self.persona = None 
        self.tools = None  # –ù–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py (ToolHandler)        
        # –ü—É–ª –º–æ–¥–µ–ª–µ–π ‚Äî —á–∏—Ç–∞–µ–º –∏–∑ .env, –¥–µ—Ñ–æ–ª—Ç—ã –∫–∞–∫ fallback
        # –ü—Ä–∏—á–∏–Ω–∞: —Ö–∞—Ä–¥–∫–æ–¥ gemini-2.0-flash –º–µ—à–∞–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø—Ä–∞–≤–∫–∏ –∫–æ–¥–∞
        self.models = {
            "chat": config.get("GEMINI_CHAT_MODEL", "gemini-2.0-flash"),
            "thinking": config.get("GEMINI_THINKING_MODEL", "gemini-2.0-flash-thinking-exp"),
            "pro": config.get("GEMINI_PRO_MODEL", "gemini-2.0-pro-exp"),
            "coding": config.get("GEMINI_CODING_MODEL", "gemini-2.0-flash"),
        }

    async def check_local_health(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–ø—É—â–µ–Ω –ª–∏ LM Studio –∏–ª–∏ Ollama."""
        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º LM Studio (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.lm_studio_url}/models", timeout=2) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.local_engine = 'lm-studio'
                            self.is_local_available = True
                            logger.info(f"Local AI Available (LM Studio): {self.active_local_model}")
                            return
        except Exception:
            pass

        # 2. –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º Ollama
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.ollama_url.replace('/api', '/v1')}/models", timeout=2) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.local_engine = 'ollama'
                            self.is_local_available = True
                            logger.info(f"Local AI Available (Ollama): {self.active_local_model}")
                            return
        except Exception:
            pass

        self.is_local_available = False
        self.local_engine = None
        self.active_local_model = None

    async def _call_local_llm(self, prompt: str, context: list = None, is_private: bool = True) -> str:
        """
        –í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å (aiohttp).
        """
        try:
            # –í—ã–±–∏—Ä–∞–µ–º URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–≤–∏–∂–∫–∞
            base_url = self.lm_studio_url if self.local_engine == 'lm-studio' else \
                       self.ollama_url.replace('/api', '/v1')

            # –§–æ—Ä–º–∏—Ä—É–µ–º payload
            messages = []
            if context:
                messages.extend(context)
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self.active_local_model or "local-model",
                "messages": messages,
                "temperature": 0.7
            }

            headers = {"Content-Type": "application/json"}
            
            # –¢–∞–π–º–∞—É—Ç –ø–æ–±–æ–ª—å—à–µ –¥–ª—è –ª–æ–∫–∞–ª–∫–∏
            timeout = aiohttp.ClientTimeout(total=60)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/chat/completions", 
                    json=payload, 
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        # LOGGING RAW RESPONSE FOR DEBUGGING
                        logger.info(f"Local LLM Raw Response: {data}")
                        
                        # –ó–∞—â–∏—Ç–∞ –æ—Ç NoneType errors
                        choices = data.get('choices')
                        if choices and len(choices) > 0:
                            content = choices[0].get('message', {}).get('content')
                            if content:
                                return content
                        
                        logger.error(f"Local LLM Invalid Response: {data}")
                        return None # Return None to trigger fallback
                    else:
                        error_text = await response.text()
                        logger.error(f"Local LLM HTTP {response.status}: {error_text}")
                        return None # Return None to trigger fallback

        except Exception as e:
            import traceback
            logger.error(f"Local LLM Connection Error: {e}\n{traceback.format_exc()}")
            return None # Return None to trigger fallback

    async def route_query(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative'] = 'chat',
                          context: list = None,
                          is_private: bool = True,
                          use_rag: bool = True):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å Auto-Fallback –∏ RAG.
        """
        
        # 0. RAG Lookup
        if use_rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.1. Tool Orchestration (Phase 6)
        if self.tools:
            tool_data = await self.tools.execute_tool_chain(prompt)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.5. Reasoning Mode (Thinker)
        if task_type == 'reasoning':
            logger.info("üß† Thinking mode activated...")
            return await self._call_gemini(prompt, self.models["thinking"], context, is_private)

        await self.check_local_health() 

        # 1. –ü—ã—Ç–∞–µ–º—Å—è –ª–æ–∫–∞–ª—å–Ω–æ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –∏ –∑–∞–¥–∞—á–∞ –ø—Ä–æ—Å—Ç–∞—è/—á–∞—Ç)
        if self.is_local_available and task_type in ['chat', 'coding']:
            logger.info("Routing to LOCAL", model=self.active_local_model)
            response = await self._call_local_llm(prompt, context, is_private)
            
            if response: # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
                return response
            
            logger.warning("Local LLM failed. Falling back to CLOUD.")

        # 2. Fallback –∏–ª–∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ -> Gemini Cloud
        model_name = self.models.get(task_type, self.models["chat"])
        logger.info("Routing to CLOUD", model=model_name)

        return await self._call_gemini(prompt, model_name, context, is_private)

    async def _call_gemini(self, prompt: str, model_name: str, context: list = None, is_private: bool = True) -> str:
        """–í—ã–∑–æ–≤ Google Gemini —á–µ—Ä–µ–∑ Generative AI SDK."""
        try:
            import google.generativeai as genai
            
            if not self.gemini_key:
                return "‚ùå –û—à–∏–±–∫–∞: –ù–µ –∑–∞–¥–∞–Ω `GEMINI_API_KEY` –≤ `.env`."

            genai.configure(api_key=self.gemini_key)
            
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏—á–Ω–æ—Å—Ç–∏ (Persona)
            from src.core.prompts import get_system_prompt
            base_instructions = get_system_prompt(is_private)
            
            persona_prompt = ""
            if self.persona:
                persona_prompt = self.persona.get_current_prompt()
            
            system_instructions = f"{persona_prompt}\n\n{base_instructions}"
            
            # –ü–µ—Ä–µ–¥–∞–µ–º system instruction
            model = genai.GenerativeModel(model_name, system_instruction=system_instructions)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
            full_prompt = prompt
            if context:
                history_str = "\n".join([f"{msg.get('role', 'user')}: {msg.get('text', '')}" for msg in context])
                full_prompt = f"History:\n{history_str}\n\nCurrent Request: {prompt}"

            response = await model.generate_content_async(full_prompt)
            
            if not response or not response.text:
                return "‚ùå AI –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (–∏–ª–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∫–æ–Ω—Ç–µ–Ω—Ç)."
                
            return response.text
        except Exception as e:
            logger.error("Gemini API Error", error=str(e))
            return f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}"

    def get_ram_usage(self) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM —á–µ—Ä–µ–∑ SystemMonitor.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π —Ç—è–∂—ë–ª—ã—Ö –º–æ–¥–µ–ª–µ–π (Flux, Whisper Large)
        —á—Ç–æ–±—ã –Ω–µ –∫—Ä–∞—à–Ω—É—Ç—å —Å–∏—Å—Ç–µ–º—É.
        """
        try:
            from src.utils.system_monitor import SystemMonitor
            snapshot = SystemMonitor.get_snapshot()
            return {
                "total_gb": round(snapshot.ram_total_gb, 1),
                "used_gb": round(snapshot.ram_used_gb, 1),
                "available_gb": round(snapshot.ram_available_gb, 1),
                "percent": snapshot.ram_percent,
                "can_load_heavy": SystemMonitor.can_load_heavy_model()
            }
        except Exception as e:
            logger.warning(f"RAM check failed: {e}")
            return {"error": str(e), "can_load_heavy": True}  # –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ–º