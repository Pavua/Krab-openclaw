# -*- coding: utf-8 -*-
"""
Model Manager (Router) –¥–ª—è Krab v6.5.
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Cloud vs Local).

–°—Ç—Ä–∞—Ç–µ–≥–∏—è: Local First ‚Üí Cloud Fallback.
- –ü—Ä–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio/Ollama ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö (–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å + —Å–∫–æ—Ä–æ—Å—Ç—å)
- –ü—Ä–∏ –æ—à–∏–±–∫–µ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ Gemini Cloud
- RAG –∏ Tool Orchestration —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –ö–ê–ñ–î–´–ô –∑–∞–ø—Ä–æ—Å
"""

import os
import time
import asyncio
import json
import aiohttp
from pathlib import Path
import re
from datetime import datetime, timezone
from contextlib import asynccontextmanager
from typing import Literal, Optional, Dict, Any, List, Set
# from src.core.rag_engine import RAGEngine # Deprecated

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
import structlog
logger = structlog.get_logger("ModelRouter")

from src.core.openclaw_client import OpenClawClient
from src.core.agent_swarm import SwarmManager

class ModelRouter:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.lm_studio_url = config.get("LM_STUDIO_URL", "http://localhost:1234/v1")
        self.ollama_url = config.get("OLLAMA_URL", "http://localhost:11434/api")
        self.gemini_key = config.get("GEMINI_API_KEY")

        # –°—Ç–∞—Ç—É—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.is_local_available = False
        self.local_engine = None  # 'lm-studio' or 'ollama'
        self.active_local_model = None

        # –ö–µ—à –¥–ª—è health-check (—á—Ç–æ–±—ã –Ω–µ –¥—ë—Ä–≥–∞—Ç—å API –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å)
        self._health_cache_ts = 0
        self._health_cache_ttl = 30  # —Å–µ–∫—É–Ω–¥

        # OpenClaw Client (Cloud Model Gateway)
        self.openclaw_client = OpenClawClient(
            base_url=config.get("OPENCLAW_BASE_URL", "http://localhost:18789"),
            api_key=config.get("OPENCLAW_API_KEY")
        )
        logger.info("‚òÅÔ∏è OpenClaw Client configured for Cloud Models")

        # RAG Engine (Deprecated, use OpenClaw)
        self.rag = None # RAGEngine()

        # Persona Manager (–Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py)
        self.persona = None
        self.tools = None  # –ù–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py (ToolHandler)

        # Agent Swarm Manager
        self.swarm = SwarmManager(model_router=self)

        # –ü—É–ª –º–æ–¥–µ–ª–µ–π ‚Äî —á–∏—Ç–∞–µ–º –∏–∑ .env, –¥–µ—Ñ–æ–ª—Ç—ã –∫–∞–∫ fallback
        self.models = {
            "chat": config.get("GEMINI_CHAT_MODEL", "google/gemini-1.5-flash"),
            "thinking": config.get("GEMINI_THINKING_MODEL", "google/gemini-1.5-pro"),
            "pro": config.get("GEMINI_PRO_MODEL", "google/gemini-1.5-pro"),
            "coding": config.get("GEMINI_CODING_MODEL", "google/gemini-1.5-flash"),
        }

        # –°—á—ë—Ç—á–∏–∫–∏ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
        self._stats = {
            "local_calls": 0,
            "cloud_calls": 0,
            "local_failures": 0,
            "cloud_failures": 0,
        }

        # Fallback –º–æ–¥–µ–ª–∏ (–¥–ª—è Gemini Quota Handling)
        self.fallback_models = [
            "gemini-2.0-flash-lite-preview-02-05", # Flash Lite (User requested)
            "gemini-2.0-flash",         # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–Ω—è—Ç
            "gemini-2.0-flash-001",     # –°—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
            "gemini-flash-latest",      # –ê–ª–∏–∞—Å –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—É—é flash
            "gemini-pro-latest"         # –ê–ª–∏–∞—Å –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—É—é pro
        ]
        
        # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: 'auto', 'force_local', 'force_cloud'
        self.force_mode = "auto"

        # –ü–æ–ª–∏—Ç–∏–∫–∞ —Ä–æ—É—Ç–∏–Ω–≥–∞ (Phase D): free-first hybrid.
        self.routing_policy = str(config.get("MODEL_ROUTING_POLICY", "free_first_hybrid")).strip().lower()
        self.require_confirm_expensive = str(config.get("MODEL_REQUIRE_CONFIRM_EXPENSIVE", "0")).strip().lower() in {
            "1", "true", "yes", "on"
        }
        self.enable_cloud_review_for_critical = str(
            config.get("MODEL_ENABLE_CLOUD_REVIEW_CRITICAL", "0")
        ).strip().lower() in {"1", "true", "yes", "on"}

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–¥–æ–≤ –≤ –≤—ã–∑–æ–≤–∞—Ö (–±—é–¥–∂–µ—Ç–Ω—ã–π guardrail –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É).
        try:
            self.cloud_soft_cap_calls = int(config.get("CLOUD_SOFT_CAP_CALLS", 10000))
        except Exception:
            self.cloud_soft_cap_calls = 10000
        self.cloud_soft_cap_reached = False
        try:
            self.cloud_cost_per_call_usd = float(config.get("CLOUD_COST_PER_CALL_USD", 0.01))
        except Exception:
            self.cloud_cost_per_call_usd = 0.01
        try:
            self.local_cost_per_call_usd = float(config.get("LOCAL_COST_PER_CALL_USD", 0.0))
        except Exception:
            self.local_cost_per_call_usd = 0.0
        try:
            self.cloud_monthly_budget_usd = float(config.get("CLOUD_MONTHLY_BUDGET_USD", 25.0))
        except Exception:
            self.cloud_monthly_budget_usd = 25.0
        try:
            self.monthly_calls_forecast = int(config.get("MONTHLY_CALLS_FORECAST", 5000))
        except Exception:
            self.monthly_calls_forecast = 5000

        # –ü–æ–ª–∏—Ç–∏–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞: 1 heavy + 1 light.
        self._local_heavy_slot = asyncio.Semaphore(1)
        self._local_light_slot = asyncio.Semaphore(1)

        self.local_timeout_seconds = float(config.get("LOCAL_CHAT_TIMEOUT_SECONDS", 300))
        self.last_cloud_error: Optional[str] = None
        self.last_cloud_model: Optional[str] = None
        self.cloud_priority_models = self._parse_cloud_priority(config.get(
            "MODEL_CLOUD_PRIORITY_LIST",
            "google/gemini-2.0-flash,google/gemini-2.0-flash-lite-preview-02-05,openai/gpt-4o-mini,openai/gpt-4o-mini-standalone,wormgpt-1.0,kimi/k2-llama-mix"
        ))

        # –ü–∞–º—è—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –ø–æ –ø—Ä–æ—Ñ–∏–ª—è–º –∑–∞–¥–∞—á.
        self._routing_memory_path = Path(
            config.get("MODEL_ROUTING_MEMORY_PATH", "artifacts/model_routing_memory.json")
        )
        self._usage_report_path = Path(
            config.get("MODEL_USAGE_REPORT_PATH", "artifacts/model_usage_report.json")
        )
        self._routing_memory = self._load_json(self._routing_memory_path, default={})
        self._usage_report = self._load_json(
            self._usage_report_path,
            default={"profiles": {}, "models": {}, "channels": {"local": 0, "cloud": 0}},
        )
        self._ops_state_path = Path(
            config.get("MODEL_OPS_STATE_PATH", "artifacts/model_ops_state.json")
        )
        self._ops_state = self._load_json(
            self._ops_state_path,
            default={"acknowledged": {}, "history": []},
        )
        if not isinstance(self._ops_state.get("acknowledged"), dict):
            self._ops_state["acknowledged"] = {}
        if not isinstance(self._ops_state.get("history"), list):
            self._ops_state["history"] = []

        # –ö–æ–Ω—Ç—É—Ä –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (1-5) –¥–ª—è —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è —Ä–æ—É—Ç–∏–Ω–≥–∞.
        self._feedback_path = Path(
            config.get("MODEL_FEEDBACK_PATH", "artifacts/model_feedback.json")
        )
        self._feedback_store = self._load_json(
            self._feedback_path,
            default={"profiles": {}, "events": [], "last_route": {}, "updated_at": None},
        )
        if not isinstance(self._feedback_store.get("profiles"), dict):
            self._feedback_store["profiles"] = {}
        if not isinstance(self._feedback_store.get("events"), list):
            self._feedback_store["events"] = []
        if not isinstance(self._feedback_store.get("last_route"), dict):
            self._feedback_store["last_route"] = {}

        existing_cloud_calls = int(self._usage_report.get("channels", {}).get("cloud", 0))
        if existing_cloud_calls >= self.cloud_soft_cap_calls:
            self.cloud_soft_cap_reached = True
            logger.warning(f"Cloud Soft Cap reached at startup ({existing_cloud_calls}/{self.cloud_soft_cap_calls})")
        else:
            self.cloud_soft_cap_reached = False
            logger.info(f"Cloud Soft Cap status: {existing_cloud_calls}/{self.cloud_soft_cap_calls} ok")

    def set_force_mode(self, mode: Literal['auto', 'local', 'cloud']) -> str:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Ä–æ—É—Ç–µ—Ä–∞."""
        if mode not in ['auto', 'local', 'cloud']:
            return "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º. –ò—Å–ø–æ–ª—å–∑—É–π: auto, local, cloud"
        
        old = self.force_mode
        if mode == 'local':
            self.force_mode = 'force_local'
        elif mode == 'cloud':
            self.force_mode = 'force_cloud'
        else:
            self.force_mode = 'auto'
            
        return f"–†–µ–∂–∏–º –∏–∑–º–µ–Ω–µ–Ω: {old} -> {self.force_mode}"

    def _load_json(self, path: Path, default: dict) -> dict:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JSON-—Ñ–∞–π–ª–∞."""
        try:
            if not path.exists():
                return default
            with path.open("r", encoding="utf-8") as fp:
                data = json.load(fp)
                return data if isinstance(data, dict) else default
        except Exception:
            return default

    def _save_json(self, path: Path, payload: dict) -> None:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–ø–∏—Å—å JSON-—Ñ–∞–π–ª–∞."""
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            with path.open("w", encoding="utf-8") as fp:
                json.dump(payload, fp, ensure_ascii=False, indent=2)
        except Exception as exc:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON –º–µ—Ç—Ä–∏–∫–∏ —Ä–æ—É—Ç–µ—Ä–∞", path=str(path), error=str(exc))

    def _parse_cloud_priority(self, raw: Optional[str]) -> List[str]:
        """
        –†–∞–∑–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ —Å—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ —É–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏.
        """
        if not raw:
            return []
        result: list[str] = []
        seen: Set[str] = set()
        for token in str(raw).split(","):
            token = token.strip()
            if not token or token in seen:
                continue
            seen.add(token)
            result.append(token)
        return result

    def _lm_studio_api_root(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –∞–¥—Ä–µ—Å LM Studio –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞ /v1 –∏–ª–∏ /api/v1.
        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–∑–Ω—ã–µ REST-–ø—É—Ç–∏ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω –∫–æ—Ä–µ–Ω—å.
        """
        root = self.lm_studio_url.rstrip("/")
        for suffix in ("/api/v1", "/v1"):
            if root.endswith(suffix):
                root = root[: -len(suffix)]
                break
        return root.rstrip("/")

    def _normalize_model_entries(self, payload: Any) -> List[Dict[str, Any]]:
        """
        –ü—Ä–∏–≤–æ–¥–∏—Ç –æ—Ç–≤–µ—Ç LM Studio / OpenClaw –∫ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–æ–¥–µ–ª—è–º–∏.
        """
        entries: List[Dict[str, Any]] = []
        candidate = []
        if isinstance(payload, dict):
            if isinstance(payload.get("models"), list):
                candidate = payload["models"]
            elif isinstance(payload.get("data"), list):
                candidate = payload["data"]
            elif isinstance(payload.get("result"), list):
                candidate = payload["result"]
            else:
                candidate = []
        elif isinstance(payload, list):
            candidate = payload
        else:
            candidate = []

        for item in candidate:
            if isinstance(item, dict):
                entries.append(item)
            else:
                entries.append({"id": str(item)})
        return entries

    def _extract_model_id(self, entry: Dict[str, Any]) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Ç–∞–µ–º—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –∏–∑ –∑–∞–ø–∏—Å–∏ LM Studio.
        """
        for key in ("id", "key", "modelId", "identifier", "name"):
            value = entry.get(key)
            if value:
                return str(value)
        return None

    def _is_cloud_error_message(self, text: Optional[str]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç OpenClaw —è–≤–Ω–æ–π –æ—à–∏–±–∫–æ–π.
        """
        if not text:
            return True
        lowered = text.strip().lower()
        return lowered.startswith("‚ùå") or lowered.startswith("‚ö†Ô∏è")

    def _is_cloud_billing_error(self, text: str) -> bool:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç billing-–æ—à–∏–±–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
        –ò—Å–∫–ª—é—á–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ Rate Limit (quota exceeded).
        """
        lowered = text.lower()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ rate limit –∏–ª–∏ 429 ‚Äî —ç—Ç–æ –ù–ï –æ—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞, –∞ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∞
        if "rate limit" in lowered or "429" in lowered:
            return False

        billing_keywords = [
            "billing error",
            "out of credits",
            "insufficient balance",
            "insufficient funds",
            "billing",
            "credit balance",
        ]
        
        # 'quota' —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏ –¥–ª—è –±–∏–ª–ª–∏–Ω–≥–∞ –∏ –¥–ª—è —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–æ–≤. 
        # –°—á–∏—Ç–∞–µ–º –∑–∞ –±–∏–ª–ª–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï–¢ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è rate limit.
        if "quota" in lowered and "rate" not in lowered:
             return True

        return any(keyword in lowered for keyword in billing_keywords)

    def _mark_cloud_soft_cap_if_needed(self, error_text: str) -> None:
        """
        –ü—Ä–∏ billing-–æ—à–∏–±–∫–µ –ø–∏—à–µ—Ç –≤ –ª–æ–≥, –Ω–æ –ù–ï –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ–±–ª–∞–∫–æ, 
        —Ç–∞–∫ –∫–∞–∫ –º—ã –¥–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        """
        if self._is_cloud_billing_error(error_text):
            logger.warning("Cloud warning (billing-related): %s. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–ø—ã—Ç–∫–∏.", error_text)
            # self.cloud_soft_cap_reached = True  <-- –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞

    def _ensure_feedback_store(self) -> dict:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç feedback store –∫ –æ–∂–∏–¥–∞–µ–º–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ."""
        if not isinstance(self._feedback_store, dict):
            self._feedback_store = {"profiles": {}, "events": [], "last_route": {}, "updated_at": None}
        if not isinstance(self._feedback_store.get("profiles"), dict):
            self._feedback_store["profiles"] = {}
        if not isinstance(self._feedback_store.get("events"), list):
            self._feedback_store["events"] = []
        if not isinstance(self._feedback_store.get("last_route"), dict):
            self._feedback_store["last_route"] = {}
        return self._feedback_store

    def _normalize_channel(self, channel: Optional[str]) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–º—è –∫–∞–Ω–∞–ª–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏."""
        lowered = str(channel or "").strip().lower()
        if lowered in {"local", "cloud"}:
            return lowered
        return "local"

    def _remember_last_route(
        self,
        profile: str,
        task_type: str,
        channel: str,
        model_name: str,
        prompt: str = "",
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞,
        —á—Ç–æ–±—ã –≤–ª–∞–¥–µ–ª–µ—Ü –º–æ–≥ –æ—Ü–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞ profile/model.
        """
        store = self._ensure_feedback_store()
        route = {
            "ts": self._now_iso(),
            "profile": (profile or "chat").strip().lower() or "chat",
            "task_type": (task_type or "chat").strip().lower() or "chat",
            "channel": self._normalize_channel(channel),
            "model": (model_name or "unknown").strip() or "unknown",
            "prompt_preview": (prompt or "").strip()[:160],
        }
        store["last_route"] = route
        history = store.setdefault("route_history", [])
        if not isinstance(history, list):
            history = []
            store["route_history"] = history
        history.append(route)
        if len(history) > 60:
            del history[: len(history) - 60]
        store["updated_at"] = self._now_iso()
        self._save_json(self._feedback_path, store)

    def _get_model_feedback_stats(self, profile: str, model_name: str) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É feedback –ø–æ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø—Ä–æ—Ñ–∏–ª–µ."""
        store = self._ensure_feedback_store()
        profiles = store.get("profiles", {})
        profile_data = profiles.get(profile, {}) if isinstance(profiles, dict) else {}
        models = profile_data.get("models", {}) if isinstance(profile_data, dict) else {}
        entry = models.get(model_name, {}) if isinstance(models, dict) else {}
        count = int(entry.get("count", 0)) if isinstance(entry, dict) else 0
        avg = float(entry.get("avg", 0.0)) if isinstance(entry, dict) else 0.0
        return {"count": count, "avg": round(avg, 3)}

    def classify_task_profile(self, prompt: str, task_type: str = "chat") -> str:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–æ—É—Ç–∏–Ω–≥–∞.
        –ü—Ä–æ—Ñ–∏–ª–∏: chat, moderation, code, security, infra, review, communication.
        """
        normalized_type = (task_type or "chat").strip().lower()
        if normalized_type in {"coding", "code"}:
            return "code"
        if normalized_type in {"reasoning", "review"}:
            return "review"

        text = (prompt or "").lower()
        keyword_map = {
            "moderation": ["ban", "mute", "warn", "delete message", "—Å–ø–∞–º", "–º–æ–¥–µ—Ä–∞—Ü", "muted"],
            "security": ["vulnerability", "security", "audit", "exploit", "—É—è–∑–≤", "–±–µ–∑–æ–ø–∞—Å"],
            "infra": ["deploy", "terraform", "k8s", "kubernetes", "docker", "infra", "—Å–µ—Ä–≤–µ—Ä", "ci/cd"],
            "review": ["code review", "critique", "–ø—Ä–æ–≤–µ—Ä—å –∫–æ–¥", "—Ä–µ–≤—å—é", "–∫—Ä–∏—Ç–∏–∫–∞"],
            "communication": ["translate", "–ø–µ—Ä–µ–≤–æ–¥", "summary", "—Å–∞–º–º–∞—Ä–∏", "telegram", "—á–∞—Ç"],
            "code": ["python", "typescript", "javascript", "refactor", "bugfix", "–∫–æ–¥", "—Å–∫—Ä–∏–ø—Ç"],
        }
        for profile, markers in keyword_map.items():
            if any(marker in text for marker in markers):
                return profile
        return "chat"

    def _is_critical_profile(self, profile: str) -> bool:
        """–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏, –≥–¥–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞."""
        return profile in {"security", "infra", "review"}

    def _model_tier(self, model_name: Optional[str]) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è scheduler-–∞:
        heavy –∏–ª–∏ light.
        """
        if not model_name:
            return "light"
        lowered = model_name.lower()
        if any(token in lowered for token in ["70b", "72b", "34b", "32b", "30b", "27b", "22b", "20b", "mixtral"]):
            return "heavy"

        match = re.search(r"(\d+)\s*b", lowered)
        if match:
            try:
                size_b = int(match.group(1))
                return "heavy" if size_b >= 20 else "light"
            except ValueError:
                return "light"
        return "light"

    @asynccontextmanager
    async def _acquire_local_slot(self, model_name: Optional[str]):
        """
        –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤:
        - heavy: –º–∞–∫—Å–∏–º—É–º 1 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π heavy.
        - light: –º–∞–∫—Å–∏–º—É–º 1 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π light.
        """
        tier = self._model_tier(model_name)
        semaphore = self._local_heavy_slot if tier == "heavy" else self._local_light_slot
        await semaphore.acquire()
        try:
            yield tier
        finally:
            semaphore.release()

    def _remember_model_choice(self, profile: str, model_name: str, channel: str) -> None:
        """
        –ó–∞–ø–æ–º–∏–Ω–∞–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á.
        """
        if not profile or not model_name:
            return

        memory = self._routing_memory.setdefault("profiles", {})
        profile_entry = memory.setdefault(profile, {"models": {}, "channels": {}})
        profile_entry["models"][model_name] = int(profile_entry["models"].get(model_name, 0)) + 1
        profile_entry["channels"][channel] = int(profile_entry["channels"].get(channel, 0)) + 1
        self._save_json(self._routing_memory_path, self._routing_memory)

    def _update_usage_report(self, profile: str, model_name: str, channel: str) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ—Ç—á—ë—Ç usage/cost guardrails."""
        profiles = self._usage_report.setdefault("profiles", {})
        profiles[profile] = int(profiles.get(profile, 0)) + 1

        models = self._usage_report.setdefault("models", {})
        models[model_name] = int(models.get(model_name, 0)) + 1

        channels = self._usage_report.setdefault("channels", {"local": 0, "cloud": 0})
        channels[channel] = int(channels.get(channel, 0)) + 1

        if channel == "cloud" and channels.get("cloud", 0) >= self.cloud_soft_cap_calls:
            self.cloud_soft_cap_reached = True

        self._save_json(self._usage_report_path, self._usage_report)

    def _get_profile_recommendation(self, profile: str) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –∫–∞–Ω–∞–ª –¥–ª—è –ø—Ä–æ—Ñ–∏–ª—è.
        """
        profile = profile or "chat"
        profiles = self._routing_memory.get("profiles", {})
        memorized = profiles.get(profile, {})
        memorized_models = memorized.get("models", {})
        memorized_channels = memorized.get("channels", {})

        top_model = None
        top_channel = None
        if memorized_models:
            top_model = max(memorized_models.items(), key=lambda item: int(item[1]))[0]
        if memorized_channels:
            top_channel = max(memorized_channels.items(), key=lambda item: int(item[1]))[0]

        if profile in {"security", "infra", "review"}:
            default_model = self.models.get("pro", self.models.get("thinking", self.models["chat"]))
            default_channel = "cloud"
        elif profile == "code":
            default_model = self.models.get("coding", self.models["chat"])
            default_channel = "local"
        elif profile == "moderation":
            default_model = self.models.get("chat", "gemini-2.0-flash")
        
        if not profile:
            default_model = self.models.get("chat", "gemini-2.0-flash")
            default_channel = "local"

        # Adaptive feedback loop: –µ—Å–ª–∏ –ø–æ –º–æ–¥–µ–ª–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω—ã –æ—Ü–µ–Ω–∫–∏,
        # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤–∑–≤–µ—à–∏–≤–∞–µ–º –≤—ã–±–æ—Ä –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É –∫–∞—á–µ—Å—Ç–≤—É.
        store = self._ensure_feedback_store()
        feedback_profiles = store.get("profiles", {})
        feedback_profile = feedback_profiles.get(profile, {}) if isinstance(feedback_profiles, dict) else {}
        feedback_models = feedback_profile.get("models", {}) if isinstance(feedback_profile, dict) else {}

        candidate_models = set(memorized_models.keys()) if isinstance(memorized_models, dict) else set()
        if isinstance(feedback_models, dict):
            candidate_models.update(feedback_models.keys())
        if not candidate_models and default_model:
            candidate_models.add(default_model)

        if candidate_models:
            best_model = None
            best_score = None
            for candidate in candidate_models:
                usage_count = int(memorized_models.get(candidate, 0)) if isinstance(memorized_models, dict) else 0
                feedback_entry = feedback_models.get(candidate, {}) if isinstance(feedback_models, dict) else {}
                feedback_count = int(feedback_entry.get("count", 0)) if isinstance(feedback_entry, dict) else 0
                feedback_avg = float(feedback_entry.get("avg", 0.0)) if isinstance(feedback_entry, dict) else 0.0

                # –ë–∞–∑–æ–≤—ã–π –≤–µ—Å usage + –≤–µ—Å –∫–∞—á–µ—Å—Ç–≤–∞.
                quality_weight = (feedback_avg / 5.0) * min(feedback_count, 12)
                score = float(usage_count) + float(quality_weight)

                # –ñ–µ—Å—Ç–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ —Å–∏—Å—Ç–µ–º–Ω–æ –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏.
                if feedback_count >= 3 and feedback_avg <= 2.4:
                    score -= 4.0

                if best_score is None or score > best_score:
                    best_score = score
                    best_model = candidate

            if best_model:
                top_model = best_model

        selected_model = top_model or default_model
        feedback_hint = self._get_model_feedback_stats(profile, selected_model)

        return {
            "profile": profile,
            "model": selected_model,
            "channel": top_channel or default_channel,
            "critical": self._is_critical_profile(profile),
            "feedback_hint": {
                "avg_score": feedback_hint.get("avg", 0.0),
                "count": feedback_hint.get("count", 0),
            },
        }

    def _resolve_cloud_model(self, task_type: str, profile: str, preferred_model: Optional[str] = None) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç –æ–±–ª–∞—á–Ω—É—é –º–æ–¥–µ–ª—å —Å —É—á–µ—Ç–æ–º –ø—Ä–æ—Ñ–∏–ª—è –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π."""
        if preferred_model and "gemini" in preferred_model:
            return preferred_model
        if profile in {"security", "infra", "review"}:
            return self.models.get("pro", self.models.get("thinking", self.models["chat"]))
        if profile == "code":
            return self.models.get("coding", self.models["chat"])
        if task_type == "reasoning":
            return self.models.get("thinking", self.models["chat"])
        return self.models.get(task_type, self.models["chat"])

    def _build_cloud_candidates(self, task_type: str, profile: str, preferred_model: Optional[str] = None) -> List[str]:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–ª—è cloud-–ø–æ–¥—Å–∏—Å—Ç–µ–º—ã.
        """
        base = self._resolve_cloud_model(task_type, profile, preferred_model)
        candidates: list[str] = []
        seen: Set[str] = set()

        def add(model_name: Optional[str]) -> None:
            if not model_name:
                return
            normalized = model_name.strip()
            if not normalized or normalized in seen:
                return
            seen.add(normalized)
            candidates.append(normalized)

        add(preferred_model or "")
        add(base)
        for extra in self.cloud_priority_models:
            add(extra)

        return candidates

    async def check_local_health(self, force: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞ (LM Studio ‚Üí Ollama).
        """
        now = time.time()
        if not force and (now - self._health_cache_ts) < self._health_cache_ttl:
            return self.is_local_available

        self._health_cache_ts = now

        base_root = self._lm_studio_api_root()
        if not base_root:
            base_root = self.lm_studio_url.rstrip("/")

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –†–ï–ê–õ–¨–ù–û –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ /api/v1/models
        # (–≤ 0.3.x –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –∏–ª–∏ —ç—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–±)
        try:
            models = await self._scan_local_models()
            loaded_models = [m for m in models if m.get("loaded")]
            
            if loaded_models:
                self.local_engine = "lm-studio"
                self.is_local_available = True
                self.active_local_model = loaded_models[0]["id"]
                logger.info(f"‚úÖ Local AI active: {self.active_local_model} (LM Studio)")
                return True
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–∞–º–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{base_root}/api/v1/models") as resp:
                    if resp.status == 200:
                        self.local_engine = "lm-studio"
                        self.is_local_available = False # –ù–æ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!
                        self.active_local_model = None
                        logger.info("üì° LM Studio server alive, but no models loaded.")
                        return False
        except Exception:
            pass

        # Fallback to Ollama
        try:
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{self.ollama_url.replace('/api', '/v1')}/models") as response:
                    if response.status == 200:
                        payload = await response.json()
                        models = self._normalize_model_entries(payload)
                        if models:
                            self.active_local_model = self._extract_model_id(models[0]) or models[0].get("id")
                            self.local_engine = "ollama"
                            self.is_local_available = True
                            return True
        except Exception:
            pass

        self.is_local_available = False
        self.local_engine = None
        self.active_local_model = None
        return False

    async def _scan_local_models(self) -> List[Dict[str, Any]]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ REST API LM Studio 0.3.x –∏–ª–∏ CLI.
        """
        base = self._lm_studio_api_root()
        url = f"{base}/api/v1/models"
        
        try:
            timeout = aiohttp.ClientTimeout(total=5)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        payload = await resp.json(content_type=None)
                        normalized = []
                        if isinstance(payload, dict):
                            normalized = payload.get("data") or payload.get("models") or []
                        elif isinstance(payload, list):
                            normalized = payload

                        models = []
                        for m in normalized:
                            identifier = self._extract_model_id(m) or m.get("id", "")
                            if not identifier: continue
                            
                            # –í 0.3.x –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ –∏–º–µ–µ—Ç state="loaded" –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ
                            # –ù–æ —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± ‚Äî –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –µ—Å—Ç—å –ª–∏ —É –Ω–µ–µ –∏–Ω—Å—Ç–∞–Ω—Å –≤ API
                            state = m.get("state", "").lower()
                            is_loaded = (state == "loaded" or m.get("is_loaded") is True)
                            
                            models.append({
                                "id": identifier,
                                "type": "embedding" if "embedding" in identifier.lower() else "llm",
                                "name": m.get("name", identifier),
                                "loaded": is_loaded
                            })
                        return models
        except Exception:
            pass

        # Fallback to CLI only if API fails or exception occurs

        # Fallback to CLI only if API fails
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if not os.path.exists(lms_path):
            return []

        try:
            proc = await asyncio.create_subprocess_exec(
                lms_path, "ls",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await proc.communicate()
            output = stdout.decode()
            
            models = []
            is_embedding_section = False
            for line in output.splitlines():
                line = line.strip()
                if not line or "SIZE" in line: continue
                if "EMBEDDING" in line: is_embedding_section = True; continue
                if "LLM" in line: is_embedding_section = False; continue
                parts = line.split()
                if parts and ("/" in parts[0] or "-" in parts[0]):
                    models.append({
                        "id": parts[0],
                        "type": "embedding" if is_embedding_section else "llm"
                    })
            return models
        except Exception:
            return []

    async def _ensure_chat_model_loaded(self) -> bool:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª—é–±—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é LLM –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ REST API.
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
        if await self.check_local_health(force=True):
            if self.active_local_model and "embed" not in self.active_local_model.lower():
                return True

        models = await self._scan_local_models()
        chat_candidate = next((m["id"] for m in models if m["type"] == "llm"), None)
        
        if chat_candidate:
            return await self.load_local_model(chat_candidate)
        return False
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if not os.path.exists(lms_path):
            return False

        try:
            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            proc = await asyncio.create_subprocess_exec(
                lms_path, "ps",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await proc.communicate()
            output = stdout.decode()
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å 'Text Embedding', –≤—ã–≥—Ä—É–∂–∞–µ–º
            if "embed" in output.lower():
                # –ü–∞—Ä—Å–∏–º ID (—É–ø—Ä–æ—â–µ–Ω–Ω–æ: –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –∏—â–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)
                if "LOADED" in output: 
                    logger.info("üîÑ Unloading Embedding Model...")
                    await asyncio.create_subprocess_exec(lms_path, "unload", "--all")
                    await asyncio.sleep(2) # Wait for unload

            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞ ps, –µ—Å–ª–∏ –ø—É—Å—Ç–æ - –≥—Ä—É–∑–∏–º
            proc_ps = await asyncio.create_subprocess_exec(
                lms_path, "ps",
                stdout=asyncio.subprocess.PIPE
            )
            out_ps, _ = await proc_ps.communicate()
            if "LOADED" in out_ps.decode() and "embed" not in out_ps.decode().lower():
                return True # –£–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ Chat –º–æ–¥–µ–ª—å

            # 3. –ò—â–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ
            models = await self._scan_local_models()
            
            # –ò—â–µ–º LLM (–Ω–µ embedding)
            chat_candidate = None
            
            # Priority 1: Instruct/Chat models
            for m in models:
                if m["type"] == "embedding":
                    continue
                mid = m["id"].lower()
                if "instruct" in mid or "chat" in mid:
                    chat_candidate = m["id"]
                    break
            
            # Priority 2: Any LLM
            if not chat_candidate:
                for m in models:
                    if m["type"] == "embedding":
                        continue
                    chat_candidate = m["id"]
                    break
            
            if chat_candidate:
                logger.info(f"üöÄ Auto-Loading Local Model: {chat_candidate}")
                # Use -y to accept defaults for variants
                await asyncio.create_subprocess_exec(lms_path, "load", chat_candidate, "--gpu", "auto", "-y")
                await asyncio.sleep(5) # Wait for load
                return True
            else:
                logger.warning("‚ö†Ô∏è No Chat models found in 'lms ls'.")
                return False

        except Exception as e:
            logger.error(f"‚ùå Auto-load failed: {e}")
            return False

    async def list_local_models(self) -> List[str]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (lms ls) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID."""
        models = await self._scan_local_models()
        ids: list[str] = []
        for entry in models:
            identifier = self._extract_model_id(entry)
            if identifier:
                ids.append(identifier)
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –≤ —É—Å—Ç–æ–π—á–∏–≤–æ–º –ø–æ—Ä—è–¥–∫–µ
        return sorted(set(ids))

    async def load_local_model(self, model_name: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ LM Studio —á–µ—Ä–µ–∑ REST API (0.3.x).
        """
        base = self._lm_studio_api_root()
        # –í 0.3.x —ç–Ω–¥–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∑–∫–∏: POST /api/v1/models/load
        url = f"{base}/api/v1/models/load"
        
        try:
            logger.info(f"üöÄ Loading model via REST API: {model_name}")
            timeout = aiohttp.ClientTimeout(total=35)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                payload = {
                    "identifier": model_name,
                    "gpu_offload": "auto"
                }
                async with session.post(url, json=payload, timeout=30) as resp:
                    if resp.status == 200:
                        logger.info(f"‚úÖ REST API Load Success: {model_name}")
                        self.active_local_model = model_name
                        self.is_local_available = True
                        return True
                    else:
                        text = await resp.text()
                        logger.warning(f"‚ö†Ô∏è REST API Load failed ({resp.status}): {text}")
        except Exception as e:
            logger.error(f"‚ùå REST API Load Exception: {e}")

        # Fallback to CLI for backwards compatibility
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if os.path.exists(lms_path):
            try:
                proc = await asyncio.create_subprocess_exec(
                    lms_path, "load", model_name, "--gpu", "auto", "-y"
                )
                await proc.communicate()
                if proc.returncode == 0:
                    self.active_local_model = model_name
                    self.is_local_available = True
                    return True
            except Exception:
                pass
        
        return False

    async def unload_local_model(self, model_name: str = None) -> bool:
        """
        –í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ LM Studio —á–µ—Ä–µ–∑ REST API.
        """
        base = self._lm_studio_api_root()
        url = f"{base}/api/v1/models/unload"
        
        try:
            payload = {}
            if model_name:
                payload["identifier"] = model_name
            
            timeout = aiohttp.ClientTimeout(total=15)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, json=payload) as resp:
                    if resp.status == 200:
                        logger.info(f"‚úÖ REST API Unload Success")
                        if not model_name:
                            self.active_local_model = None
                        return True
        except Exception as e:
            logger.error(f"‚ùå REST API Unload failed: {e}")

        # Fallback to CLI
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if os.path.exists(lms_path):
            try:
                cmd = [lms_path, "unload", "--all"] if not model_name else [lms_path, "unload", model_name]
                proc = await asyncio.create_subprocess_exec(*cmd)
                await proc.communicate()
                return proc.returncode == 0
            except Exception:
                pass
        return False

        # Legacy fallback removed

    async def list_cloud_models(self) -> List[str]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ Cloud –º–æ–¥–µ–ª–∏ (via OpenClaw)."""
        if not self.openclaw_client:
            return ["–û—à–∏–±–∫–∞: OpenClaw –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"]
        
        try:
            raw_models = await self.openclaw_client.get_models()
            models = []
            for m in raw_models:
                # OpenAI format: {"id": "foo", "object": "model"}
                if isinstance(m, dict) and "id" in m:
                    models.append(m["id"])
                # Fallback: simple string list
                elif isinstance(m, str):
                    models.append(m)
            
            self.last_cloud_error = None  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ä—É—é –æ—à–∏–±–∫—É –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
            return sorted(models)
        except Exception as e:
            err_msg = str(e)
            logger.error(f"Cloud scan error: {err_msg}")
            # –ï—Å–ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞, –ø–æ–º–µ—á–∞–µ–º soft cap
            self._mark_cloud_soft_cap_if_needed(err_msg)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model scan
            if self._is_cloud_billing_error(err_msg):
                return [f"‚ùå –û—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞ (Cloud): –û–ø–ª–∞—Ç–∏—Ç–µ —Å—á–µ—Ç –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç–µ API –∫–ª—é—á –≤ .env"]
            return [f"–û—à–∏–±–∫–∞ API: {err_msg}"]

    async def _call_local_llm(self, prompt: str, context: list = None, chat_type: str = "private", is_owner: bool = False) -> str:
        """
        –í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å (aiohttp).
        """
        try:
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt –¥–ª—è –ª–æ–∫–∞–ª–∫–∏
            system_msg = "You are a helpful assistant."
            if self.persona:
                system_msg = self.persona.get_current_prompt(chat_type, is_owner)

            # –í—ã–±–∏—Ä–∞–µ–º URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–≤–∏–∂–∫–∞
            base_url = self.lm_studio_url if self.local_engine == 'lm-studio' else \
                       self.ollama_url.replace('/api', '/v1')

            # –§–æ—Ä–º–∏—Ä—É–µ–º payload
            messages = [{"role": "system", "content": system_msg}]
            if context:
                for idx, msg in enumerate(context):
                    if not isinstance(msg, dict):
                        logger.debug("Skipping context entry (not dict) #%s: %s", idx, type(msg))
                        continue
                    mrole = str(msg.get("role") or "user")
                    content = msg.get("content") or msg.get("text") or msg.get("message")
                    if isinstance(content, list):
                        content = "\n".join(str(item) for item in content if item is not None)
                    elif isinstance(content, dict):
                        content = json.dumps(content, ensure_ascii=False)
                    if content is None:
                        logger.debug("Skipping context entry #%s due to missing content", idx)
                        continue
                    messages.append({"role": mrole, "content": str(content)})
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self.active_local_model or "local-model",
                "messages": messages,
                "temperature": 0.7,
                "include_reasoning": True  # User requested reasoning back
            }

            headers = {"Content-Type": "application/json"}
            
            # –¢–∞–π–º–∞—É—Ç 300—Å –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π
            timeout = aiohttp.ClientTimeout(total=max(300, self.local_timeout_seconds))
            start_t = time.time()

            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/chat/completions", 
                    json=payload, 
                    headers=headers
                ) as response:
                    
                        if response.status == 200:
                            data = await response.json()
                            duration = time.time() - start_t
                            
                            choices = data.get('choices')
                            if choices and len(choices) > 0:
                                content = choices[0].get('message', {}).get('content')
                                reasoning = choices[0].get('message', {}).get('reasoning_content')
                                
                                if content:
                                    logger.info(
                                        "Local LLM success",
                                        duration_sec=round(duration, 2),
                                        char_count=len(content),
                                        has_reasoning=bool(reasoning)
                                    )
                                    return content
                            
                            logger.warning("Local LLM returned empty choices")
                            return None 
                        else:
                            error_text = await response.text()
                            logger.error(f"Local LLM HTTP {response.status}: {error_text}")
                            return None 

        except Exception as e:
            self._stats["local_failures"] += 1
            return None  

    async def route_query(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative', 'moderation', 'security', 'infra', 'review'] = 'chat',
                          context: list = None,
                          chat_type: str = "private",
                          is_owner: bool = False,
                          use_rag: bool = True,
                          preferred_model: Optional[str] = None,
                          confirm_expensive: bool = False):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å Auto-Fallback, RAG –∏ policy-—Ä–æ—É—Ç–∏–Ω–≥–æ–º.
        """

        profile = self.classify_task_profile(prompt, task_type)
        recommendation = self._get_profile_recommendation(profile)
        is_critical = recommendation["critical"]

        # 0. RAG Lookup
        if use_rag and self.rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.1. Tool Orchestration (Phase 6)
        if self.tools:
            tool_data = await self.tools.execute_tool_chain(prompt)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        await self.check_local_health()

        async def _run_local() -> Optional[str]:
            if not self.is_local_available:
                return None
            async with self._acquire_local_slot(self.active_local_model):
                logger.info(
                    "Routing to LOCAL",
                    model=self.active_local_model,
                    profile=profile,
                    tier=self._model_tier(self.active_local_model),
                )
                local_response = await self._call_local_llm(prompt, context, chat_type, is_owner)
                if local_response:
                    self._stats["local_calls"] += 1
                    local_model = self.active_local_model or "local-model"
                    self._remember_model_choice(profile, local_model, "local")
                    self._update_usage_report(profile, local_model, "local")
                    self._remember_last_route(
                        profile=profile,
                        task_type=task_type,
                        channel="local",
                        model_name=local_model,
                        prompt=prompt,
                    )
                return local_response

        async def _run_cloud():
            if self.require_confirm_expensive and is_critical and not confirm_expensive:
                return "confirm_needed", "‚ö†Ô∏è –î–ª—è –∫—Ä–∏—Ç–∏—á–Ω–æ–π –∑–∞–¥–∞—á–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ—Ä–æ–≥–æ–≥–æ –æ–±–ª–∞—á–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞. –ü–æ–≤—Ç–æ—Ä–∏ –∫–æ–º–∞–Ω–¥—É —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º."
            for i, candidate in enumerate(self._build_cloud_candidates(task_type, profile, preferred_model or recommendation.get("model"))):
                logger.info("Routing to CLOUD", model=candidate, profile=profile)
                # –î–ª—è –ø–µ—Ä–≤–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –¥–µ–ª–∞–µ–º —Ä–µ—Ç—Ä–∞–∏, –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –ø—Ä–æ–±—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑ –∏ –∏–¥–µ–º –¥–∞–ª—å—à–µ
                max_retries_cloud = 1 if i == 0 else 0
                response = await self._call_gemini(prompt, candidate, context, chat_type, is_owner, max_retries=max_retries_cloud)
                normalized = (response or "").strip()
                cloud_issue = (
                    self._is_cloud_error_message(normalized) or self._is_cloud_billing_error(normalized)
                )
                if cloud_issue:
                    error_label = normalized or response or "cloud_error"
                    logger.warning("Cloud candidate %s failed: %s", candidate, error_label)
                    self._mark_cloud_soft_cap_if_needed(error_label)
                    self.last_cloud_error = error_label
                    self.last_cloud_model = candidate
                    continue
                self.last_cloud_error = None
                self.last_cloud_model = candidate
                return candidate, response or ""
            return None

        if self.force_mode == "force_local":
            if not self.is_local_available:
                return "‚ùå –†–µ–∂–∏–º 'Force Local' –≤–∫–ª—é—á–µ–Ω, –Ω–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (LM Studio/Ollama offline)."
            forced_local = await _run_local()
            if forced_local:
                return forced_local
            return "‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Force Local active)."

        def _finalize_cloud(candidate: str, response_text: str) -> Optional[str]:
            if not response_text:
                return None
            self._remember_model_choice(profile, candidate, "cloud")
            self._update_usage_report(profile, candidate, "cloud")
            self._remember_last_route(
                profile=profile,
                task_type=task_type,
                channel="cloud",
                model_name=candidate,
                prompt=prompt,
            )
            return response_text

        if self.force_mode == "force_cloud":
            cloud_result = await _run_cloud()
            if isinstance(cloud_result, str):
                return cloud_result
            if cloud_result:
                candidate, response = cloud_result
                finalized = _finalize_cloud(candidate, response)
                if finalized:
                    return finalized
            return self.last_cloud_error or "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –Ω–∏ –æ—Ç –æ–±–ª–∞—á–Ω–æ–π, –Ω–∏ –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏."

        # Soft cap: –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ –æ–±–ª–∞–∫–∞, –Ω–µ-–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ —É–≤–æ–¥–∏–º –≤ –ª–æ–∫–∞–ª–∫—É.
        force_local_due_cost = self.cloud_soft_cap_reached and not is_critical
        prefer_cloud = is_critical or task_type == "reasoning"
        if recommendation.get("channel") == "cloud":
            prefer_cloud = True
        if force_local_due_cost:
            prefer_cloud = False

        local_response: Optional[str] = None
        if not prefer_cloud and self.is_local_available:
            local_response = await _run_local()
            if local_response:
                return local_response

        latest_cloud_error: Optional[str] = None
        cloud_result = await _run_cloud()
        cloud_response = None
        response_model = None
        if isinstance(cloud_result, tuple):
            response_model, cloud_response = cloud_result
        elif isinstance(cloud_result, str):
            cloud_response = cloud_result

        if isinstance(cloud_result, tuple):
            finalized = _finalize_cloud(response_model, cloud_response or "")
            if finalized:
                return finalized
        elif isinstance(cloud_result, str):
            return cloud_result

        # –ï—Å–ª–∏ –æ–±–ª–∞–∫–æ –Ω–µ –¥–∞–ª–æ –æ—Ç–≤–µ—Ç–∞, –ø—ã—Ç–∞–µ–º—Å—è –ª–æ–∫–∞–ª—å–Ω—ã–π fallback.
        if self.is_local_available and not local_response:
            local_response = await _run_local()
            if local_response:
                if is_critical and self.enable_cloud_review_for_critical and self.gemini_client:
                    review_model = self._resolve_cloud_model("reasoning", "review", self.models.get("pro"))
                    review_prompt = (
                        "–ü—Ä–æ–≤–µ–¥–∏ —Å—Ç—Ä–æ–≥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∏ —É–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏.\n\n"
                        f"–ó–∞–ø—Ä–æ—Å:\n{prompt}\n\n"
                        f"–ß–µ—Ä–Ω–æ–≤–æ–π –æ—Ç–≤–µ—Ç:\n{local_response}\n\n"
                        "–í–µ—Ä–Ω–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç."
                    )
                    reviewed = await self._call_gemini(review_prompt, review_model, None, chat_type, is_owner)
                    if reviewed and not reviewed.startswith("‚ùå"):
                        self._remember_model_choice("review", review_model, "cloud")
                        self._update_usage_report("review", review_model, "cloud")
                        self._remember_last_route(
                            profile="review",
                            task_type="reasoning",
                            channel="cloud",
                            model_name=review_model,
                            prompt=review_prompt,
                        )
                        return reviewed
                return local_response

        if not latest_cloud_error:
            latest_cloud_error = self.last_cloud_error
        return latest_cloud_error or "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –Ω–∏ –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π, –Ω–∏ –æ—Ç –æ–±–ª–∞—á–Ω–æ–π –º–æ–¥–µ–ª–∏."

    async def _call_gemini(self, prompt: str, model_name: str, context: list = None,
                           chat_type: str = "private", is_owner: bool = False, max_retries: int = 2) -> str:
        """
        –í—ã–∑–æ–≤ Cloud –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ OpenClaw Gateway.
        """
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt
        from src.core.prompts import get_system_prompt
        base_instructions = get_system_prompt(chat_type == "private")

        persona_prompt = ""
        if self.persona:
            persona_prompt = self.persona.get_current_prompt(chat_type, is_owner)

        system_instructions = f"{persona_prompt}\n\n{base_instructions}".strip()

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è OpenClaw (OpenAI-like format)
        messages = []
        if system_instructions:
            messages.append({"role": "system", "content": system_instructions})
        
        if context:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π
            for msg in context:
                role = msg.get("role", "user")
                # –ú–∞–ø–ø–∏–Ω–≥ —Ä–æ–ª–µ–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ, –Ω–æ –æ–±—ã—á–Ω–æ user/model/assistant —Å–æ–≤–ø–∞–¥–∞—é—Ç
                if role == "model": role = "assistant"
                messages.append({"role": role, "content": msg.get("text", "")})
        
        messages.append({"role": "user", "content": prompt})

        for attempt in range(max_retries + 1):
            try:
                response_text = await self.openclaw_client.chat_completions(messages, model=model_name)

                normalized = (response_text or "").strip()
                error_detected = self._is_cloud_error_message(normalized)
                billing_issue = self._is_cloud_billing_error(normalized)

                if error_detected or billing_issue:
                    self._mark_cloud_soft_cap_if_needed(normalized or "–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    if attempt < max_retries:
                        logger.warning(f"OpenClaw Attempt {attempt+1} failed: {response_text}")
                        await asyncio.sleep(2 ** (attempt + 1))
                        continue
                        
                    if billing_issue:
                        return f"‚ùå –û—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞ (OpenClaw): –ü–æ—Ö–æ–∂–µ, –Ω–∞ –∞–∫–∫–∞—É–Ω—Ç–µ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å —Å—Ä–µ–¥—Å—Ç–≤–∞ –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–ª–∞–Ω—Å –Ω–∞ —à–ª—é–∑–µ. (–î–µ—Ç–∞–ª–∏: {response_text})"
                    return f"‚ùå –û—à–∏–±–∫–∞ Cloud: {response_text}"

                self._stats["cloud_calls"] += 1
                return response_text

            except Exception as e:
                logger.error(f"Cloud call failed: {e}")
                if attempt < max_retries:
                    await asyncio.sleep(2 ** (attempt + 1))
                    continue
                
                self._stats["cloud_failures"] += 1
                return f"‚ùå –û—à–∏–±–∫–∞ Cloud: {e}"

    async def route_query_stream(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative'] = 'chat',
                          context: list = None,
                          chat_type: str = "private",
                          is_owner: bool = False,
                          use_rag: bool = True):
        """
        –í–µ—Ä—Å–∏—è route_query —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (–ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è Cloud).
        """
        # 1. –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∞–µ–º –≤—Å—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É (RAG, Tools) - —Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –≤ route_query
        if use_rag and self.rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        if self.tools:
            tool_data = await self.tools.execute_tool_chain(prompt)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        if self.force_mode == 'force_local' and not self.is_local_available:
             yield "‚ùå –†–µ–∂–∏–º 'Force Local' –≤–∫–ª—é—á–µ–Ω, –Ω–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
             return
             
        if not self.is_local_available and not self.openclaw_client:
             yield "‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –æ—Ñ—Ñ–ª–∞–π–Ω, –æ–±–ª–∞—á–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω)."
             return

        # 3. –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è
        model_name = self.models.get(task_type, self.models["chat"])
        
        # –ï—Å–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ª–æ–∫–∞–ª–∫–∞ –∏–ª–∏ –æ–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ —ç—Ç–æ —á–∞—Ç/–∫–æ–¥
        if self.force_mode == 'force_local' or (self.is_local_available and task_type in ['chat', 'coding']):
             try:
                 full_res = await self.route_query(prompt, task_type, context, chat_type, is_owner, use_rag=False)
                 yield full_res
             except Exception as e:
                 logger.error(f"Fallback routing in stream failed: {e}")
                 yield f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏: {e}"
             return

        # 4. –°—Ç—Ä–∏–º–∏–Ω–≥ —á–µ—Ä–µ–∑ –æ–±–ª–∞–∫–æ (Gemini)
        async for chunk in self._call_gemini_stream(prompt, model_name, context, chat_type, is_owner):
            if chunk:
                yield chunk
            else:
                break

    async def _call_gemini_stream(self, prompt: str, model_name: str, context: list = None,
                                  chat_type: str = "private", is_owner: bool = False):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ Cloud (OpenClaw).
        –ü–æ–∫–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∫–∞–∫ –ø—Å–µ–≤–¥–æ-—Å—Ç—Ä–∏–º–∏–Ω–≥ (–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –∑–∞ —Ä–∞–∑), —Ç–∞–∫ –∫–∞–∫ OpenClawClient.chat_completions –Ω–µ —Å—Ç—Ä–∏–º–∏—Ç.
        """
        # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å stream=True –≤ OpenClawClient
        full_response = await self._call_gemini(prompt, model_name, context, chat_type, is_owner)
        yield full_response

    async def diagnose(self) -> dict:
        """
        –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º.
        """
        result = {}

        # 1. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_ok = await self.check_local_health(force=True)
        
        # Enhanced diagnostics via CLI scan
        local_models = await self._scan_local_models()
        local_count = len(local_models)
        
        local_status = "Offline"
        if local_ok:
            if self.active_local_model:
                local_status = f"{self.local_engine}: {self.active_local_model} ({local_count} models available)"
            else:
                local_status = f"{self.local_engine}: Ready (No Model Loaded, {local_count} available)"
        elif local_count > 0:
             local_status = f"Offline ({local_count} models detected via CLI)"
                
        result["Local AI"] = {
            "ok": local_ok,
            "status": local_status,
            "engine": self.local_engine or "Unknown",
            "model_count": local_count,
            "active_model": self.active_local_model
        }

        # 2. Gemini Cloud (via OpenClaw)
        openclaw_health = await self.openclaw_client.health_check()
        result["Cloud (OpenClaw)"] = {
            "ok": openclaw_health,
            "status": f"Ready ({self.models['chat']})" if openclaw_health else "Unreachable",
        }

        # 3. RAG Engine
        if self.rag:
            try:
                rag_count = self.rag.get_total_documents()
                result["RAG Engine"] = {"ok": True, "status": f"{rag_count} documents"}
            except Exception as e:
                result["RAG Engine"] = {"ok": False, "status": str(e)}
        else:
             result["RAG Engine"] = {"ok": True, "status": "Disabled (OpenClaw)"}

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–∑–æ–≤–æ–≤
        result["Call Stats"] = {
            "ok": True,
            "status": (
                f"Local: {self._stats['local_calls']} ok / {self._stats['local_failures']} fail, "
                f"Cloud: {self._stats['cloud_calls']} ok / {self._stats['cloud_failures']} fail"
            ),
        }

        # 6. Workspace Check
        handover_path = Path(os.getcwd()) / "HANDOVER.md"
        result["üìÅ Workspace"] = {
            "ok": handover_path.exists(),
            "status": f"Root: {os.getcwd()} (HANDOVER.md: {'Found' if handover_path.exists() else 'MISSING'})"
        }

        return result

    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–∏—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model."""
        recommendations = {
            profile: self._get_profile_recommendation(profile)
            for profile in ["chat", "moderation", "code", "security", "infra", "review", "communication"]
        }
        return {
            "cloud_models": self.models.copy(),
            "local_engine": self.local_engine,
            "local_model": self.active_local_model,
            "local_available": self.is_local_available,
            "stats": self._stats.copy(),
            "force_mode": self.force_mode,
            "fallback_models": self.fallback_models,
            "routing_policy": self.routing_policy,
            "cloud_soft_cap_calls": self.cloud_soft_cap_calls,
            "cloud_soft_cap_reached": self.cloud_soft_cap_reached,
            "recommendations": recommendations,
            "usage_report": self._usage_report.copy(),
            "feedback_summary": self.get_feedback_summary(top=3),
        }

    def get_profile_recommendation(self, profile: str = "chat") -> dict:
        """–ü—É–±–ª–∏—á–Ω—ã–π helper –¥–ª—è –ø–æ–∫–∞–∑–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ø—Ä–æ—Ñ–∏–ª—é."""
        return self._get_profile_recommendation(profile)

    def get_last_route(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ —Ä–æ—É—Ç–µ—Ä–∞."""
        store = self._ensure_feedback_store()
        last_route = store.get("last_route", {})
        return dict(last_route) if isinstance(last_route, dict) else {}

    def submit_feedback(
        self,
        score: int,
        profile: str | None = None,
        model_name: str | None = None,
        channel: str | None = None,
        note: str = "",
    ) -> dict:
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ (1-5) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ—ë
        –≤ –ø—Ä–æ—Ñ–∏–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π.
        """
        try:
            normalized_score = int(score)
        except Exception as exc:
            raise ValueError("score_must_be_integer_1_5") from exc
        if normalized_score < 1 or normalized_score > 5:
            raise ValueError("score_out_of_range_1_5")

        store = self._ensure_feedback_store()
        last_route = store.get("last_route", {}) if isinstance(store.get("last_route"), dict) else {}

        resolved_profile = str(profile or last_route.get("profile", "")).strip().lower()
        resolved_model = str(model_name or last_route.get("model", "")).strip()
        resolved_channel = self._normalize_channel(channel or last_route.get("channel"))

        if not resolved_profile or not resolved_model:
            raise ValueError("profile_and_model_required_or_run_task_first")

        profiles = store.setdefault("profiles", {})
        profile_entry = profiles.setdefault(
            resolved_profile,
            {"models": {}, "channels": {}, "feedback_total": 0},
        )
        if not isinstance(profile_entry.get("models"), dict):
            profile_entry["models"] = {}
        if not isinstance(profile_entry.get("channels"), dict):
            profile_entry["channels"] = {}

        model_entry = profile_entry["models"].setdefault(
            resolved_model,
            {"count": 0, "sum": 0, "avg": 0.0, "channels": {}, "last_score": 0, "last_ts": ""},
        )
        model_entry["count"] = int(model_entry.get("count", 0)) + 1
        model_entry["sum"] = int(model_entry.get("sum", 0)) + normalized_score
        model_entry["avg"] = round(model_entry["sum"] / model_entry["count"], 3)
        model_entry["last_score"] = normalized_score
        model_entry["last_ts"] = self._now_iso()
        if not isinstance(model_entry.get("channels"), dict):
            model_entry["channels"] = {}

        model_channel_entry = model_entry["channels"].setdefault(
            resolved_channel,
            {"count": 0, "sum": 0, "avg": 0.0},
        )
        model_channel_entry["count"] = int(model_channel_entry.get("count", 0)) + 1
        model_channel_entry["sum"] = int(model_channel_entry.get("sum", 0)) + normalized_score
        model_channel_entry["avg"] = round(model_channel_entry["sum"] / model_channel_entry["count"], 3)

        profile_channel_entry = profile_entry["channels"].setdefault(
            resolved_channel,
            {"count": 0, "sum": 0, "avg": 0.0},
        )
        profile_channel_entry["count"] = int(profile_channel_entry.get("count", 0)) + 1
        profile_channel_entry["sum"] = int(profile_channel_entry.get("sum", 0)) + normalized_score
        profile_channel_entry["avg"] = round(profile_channel_entry["sum"] / profile_channel_entry["count"], 3)
        profile_entry["feedback_total"] = int(profile_entry.get("feedback_total", 0)) + 1

        events = store.setdefault("events", [])
        if not isinstance(events, list):
            events = []
            store["events"] = events
        events.append(
            {
                "ts": self._now_iso(),
                "score": normalized_score,
                "profile": resolved_profile,
                "model": resolved_model,
                "channel": resolved_channel,
                "note": (note or "").strip()[:240],
            }
        )
        if len(events) > 400:
            del events[: len(events) - 400]

        store["updated_at"] = self._now_iso()
        self._save_json(self._feedback_path, store)
        return {
            "ok": True,
            "score": normalized_score,
            "profile": resolved_profile,
            "model": resolved_model,
            "channel": resolved_channel,
            "used_last_route": bool(not profile and not model_name),
            "profile_model_stats": {
                "count": int(model_entry.get("count", 0)),
                "avg": float(model_entry.get("avg", 0.0)),
            },
            "profile_channel_stats": {
                "count": int(profile_channel_entry.get("count", 0)),
                "avg": float(profile_channel_entry.get("avg", 0.0)),
            },
        }

    def get_feedback_summary(self, profile: str | None = None, top: int = 5) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–≤–æ–¥–∫—É –ø–æ –æ—Ü–µ–Ω–∫–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏.
        """
        safe_top = max(1, min(int(top), 20))
        store = self._ensure_feedback_store()
        profiles = store.get("profiles", {})
        events = store.get("events", [])
        last_route = store.get("last_route", {})

        profile_key = (profile or "").strip().lower() or None
        selected_profiles: list[tuple[str, dict]] = []
        if profile_key:
            selected_profiles.append((profile_key, profiles.get(profile_key, {})))
        else:
            selected_profiles = list(profiles.items())

        top_models: list[dict[str, Any]] = []
        channels_agg: dict[str, dict[str, float]] = {}
        total_feedback = 0

        for profile_name, pdata in selected_profiles:
            if not isinstance(pdata, dict):
                continue
            models = pdata.get("models", {})
            channels = pdata.get("channels", {})
            if not isinstance(models, dict):
                models = {}
            if not isinstance(channels, dict):
                channels = {}

            for model_name, mdata in models.items():
                if not isinstance(mdata, dict):
                    continue
                count = int(mdata.get("count", 0))
                avg = float(mdata.get("avg", 0.0))
                total_feedback += count
                top_models.append(
                    {
                        "profile": profile_name,
                        "model": str(model_name),
                        "count": count,
                        "avg_score": round(avg, 3),
                        "last_score": int(mdata.get("last_score", 0)),
                        "last_ts": str(mdata.get("last_ts", "")),
                    }
                )

            for channel_name, cdata in channels.items():
                if not isinstance(cdata, dict):
                    continue
                entry = channels_agg.setdefault(
                    str(channel_name),
                    {"count": 0, "sum": 0.0},
                )
                ch_count = int(cdata.get("count", 0))
                ch_avg = float(cdata.get("avg", 0.0))
                entry["count"] += ch_count
                entry["sum"] += ch_avg * ch_count

        top_models_sorted = sorted(
            top_models,
            key=lambda item: (float(item.get("avg_score", 0.0)), int(item.get("count", 0))),
            reverse=True,
        )[:safe_top]

        top_channels: list[dict[str, Any]] = []
        for channel_name, cdata in channels_agg.items():
            ccount = int(cdata.get("count", 0))
            csum = float(cdata.get("sum", 0.0))
            avg = (csum / ccount) if ccount > 0 else 0.0
            top_channels.append({"channel": channel_name, "count": ccount, "avg_score": round(avg, 3)})
        top_channels = sorted(
            top_channels,
            key=lambda item: (float(item.get("avg_score", 0.0)), int(item.get("count", 0))),
            reverse=True,
        )[:3]

        recent_events = []
        if isinstance(events, list):
            for item in events[-5:]:
                if isinstance(item, dict):
                    recent_events.append(
                        {
                            "ts": str(item.get("ts", "")),
                            "score": int(item.get("score", 0)),
                            "profile": str(item.get("profile", "")),
                            "model": str(item.get("model", "")),
                            "channel": str(item.get("channel", "")),
                        }
                    )

        return {
            "generated_at": self._now_iso(),
            "profile": profile_key,
            "top_models": top_models_sorted,
            "top_channels": top_channels,
            "total_feedback": total_feedback,
            "recent_events": recent_events,
            "last_route": dict(last_route) if isinstance(last_route, dict) else {},
        }

    def get_task_preflight(
        self,
        prompt: str,
        task_type: str = "chat",
        preferred_model: str | None = None,
        confirm_expensive: bool = False,
    ) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç preflight-–ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –¥–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞:
        - –ø—Ä–æ—Ñ–∏–ª—å –∏ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å;
        - –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª/–º–æ–¥–µ–ª—å;
        - —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è confirm-step;
        - –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è/—Ä–∏—Å–∫–∏;
        - –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è –º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å.
        """
        normalized_prompt = (prompt or "").strip()
        normalized_task_type = (task_type or "chat").strip().lower() or "chat"
        profile = self.classify_task_profile(normalized_prompt, normalized_task_type)
        recommendation = self._get_profile_recommendation(profile)
        is_critical = bool(recommendation.get("critical"))

        chosen_channel = recommendation.get("channel", "local")
        if self.force_mode == "force_local":
            chosen_channel = "local"
        elif self.force_mode == "force_cloud":
            chosen_channel = "cloud"
        else:
            prefer_cloud = is_critical or normalized_task_type == "reasoning"
            if recommendation.get("channel") == "cloud":
                prefer_cloud = True
            if self.cloud_soft_cap_reached and not is_critical:
                prefer_cloud = False
            chosen_channel = "cloud" if prefer_cloud else "local"

        if chosen_channel == "cloud":
            chosen_model = self._resolve_cloud_model(
                normalized_task_type,
                profile,
                preferred_model or recommendation.get("model"),
            )
        else:
            chosen_model = self.active_local_model or "local-auto"

        requires_confirm = bool(
            self.require_confirm_expensive and is_critical and chosen_channel == "cloud" and not confirm_expensive
        )
        can_run_now = not requires_confirm

        warnings: list[str] = []
        if chosen_channel == "local" and not self.is_local_available:
            warnings.append("–õ–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª —Å–µ–π—á–∞—Å offline; –≤–æ–∑–º–æ–∂–µ–Ω fallback –≤ cloud.")
        if self.cloud_soft_cap_reached and chosen_channel == "cloud":
            warnings.append("Cloud soft cap —É–∂–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç: –ø—Ä–æ–≤–µ—Ä—å policy/–ª–∏–º–∏—Ç—ã –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º.")
        if requires_confirm:
            warnings.append("–î–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω confirm-step (`--confirm-expensive`).")
        feedback_hint = recommendation.get("feedback_hint", {})
        feedback_count = int(feedback_hint.get("count", 0)) if isinstance(feedback_hint, dict) else 0
        feedback_avg = float(feedback_hint.get("avg_score", 0.0)) if isinstance(feedback_hint, dict) else 0.0
        if feedback_count >= 3 and feedback_avg <= 2.5:
            warnings.append(
                f"–£ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∏–∑–∫–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥ ({feedback_avg}/5); "
                "—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º."
            )

        marginal_cost_usd = (
            float(self.cloud_cost_per_call_usd)
            if chosen_channel == "cloud"
            else float(self.local_cost_per_call_usd)
        )

        reasons: list[str] = []
        if is_critical:
            reasons.append("–ö—Ä–∏—Ç–∏—á–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –∑–∞–¥–∞—á–∏.")
        if normalized_task_type == "reasoning":
            reasons.append("Reasoning-–∑–∞–¥–∞—á–∞ —Å –ø–æ–≤—ã—à–µ–Ω–Ω—ã–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞.")
        if self.force_mode == "force_local":
            reasons.append("–í–∫–ª—é—á–µ–Ω –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º force_local.")
        elif self.force_mode == "force_cloud":
            reasons.append("–í–∫–ª—é—á–µ–Ω –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º force_cloud.")
        if self.cloud_soft_cap_reached and not is_critical:
            reasons.append("Cloud soft cap –∞–∫—Ç–∏–≤–µ–Ω: non-critical –∑–∞–¥–∞—á–∏ —Å–¥–≤–∏–Ω—É—Ç—ã –≤ local.")
        if feedback_count >= 2:
            reasons.append(
                f"–ò—Å—Ç–æ—Ä–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –º–æ–¥–µ–ª–∏: {feedback_avg}/5 –Ω–∞ {feedback_count} –æ—Ü–µ–Ω–∫–∞—Ö."
            )
        if not reasons:
            reasons.append("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è policy free-first hybrid.")

        return {
            "generated_at": self._now_iso(),
            "task_type": normalized_task_type,
            "profile": profile,
            "critical": is_critical,
            "prompt_preview": normalized_prompt[:240],
            "recommendation": recommendation,
            "execution": {
                "channel": chosen_channel,
                "model": chosen_model,
                "can_run_now": can_run_now,
                "requires_confirm_expensive": requires_confirm,
                "confirm_expensive_received": bool(confirm_expensive),
            },
            "policy": {
                "routing_policy": self.routing_policy,
                "force_mode": self.force_mode,
                "cloud_soft_cap_reached": bool(self.cloud_soft_cap_reached),
                "local_available": bool(self.is_local_available),
            },
            "cost_hint": {
                "marginal_call_cost_usd": round(marginal_cost_usd, 6),
                "cloud_cost_per_call_usd": float(self.cloud_cost_per_call_usd),
                "local_cost_per_call_usd": float(self.local_cost_per_call_usd),
            },
            "warnings": warnings,
            "reasons": reasons,
            "next_step": (
                "–ó–∞–ø—É—Å—Ç–∏ –∑–∞–¥–∞—á—É —Å —Ñ–ª–∞–≥–æ–º --confirm-expensive."
                if requires_confirm
                else "–ú–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –∑–∞–¥–∞—á—É."
            ),
        }

    def get_usage_summary(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π usage-—Å—Ä–µ–∑ –¥–ª—è Ops –ø–∞–Ω–µ–ª–∏ –∏ –∞–ª–µ—Ä—Ç–æ–≤.
        """
        channels = self._usage_report.get("channels", {}) if isinstance(self._usage_report, dict) else {}
        local_calls = int(channels.get("local", 0))
        cloud_calls = int(channels.get("cloud", 0))
        total_calls = local_calls + cloud_calls

        cloud_share = round((cloud_calls / total_calls), 3) if total_calls > 0 else 0.0
        local_share = round((local_calls / total_calls), 3) if total_calls > 0 else 0.0

        models = self._usage_report.get("models", {}) if isinstance(self._usage_report, dict) else {}
        top_models = sorted(
            ((name, int(count)) for name, count in models.items()),
            key=lambda item: item[1],
            reverse=True,
        )[:5]
        profiles = self._usage_report.get("profiles", {}) if isinstance(self._usage_report, dict) else {}
        top_profiles = sorted(
            ((name, int(count)) for name, count in profiles.items()),
            key=lambda item: item[1],
            reverse=True,
        )[:5]

        cloud_remaining = max(0, int(self.cloud_soft_cap_calls) - cloud_calls)
        return {
            "totals": {
                "all_calls": total_calls,
                "local_calls": local_calls,
                "cloud_calls": cloud_calls,
            },
            "ratios": {
                "local_share": local_share,
                "cloud_share": cloud_share,
            },
            "soft_cap": {
                "cloud_soft_cap_calls": int(self.cloud_soft_cap_calls),
                "cloud_soft_cap_reached": bool(self.cloud_soft_cap_reached),
                "cloud_remaining_calls": cloud_remaining,
            },
            "top_models": [{"model": name, "count": count} for name, count in top_models],
            "top_profiles": [{"profile": name, "count": count} for name, count in top_profiles],
        }

    def get_ops_alerts(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤ –∏ –æ–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ops-–∫–æ–Ω—Ç—É—Ä–∞.
        """
        summary = self.get_usage_summary()
        alerts: list[dict[str, str]] = []
        cloud_calls = int(summary["totals"]["cloud_calls"])
        local_calls = int(summary["totals"]["local_calls"])
        soft_cap = int(summary["soft_cap"]["cloud_soft_cap_calls"])
        remaining = int(summary["soft_cap"]["cloud_remaining_calls"])
        cloud_share = float(summary["ratios"]["cloud_share"])

        if bool(summary["soft_cap"]["cloud_soft_cap_reached"]):
            alerts.append(
                {
                    "severity": "high",
                    "code": "cloud_soft_cap_reached",
                    "message": "–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç CLOUD_SOFT_CAP_CALLS, –Ω–µ-–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ —É–π–¥—É—Ç –≤ –ª–æ–∫–∞–ª–∫—É.",
                }
            )
        elif soft_cap > 0 and cloud_calls >= int(soft_cap * 0.8):
            alerts.append(
                {
                    "severity": "medium",
                    "code": "cloud_soft_cap_near",
                    "message": f"Cloud usage –±–ª–∏–∑–∫–æ –∫ –ª–∏–º–∏—Ç—É: –æ—Å—Ç–∞–ª–æ—Å—å {remaining} –≤—ã–∑–æ–≤–æ–≤.",
                }
            )

        if cloud_calls >= 20 and cloud_share >= 0.75:
            alerts.append(
                {
                    "severity": "medium",
                    "code": "cloud_share_high",
                    "message": "–í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –æ–±–ª–∞—á–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤; –ø—Ä–æ–≤–µ—Ä—å –ø–æ–ª–∏—Ç–∏–∫—É free-first –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏.",
                }
            )

        if local_calls == 0 and cloud_calls > 0:
            alerts.append(
                {
                    "severity": "low",
                    "code": "local_usage_absent",
                    "message": "–õ–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è; –ø—Ä–æ–≤–µ—Ä—å LM Studio/Ollama –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é.",
                }
            )

        # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π guardrail: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏.
        store = self._ensure_feedback_store()
        low_quality_models: list[str] = []
        feedback_profiles = store.get("profiles", {})
        if isinstance(feedback_profiles, dict):
            for profile_name, pdata in feedback_profiles.items():
                if not isinstance(pdata, dict):
                    continue
                models = pdata.get("models", {})
                if not isinstance(models, dict):
                    continue
                for model_name, mdata in models.items():
                    if not isinstance(mdata, dict):
                        continue
                    mcount = int(mdata.get("count", 0))
                    mavg = float(mdata.get("avg", 0.0))
                    if mcount >= 3 and mavg <= 2.5:
                        low_quality_models.append(f"{profile_name}:{model_name}({mavg}/5, n={mcount})")
                        if len(low_quality_models) >= 2:
                            break
                if len(low_quality_models) >= 2:
                    break
        if low_quality_models:
            alerts.append(
                {
                    "severity": "medium",
                    "code": "model_quality_degraded",
                    "message": "–ï—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–∏–º user-feedback: " + "; ".join(low_quality_models),
                }
            )

        # –ë—é–¥–∂–µ—Ç–Ω—ã–µ guardrails (–æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ forecast –≤—ã–∑–æ–≤–æ–≤).
        cost_report = self.get_cost_report(monthly_calls_forecast=self.monthly_calls_forecast)
        monthly = cost_report.get("monthly_forecast", {})
        forecast_total = float(monthly.get("forecast_total_cost", 0.0))
        budget = max(0.0, float(self.cloud_monthly_budget_usd))
        if budget > 0:
            ratio = forecast_total / budget if budget else 0.0
            if ratio >= 1.0:
                alerts.append(
                    {
                        "severity": "high",
                        "code": "cloud_budget_exceeded_forecast",
                        "message": (
                            f"–ü—Ä–æ–≥–Ω–æ–∑ –æ–±–ª–∞—á–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ ({forecast_total:.2f}$) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –±—é–¥–∂–µ—Ç "
                            f"({budget:.2f}$) –Ω–∞ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ—Ñ–∏–ª–µ –Ω–∞–≥—Ä—É–∑–∫–∏."
                        ),
                    }
                )
            elif ratio >= 0.9:
                alerts.append(
                    {
                        "severity": "medium",
                        "code": "cloud_budget_near_forecast",
                        "message": (
                            f"–ü—Ä–æ–≥–Ω–æ–∑ –æ–±–ª–∞—á–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ ({forecast_total:.2f}$) –±–ª–∏–∑–æ–∫ –∫ –±—é–¥–∂–µ—Ç—É "
                            f"({budget:.2f}$)."
                        ),
                    }
                )

        acknowledged = self._ops_state.get("acknowledged", {}) if isinstance(self._ops_state, dict) else {}
        for alert in alerts:
            code = str(alert.get("code", "")).strip()
            ack_meta = acknowledged.get(code, {})
            if isinstance(ack_meta, dict) and ack_meta:
                alert["acknowledged"] = True
                alert["ack"] = {
                    "ts": str(ack_meta.get("ts", "")),
                    "actor": str(ack_meta.get("actor", "")),
                    "note": str(ack_meta.get("note", "")),
                }
            else:
                alert["acknowledged"] = False

        payload = {
            "status": "alert" if alerts else "ok",
            "alerts": alerts,
            "summary": summary,
            "cost_report": cost_report,
        }
        self._append_ops_history(payload)
        return payload

    def get_cost_report(self, monthly_calls_forecast: int = 5000) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–æ—á–Ω—ã–π cost-report –ø–æ —Ç–µ–∫—É—â–µ–º—É usage.
        """
        summary = self.get_usage_summary()
        totals = summary.get("totals", {})
        local_calls = int(totals.get("local_calls", 0))
        cloud_calls = int(totals.get("cloud_calls", 0))
        total_calls = int(totals.get("all_calls", local_calls + cloud_calls))

        cloud_cost = round(cloud_calls * float(self.cloud_cost_per_call_usd), 6)
        local_cost = round(local_calls * float(self.local_cost_per_call_usd), 6)
        total_cost = round(cloud_cost + local_cost, 6)
        avg_cost_per_call = round((total_cost / total_calls), 6) if total_calls > 0 else 0.0

        forecast = max(0, int(monthly_calls_forecast))
        cloud_share = float(summary.get("ratios", {}).get("cloud_share", 0.0))
        local_share = float(summary.get("ratios", {}).get("local_share", 0.0))
        forecast_cloud_calls = round(forecast * cloud_share)
        forecast_local_calls = round(forecast * local_share)
        forecast_cloud_cost = round(forecast_cloud_calls * float(self.cloud_cost_per_call_usd), 6)
        forecast_local_cost = round(forecast_local_calls * float(self.local_cost_per_call_usd), 6)
        forecast_total_cost = round(forecast_cloud_cost + forecast_local_cost, 6)

        return {
            "costs_usd": {
                "cloud_calls_cost": cloud_cost,
                "local_calls_cost": local_cost,
                "total_cost": total_cost,
                "avg_cost_per_call": avg_cost_per_call,
            },
            "pricing": {
                "cloud_cost_per_call_usd": float(self.cloud_cost_per_call_usd),
                "local_cost_per_call_usd": float(self.local_cost_per_call_usd),
            },
            "monthly_forecast": {
                "forecast_calls": forecast,
                "forecast_cloud_calls": forecast_cloud_calls,
                "forecast_local_calls": forecast_local_calls,
                "forecast_cloud_cost": forecast_cloud_cost,
                "forecast_local_cost": forecast_local_cost,
                "forecast_total_cost": forecast_total_cost,
            },
            "usage_summary": summary,
            "budget": {
                "cloud_monthly_budget_usd": float(self.cloud_monthly_budget_usd),
                "forecast_ratio": round((forecast_total_cost / float(self.cloud_monthly_budget_usd)), 4)
                if float(self.cloud_monthly_budget_usd) > 0
                else 0.0,
            },
        }

    def acknowledge_ops_alert(self, code: str, actor: str = "owner", note: str = "") -> dict:
        """–ü–æ–º–µ—á–∞–µ—Ç alert –∫–∞–∫ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º."""
        normalized_code = (code or "").strip()
        if not normalized_code:
            raise ValueError("code_required")

        ack = self._ops_state.setdefault("acknowledged", {})
        ack[normalized_code] = {
            "ts": self._now_iso(),
            "actor": (actor or "owner").strip() or "owner",
            "note": (note or "").strip(),
        }
        self._save_json(self._ops_state_path, self._ops_state)
        return {"ok": True, "code": normalized_code, "ack": ack[normalized_code]}

    def clear_ops_alert_ack(self, code: str) -> dict:
        """–°–Ω–∏–º–∞–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ alert –∫–æ–¥–∞."""
        normalized_code = (code or "").strip()
        if not normalized_code:
            raise ValueError("code_required")

        ack = self._ops_state.setdefault("acknowledged", {})
        existed = normalized_code in ack
        ack.pop(normalized_code, None)
        self._save_json(self._ops_state_path, self._ops_state)
        return {"ok": True, "code": normalized_code, "removed": existed}

    def get_ops_history(self, limit: int = 30) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é ops snapshot-–æ–≤."""
        safe_limit = max(1, min(int(limit), 200))
        history = self._ops_state.get("history", []) if isinstance(self._ops_state, dict) else []
        if not isinstance(history, list):
            history = []
        return {
            "items": history[-safe_limit:],
            "count": min(len(history), safe_limit),
            "total": len(history),
        }

    def get_ops_report(self, history_limit: int = 20, monthly_calls_forecast: int | None = None) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–¥–∏–Ω—ã–π ops-–æ—Ç—á–µ—Ç –¥–ª—è API/–∫–æ–º–∞–Ω–¥:
        usage + alerts + costs + history.
        """
        forecast = int(monthly_calls_forecast) if monthly_calls_forecast is not None else int(self.monthly_calls_forecast)
        usage = self.get_usage_summary()
        alerts = self.get_ops_alerts()
        costs = self.get_cost_report(monthly_calls_forecast=forecast)
        history = self.get_ops_history(limit=history_limit)
        return {
            "generated_at": self._now_iso(),
            "usage": usage,
            "alerts": alerts,
            "costs": costs,
            "history": history,
        }

    def get_ops_executive_summary(self, monthly_calls_forecast: int | None = None) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π executive summary –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞:
        KPI, —Ä–∏—Å–∫–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –æ–¥–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ.
        """
        forecast = int(monthly_calls_forecast) if monthly_calls_forecast is not None else int(self.monthly_calls_forecast)
        usage = self.get_usage_summary()
        alerts_payload = self.get_ops_alerts()
        alerts = alerts_payload.get("alerts", [])
        costs = self.get_cost_report(monthly_calls_forecast=forecast)

        totals = usage.get("totals", {})
        ratios = usage.get("ratios", {})
        soft_cap = usage.get("soft_cap", {})
        budget = costs.get("budget", {})
        monthly = costs.get("monthly_forecast", {})

        severities = [str(item.get("severity", "low")).lower() for item in alerts if isinstance(item, dict)]
        risk_level = "low"
        if "high" in severities:
            risk_level = "high"
        elif "medium" in severities:
            risk_level = "medium"

        recommendations: list[str] = []
        cloud_share = float(ratios.get("cloud_share", 0.0))
        budget_ratio = float(budget.get("forecast_ratio", 0.0))
        alert_codes = {
            str(item.get("code", ""))
            for item in alerts
            if isinstance(item, dict)
        }
        if bool(soft_cap.get("cloud_soft_cap_reached")):
            recommendations.append("–°–Ω–∏–∑–∏—Ç—å cloud-–Ω–∞–≥—Ä—É–∑–∫—É: —É–≤–µ—Å—Ç–∏ non-critical –ø—Ä–æ—Ñ–∏–ª–∏ –≤ local.")
        elif cloud_share >= 0.75:
            recommendations.append("–ü–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏ routing policy: —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ–ª—é cloud.")
        if budget_ratio >= 1.0:
            recommendations.append("–°—Ä–æ—á–Ω–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –º–µ—Å—è—á–Ω—ã–π forecast/budget –∏–ª–∏ –ø–æ–Ω–∏–∑–∏—Ç—å cloud —Ç–∞—Ä–∏—Ñ –∑–∞–¥–∞—á.")
        elif budget_ratio >= 0.9:
            recommendations.append("–ë—é–¥–∂–µ—Ç –Ω–∞ –≥—Ä–∞–Ω–∏: –ø—Ä–∏–º–µ–Ω–∏—Ç—å throttling –¥–æ—Ä–æ–≥–∏—Ö cloud –ø—Ä–æ–≥–æ–Ω–æ–≤.")
        if "model_quality_degraded" in alert_codes:
            recommendations.append("–û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã: —É —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —É—Å—Ç–æ–π—á–∏–≤–æ –Ω–∏–∑–∫–∏–π feedback.")
        if int(totals.get("local_calls", 0)) == 0 and int(totals.get("cloud_calls", 0)) > 0:
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å LM Studio/Ollama: –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª —Å–µ–π—á–∞—Å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")
        if not recommendations:
            recommendations.append("–ö–æ–Ω—Ç—É—Ä —Å—Ç–∞–±–∏–ª—å–Ω—ã–π: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ç–µ–∫—É—â—É—é policy –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥.")

        return {
            "generated_at": self._now_iso(),
            "risk_level": risk_level,
            "kpi": {
                "calls_total": int(totals.get("all_calls", 0)),
                "cloud_share": cloud_share,
                "forecast_total_cost": float(monthly.get("forecast_total_cost", 0.0)),
                "budget_ratio": budget_ratio,
                "active_alerts": len(alerts),
            },
            "alerts_brief": [
                {
                    "severity": str(item.get("severity", "info")),
                    "code": str(item.get("code", "")),
                    "acknowledged": bool(item.get("acknowledged", False)),
                }
                for item in alerts[:8]
                if isinstance(item, dict)
            ],
            "recommendations": recommendations[:6],
        }

    def prune_ops_history(self, max_age_days: int = 30, keep_last: int = 100) -> dict:
        """
        –û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é ops snapshot:
        - —É–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ max_age_days,
        - –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–∏–Ω–∏–º—É–º keep_last –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π.
        """
        safe_age_days = max(1, int(max_age_days))
        safe_keep_last = max(1, int(keep_last))
        history = self._ops_state.get("history", []) if isinstance(self._ops_state, dict) else []
        if not isinstance(history, list):
            history = []

        before_count = len(history)
        if before_count == 0:
            return {
                "ok": True,
                "before": 0,
                "after": 0,
                "removed": 0,
                "max_age_days": safe_age_days,
                "keep_last": safe_keep_last,
            }

        cutoff_ts = datetime.now(timezone.utc).timestamp() - (safe_age_days * 86400)
        forced_keep_indices = set(range(max(0, before_count - safe_keep_last), before_count))
        kept: list[dict[str, Any]] = []

        for idx, item in enumerate(history):
            if idx in forced_keep_indices:
                kept.append(item)
                continue
            ts_raw = str(item.get("ts", "")).strip()
            if not ts_raw:
                continue
            ts_norm = ts_raw.replace("Z", "+00:00")
            try:
                item_ts = datetime.fromisoformat(ts_norm).timestamp()
            except Exception:
                # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ timestamp-—ã —É–±–∏—Ä–∞–µ–º –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ.
                continue
            if item_ts >= cutoff_ts:
                kept.append(item)

        self._ops_state["history"] = kept
        self._save_json(self._ops_state_path, self._ops_state)
        after_count = len(kept)
        return {
            "ok": True,
            "before": before_count,
            "after": after_count,
            "removed": max(0, before_count - after_count),
            "max_age_days": safe_age_days,
            "keep_last": safe_keep_last,
        }

    def _append_ops_history(self, payload: dict) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—Ä–∞—Ç–∫–∏–π snapshot ops-–∞–ª–µ—Ä—Ç–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏—é."""
        history = self._ops_state.setdefault("history", [])
        if not isinstance(history, list):
            self._ops_state["history"] = []
            history = self._ops_state["history"]

        alerts = payload.get("alerts", []) if isinstance(payload, dict) else []
        snapshot = {
            "ts": self._now_iso(),
            "status": str(payload.get("status", "unknown")),
            "alerts_count": len(alerts) if isinstance(alerts, list) else 0,
            "codes": [str(item.get("code", "")) for item in (alerts or []) if isinstance(item, dict)],
            "cloud_calls": int(payload.get("summary", {}).get("totals", {}).get("cloud_calls", 0)),
            "local_calls": int(payload.get("summary", {}).get("totals", {}).get("local_calls", 0)),
        }
        history.append(snapshot)
        if len(history) > 500:
            del history[: len(history) - 500]
        self._save_json(self._ops_state_path, self._ops_state)

    def _now_iso(self) -> str:
        return datetime.now(timezone.utc).isoformat(timespec="seconds")

    def get_ram_usage(self) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM —á–µ—Ä–µ–∑ SystemMonitor.
        """
        try:
            from src.utils.system_monitor import SystemMonitor
            snapshot = SystemMonitor.get_snapshot()
            return {
                "total_gb": round(snapshot.ram_total_gb, 1),
                "used_gb": round(snapshot.ram_used_gb, 1),
                "available_gb": round(snapshot.ram_available_gb, 1),
                "percent": snapshot.ram_percent,
                "can_load_heavy": SystemMonitor.can_load_heavy_model()
            }
        except Exception as e:
            return {"error": str(e), "can_load_heavy": True}
