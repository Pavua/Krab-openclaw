# -*- coding: utf-8 -*-
"""
Model Manager (Router) –¥–ª—è Krab v6.5.
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Cloud vs Local).

–°—Ç—Ä–∞—Ç–µ–≥–∏—è: Local First ‚Üí Cloud Fallback.
- –ü—Ä–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio/Ollama ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö (–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å + —Å–∫–æ—Ä–æ—Å—Ç—å)
- –ü—Ä–∏ –æ—à–∏–±–∫–µ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ Gemini Cloud
- RAG –∏ Tool Orchestration —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –ö–ê–ñ–î–´–ô –∑–∞–ø—Ä–æ—Å
"""

import os
import time
import asyncio
import aiohttp
from typing import Literal, Optional, Dict, Any, List
from src.core.rag_engine import RAGEngine

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
import structlog
logger = structlog.get_logger("ModelRouter")

# Gemini SDK ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏–º–ø–æ—Ä—Ç–µ
try:
    from google import genai
    from google.genai import types
    _GENAI_AVAILABLE = True
except ImportError:
    _GENAI_AVAILABLE = False
    genai = None

class ModelRouter:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.lm_studio_url = config.get("LM_STUDIO_URL", "http://localhost:1234/v1")
        self.ollama_url = config.get("OLLAMA_URL", "http://localhost:11434/api")
        self.gemini_key = config.get("GEMINI_API_KEY")
        # Default model for OpenClaw Gateway (Subscribed sessions)
        self.openclaw_model = config.get("OPENCLAW_MODEL", "google/gemini-2.0-flash")
        
        # Priority mapping for premium subscriptions
        self.premium_mapping = {
            "chat": "openai/gpt-4o",          # ChatGPT Plus Session
            "coding": "openai/gpt-4o",        # GPT-4o for code
            "reasoning": "google/gemini-2.0-flash", # Gemini Advanced Session
            "creative": "google/gemini-2.0-flash"
        }
        # OpenClaw Config
        self.openclaw_url = config.get("OPENCLAW_URL")
        self.openclaw_token = config.get("OPENCLAW_TOKEN")

        # –°—Ç–∞—Ç—É—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.is_local_available = False
        self.local_engine = None  # 'lm-studio' or 'ollama'
        self.active_local_model = None
        
        self.is_openclaw_available = False

        # –ö–µ—à –¥–ª—è health-check (—á—Ç–æ–±—ã –Ω–µ –¥—ë—Ä–≥–∞—Ç—å API –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å)
        self._health_cache_ts = 0
        self._health_cache_ttl = 30  # —Å–µ–∫—É–Ω–¥
        
        self._openclaw_cache_ts = 0

        # Gemini SDK ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º –û–î–ò–ù –†–ê–ó
        self.gemini_client = None
        if _GENAI_AVAILABLE and self.gemini_key:
            try:
                self.gemini_client = genai.Client(api_key=self.gemini_key)
                logger.info("‚òÅÔ∏è Gemini SDK configured successfully (google.genai)")
            except Exception as e:
                logger.error(f"‚ùå Failed to init Gemini SDK: {e}")

        # RAG Engine
        self.rag = RAGEngine()

        # Persona Manager (–Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py)
        self.persona = None
        self.tools = None  # –ù–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py (ToolHandler)

        # –ü—É–ª –º–æ–¥–µ–ª–µ–π ‚Äî —á–∏—Ç–∞–µ–º –∏–∑ .env, –¥–µ—Ñ–æ–ª—Ç—ã –∫–∞–∫ fallback
        self.models = {
            "chat": config.get("GEMINI_CHAT_MODEL", "gemini-2.0-flash"),
            "thinking": config.get("GEMINI_THINKING_MODEL", "gemini-2.0-flash-thinking-exp"),
            "pro": config.get("GEMINI_PRO_MODEL", "gemini-2.0-pro-exp"),
            "coding": config.get("GEMINI_CODING_MODEL", "gemini-2.0-flash"),
        }

        # –°—á—ë—Ç—á–∏–∫–∏ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
        self._stats = {
            "local_calls": 0,
            "cloud_calls": 0,
            "openclaw_calls": 0,
            "local_failures": 0,
            "cloud_failures": 0,
            "openclaw_failures": 0,
        }

    async def check_local_health(self, force: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–ø—É—â–µ–Ω –ª–∏ LM Studio –∏–ª–∏ Ollama.
        –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–µ—à–∏—Ä—É–µ—Ç—Å—è –Ω–∞ _health_cache_ttl —Å–µ–∫—É–Ω–¥.
        force=True ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–∞.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à (TTL 30—Å) ‚Äî –Ω–µ –¥—ë—Ä–≥–∞–µ–º API –Ω–∞ –∫–∞–∂–¥—ã–π route_query
        now = time.time()
        if not force and (now - self._health_cache_ts) < self._health_cache_ttl:
            return self.is_local_available

        self._health_cache_ts = now

        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º LM Studio (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        try:
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # –í–ê–ñ–ù–û: LM Studio API —Å–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI.
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL: —É–±–∏—Ä–∞–µ–º /v1 –µ—Å–ª–∏ –æ–Ω —É–∂–µ –µ—Å—Ç—å, —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å
                base = self.lm_studio_url.rstrip('/')
                if base.endswith('/v1'):
                    base = base[:-3]  # –£–±–∏—Ä–∞–µ–º /v1 —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ /v1/v1
                url = f"{base}/v1/models"
                
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.local_engine = 'lm-studio'
                            self.is_local_available = True
                            logger.info(f"Local AI Available (LM Studio): {self.active_local_model}")
                            return True
        except Exception:
            pass

        # 2. –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º Ollama
        try:
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{self.ollama_url.replace('/api', '/v1')}/models") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.local_engine = 'ollama'
                            self.is_local_available = True
                            logger.info(f"Local AI Available (Ollama): {self.active_local_model}")
                            return True
        except Exception:
            pass

        self.is_local_available = False
        self.local_engine = None
        self.active_local_model = None
        return False

    async def check_openclaw_health(self, force: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ OpenClaw Gateway.
        """
        if not self.openclaw_url:
            self.is_openclaw_available = False
            return False

        now = time.time()
        if not force and (now - self._openclaw_cache_ts) < self._health_cache_ttl:
            return self.is_openclaw_available
        
        self._openclaw_cache_ts = now

        try:
            timeout = aiohttp.ClientTimeout(total=3)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                url = f"{self.openclaw_url.rstrip('/')}/health"
                async with session.get(url) as response:
                    if response.status == 200 or response.status == 503:
                        self.is_openclaw_available = True
                        status_msg = "ONLINE (UI Missing)" if response.status == 503 else "ONLINE"
                        logger.info(f"‚úÖ OpenClaw Gateway is {status_msg}")
                        return True
        except Exception as e:
            logger.debug(f"OpenClaw health check failed: {e}")
        
        self.is_openclaw_available = False
        return False

    async def _call_local_llm(self, prompt: str, context: list = None, is_private: bool = True) -> str:
        """
        –í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å (aiohttp).
        """
        try:
            # –í—ã–±–∏—Ä–∞–µ–º URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–≤–∏–∂–∫–∞
            base_url = self.lm_studio_url if self.local_engine == 'lm-studio' else \
                       self.ollama_url.replace('/api', '/v1')

            # –§–æ—Ä–º–∏—Ä—É–µ–º payload
            messages = []
            if context:
                messages.extend(context)
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self.active_local_model or "local-model",
                "messages": messages,
                "temperature": 0.7
            }

            headers = {"Content-Type": "application/json"}
            
            # –¢–∞–π–º–∞—É—Ç –ø–æ–±–æ–ª—å—à–µ –¥–ª—è –ª–æ–∫–∞–ª–∫–∏
            timeout = aiohttp.ClientTimeout(total=60)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/chat/completions", 
                    json=payload, 
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        # LOGGING RAW RESPONSE FOR DEBUGGING
                        logger.info(f"Local LLM Raw Response: {data}")
                        
                        # –ó–∞—â–∏—Ç–∞ –æ—Ç NoneType errors
                        choices = data.get('choices')
                        if choices and len(choices) > 0:
                            content = choices[0].get('message', {}).get('content')
                            if content:
                                return content
                        
                        logger.error(f"Local LLM Invalid Response: {data}")
                        return None # Return None to trigger fallback
                    else:
                        error_text = await response.text()
                        logger.error(f"Local LLM HTTP {response.status}: {error_text}")
                        return None # Return None to trigger fallback

        except Exception as e:
            import traceback
            self._stats["local_failures"] += 1
            logger.error(f"Local LLM Connection Error: {e}\n{traceback.format_exc()}")
            return None  # Return None to trigger fallback

    async def _call_openclaw(self, prompt: str, task_type: str, context: list = None, is_private: bool = True) -> str:
        """
        –í—ã–∑–æ–≤ OpenClaw Gateway.
        """
        try:
            messages = []
            # System prompt handling
            from src.core.prompts import get_system_prompt
            base_instructions = get_system_prompt(is_private)
            
            if self.persona:
                persona_prompt = self.persona.get_current_prompt()
                base_instructions = f"{persona_prompt}\n\n{base_instructions}"
            
            messages.append({"role": "system", "content": base_instructions})
            
            if context:
                messages.extend(context)
            messages.append({"role": "user", "content": prompt})

            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–º–∏—É–º-–º–∞–ø–ø–∏–Ω–≥ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
            # –∏–ª–∏ –±–µ—Ä–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            model_id = self.premium_mapping.get(task_type, self.openclaw_model)

            payload = {
                "model": model_id,
                "messages": messages,
                "temperature": 0.7,
                "stream": False 
            }

            headers = {
                "Authorization": f"Bearer {self.openclaw_token}",
                "Content-Type": "application/json"
            }
            
            timeout = aiohttp.ClientTimeout(total=120) # OpenClaw –º–æ–∂–µ—Ç –¥—É–º–∞—Ç—å –¥–æ–ª–≥–æ (reasoning)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                url = f"{self.openclaw_url.rstrip('/')}/v1/chat/completions"
                async with session.post(url, json=payload, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        choices = data.get('choices')
                        if choices and len(choices) > 0:
                            self._stats["openclaw_calls"] += 1
                            return choices[0].get('message', {}).get('content')
                    else:
                        error_text = await response.text()
                        logger.error(f"OpenClaw HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            self._stats["openclaw_failures"] += 1
            logger.error(f"OpenClaw Connection Error: {e}")
        
        return None 

    async def route_query(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative'] = 'chat',
                          context: list = None,
                          is_private: bool = True,
                          use_rag: bool = True):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å Auto-Fallback –∏ RAG.
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: OpenClaw (Subscriptions) -> Local (LM Studio) -> Cloud (Gemini API)
        """
        
        # 0. RAG Lookup
        if use_rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.1. Tool Orchestration (Phase 6)
        if self.tools:
            tool_data = await self.tools.execute_tool_chain(prompt)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.5. Reasoning Mode (Thinker) sent directly to Gemini if specified, OR use OpenClaw if capable
        # –ù–æ –ø–æ–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏–º –ª–æ–≥–∏–∫—É: –µ—Å–ª–∏ reasoning —è–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω Gemini - —à–ª–µ–º —Ç—É–¥–∞, 
        # –Ω–æ –µ—Å–ª–∏ –µ—Å—Ç—å OpenClaw - –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏ –µ–≥–æ (–µ—Å–ª–∏ –æ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç reasoning –º–æ–¥–µ–ª–∏).
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã: Thinking -> Gemini Cloud (—Ç–∞–∫ –∫–∞–∫ —Ç–∞–º exp –º–æ–¥–µ–ª–∏), –æ—Å—Ç–∞–ª—å–Ω—ã–µ -> OpenClaw Priority.
        
        if task_type == 'reasoning':
            logger.info("üß† Thinking mode activated...")
            return await self._call_gemini(prompt, self.models["thinking"], context, is_private)

        await self.check_local_health()
        await self.check_openclaw_health()

        # 1. OpenClaw (Highest Priority - Subscription usage)
        if self.is_openclaw_available:
            target_model = self.premium_mapping.get(task_type, self.openclaw_model)
            logger.info(f"Routing to OPENCLAW ({target_model}) via Subscription Gateway")
            response = await self._call_openclaw(prompt, task_type, context, is_private)
            if response:
                return response
            logger.warning("OpenClaw failed. Falling back to local/cloud...")

        # 2. –ü—ã—Ç–∞–µ–º—Å—è –ª–æ–∫–∞–ª—å–Ω–æ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –∏ –∑–∞–¥–∞—á–∞ –ø—Ä–æ—Å—Ç–∞—è/—á–∞—Ç)
        if self.is_local_available and task_type in ['chat', 'coding']:
            logger.info("Routing to LOCAL", model=self.active_local_model)
            response = await self._call_local_llm(prompt, context, is_private)

            if response:  # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
                self._stats["local_calls"] += 1
                return response
            
            logger.warning("Local LLM failed. Falling back to CLOUD.")

        # 3. Fallback –∏–ª–∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ -> Gemini Cloud
        model_name = self.models.get(task_type, self.models["chat"])
        logger.info("Routing to CLOUD", model=model_name)

        return await self._call_gemini(prompt, model_name, context, is_private)

    async def _call_gemini(self, prompt: str, model_name: str, context: list = None,
                           is_private: bool = True, max_retries: int = 2) -> str:
        """
        –í—ã–∑–æ–≤ Google Gemini —á–µ—Ä–µ–∑ Generative AI SDK (google.genai).
        –í–∫–ª—é—á–∞–µ—Ç retry —Å exponential backoff –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö 429/500.
        """
        if not self.gemini_client:
            return "‚ùå –û—à–∏–±–∫–∞: Gemini SDK –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å `GEMINI_API_KEY` –≤ `.env`."

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏—á–Ω–æ—Å—Ç–∏ (Persona)
        from src.core.prompts import get_system_prompt
        base_instructions = get_system_prompt(is_private)

        persona_prompt = ""
        if self.persona:
            persona_prompt = self.persona.get_current_prompt()

        system_instructions = f"{persona_prompt}\n\n{base_instructions}"

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
        full_prompt = prompt
        if context:
            history_str = "\n".join(
                [f"{msg.get('role', 'user')}: {msg.get('text', '')}" for msg in context]
            )
            full_prompt = f"History:\n{history_str}\n\nCurrent Request: {prompt}"

        # Retry —Å exponential backoff (429 rate limit, 500 server error)
        for attempt in range(max_retries + 1):
            try:
                # –í–ê–ñ–ù–û: –ù–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å google.genai
                response = await self.gemini_client.models.generate_content(
                    model=model_name,
                    contents=full_prompt,
                    config=types.GenerateContentConfig(
                        system_instruction=system_instructions,
                        temperature=0.7
                    )
                )

                if not response or not response.text:
                    return "‚ùå AI –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (–∏–ª–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∫–æ–Ω—Ç–µ–Ω—Ç)."

                self._stats["cloud_calls"] += 1
                return response.text

            except Exception as e:
                error_str = str(e)
                # Retry –ø—Ä–∏ rate limit (429) –∏–ª–∏ server error (500)
                if attempt < max_retries and ("429" in error_str or "500" in error_str):
                    wait = 2 ** (attempt + 1)  # 2s, 4s
                    logger.warning(f"Gemini retry {attempt+1}/{max_retries}, wait {wait}s", error=error_str)
                    await asyncio.sleep(wait)
                    continue

                self._stats["cloud_failures"] += 1
                logger.error("Gemini API Error", error=error_str, attempt=attempt)
                return f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}"

    async def diagnose(self) -> dict:
        """
        –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: {subsystem: {ok: bool, status: str}}
        """
        result = {}

        # 1. OpenClaw
        oc_ok = await self.check_openclaw_health(force=True)
        result["OpenClaw"] = {
            "ok": oc_ok,
            "status": "Online" if oc_ok else "Offline/Not Configured"
        }

        # 2. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_ok = await self.check_local_health(force=True)
        result["Local AI"] = {
            "ok": local_ok,
            "status": f"{self.local_engine}: {self.active_local_model}" if local_ok else "Offline",
        }

        # 3. Gemini Cloud
        gemini_ok = self.gemini_client is not None
        result["Gemini Cloud"] = {
            "ok": gemini_ok,
            "status": f"Ready ({self.models['chat']})" if gemini_ok else "No API Key",
        }

        # 4. RAG Engine
        try:
            rag_count = self.rag.get_total_documents()
            result["RAG Engine"] = {"ok": True, "status": f"{rag_count} documents"}
        except Exception as e:
            result["RAG Engine"] = {"ok": False, "status": str(e)}

        # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–∑–æ–≤–æ–≤
        result["Call Stats"] = {
            "ok": True,
            "status": (
                f"OC: {self._stats['openclaw_calls']}/{self._stats['openclaw_failures']}, "
                f"Local: {self._stats['local_calls']}/{self._stats['local_failures']}, "
                f"Cloud: {self._stats['cloud_calls']}/{self._stats['cloud_failures']}"
            ),
        }

        # 6. RAM
        ram_info = self.get_ram_usage()
        if "error" not in ram_info:
            result["System RAM"] = {
                "ok": ram_info["percent"] < 90,
                "status": f"{ram_info['used_gb']}/{ram_info['total_gb']} GB ({ram_info['percent']}%)",
            }
        else:
            result["System RAM"] = {"ok": True, "status": "N/A"}

        return result

    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–∏—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model."""
        return {
            "cloud_models": self.models.copy(),
            "local_engine": self.local_engine,
            "local_model": self.active_local_model,
            "local_available": self.is_local_available,
            "openclaw_available": self.is_openclaw_available,
            "stats": self._stats.copy(),
        }

    def get_ram_usage(self) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM —á–µ—Ä–µ–∑ SystemMonitor.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π —Ç—è–∂—ë–ª—ã—Ö –º–æ–¥–µ–ª–µ–π (Flux, Whisper Large)
        —á—Ç–æ–±—ã –Ω–µ –∫—Ä–∞—à–Ω—É—Ç—å —Å–∏—Å—Ç–µ–º—É.
        """
        try:
            from src.utils.system_monitor import SystemMonitor
            snapshot = SystemMonitor.get_snapshot()
            return {
                "total_gb": round(snapshot.ram_total_gb, 1),
                "used_gb": round(snapshot.ram_used_gb, 1),
                "available_gb": round(snapshot.ram_available_gb, 1),
                "percent": snapshot.ram_percent,
                "can_load_heavy": SystemMonitor.can_load_heavy_model()
            }
        except Exception as e:
            logger.warning(f"RAM check failed: {e}")
            return {"error": str(e), "can_load_heavy": True}  # –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ–º