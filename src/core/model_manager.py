# -*- coding: utf-8 -*-
"""Source Code - Model Manager Logic

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E8fX0g4RgBoRUDDLUAA_mGf6aTttmDdf
"""

# -*- coding: utf-8 -*-
"""
Model Manager (Router) для Krab v2.0.
Отвечает за выбор оптимальной модели (Cloud vs Local) в зависимости от задачи и доступности ресурсов.
"""

import os
import aiohttp
import logging
from typing import Literal, Optional, Dict, Any

# Настройка логгера
logger = logging.getLogger(__name__)

class ModelRouter:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.lm_studio_url = config.get("LM_STUDIO_URL", "http://localhost:1234/v1")
        self.gemini_key = config.get("GEMINI_API_KEY")

        # Статусы доступности
        self.is_local_available = False
        self.active_local_model = None

    async def check_local_health(self):
        """Проверяет, запущен ли LM Studio и какая модель загружена."""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.lm_studio_url}/models") as response:
                    if response.status == 200:
                        data = await response.json()
                        # Предполагаем структуру ответа OpenAI-compatible
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.is_local_available = True
                            logger.info(f"Local AI Available: {self.active_local_model}")
                        else:
                            self.is_local_available = True # Сервер есть, модель не загружена
                    else:
                        self.is_local_available = False
        except Exception as e:
            logger.warning(f"Local AI Check Failed: {e}")
            self.is_local_available = False

    async def _call_local_llm(self, prompt: str, context: list = None) -> str:
        """
        Вызов локальной модели через прямой HTTP запрос (aiohttp).
        Более надежно чем OpenAI SDK для LM Studio/Ollama.
        """
        try:
            # Формируем payload вручную, как в nexus_bridge.py
            messages = []
            if context:
                messages.extend(context)
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self.active_local_model or "local-model",
                "messages": messages,
                "temperature": 0.7
            }

            headers = {"Content-Type": "application/json"}
            
            # Таймаут побольше для локалки
            timeout = aiohttp.ClientTimeout(total=60)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{self.lm_studio_url}/chat/completions", 
                    json=payload, 
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        # LOGGING RAW RESPONSE FOR DEBUGGING
                        logger.info(f"Local LLM Raw Response: {data}")
                        
                        # Защита от NoneType errors
                        choices = data.get('choices')
                        if choices and len(choices) > 0:
                            content = choices[0].get('message', {}).get('content')
                            if content:
                                return content
                        
                        logger.error(f"Local LLM Invalid Response: {data}")
                        return None # Return None to trigger fallback
                    else:
                        error_text = await response.text()
                        logger.error(f"Local LLM HTTP {response.status}: {error_text}")
                        return None # Return None to trigger fallback

        except Exception as e:
            import traceback
            logger.error(f"Local LLM Connection Error: {e}\n{traceback.format_exc()}")
            return None # Return None to trigger fallback

    async def route_query(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative'],
                          context: list = None):
        """
        Главный метод маршрутизации запроса с Auto-Fallback.
        """
        await self.check_local_health() 

        # 1. Пытаемся локально (если доступно и задача простая/чат)
        if self.is_local_available and task_type in ['chat', 'coding']:
            logger.info(f"Routing to LOCAL ({self.active_local_model})")
            response = await self._call_local_llm(prompt, context)
            
            if response: # Если успешно
                return response
            
            logger.warning("Local LLM failed (returned None). Falling back to CLOUD.")

        # 2. Fallback или сложные задачи -> Gemini Cloud
        logger.info(f"Routing to CLOUD (Gemini)")
        if task_type == 'reasoning':
            model_name = "gemini-2.0-pro-exp"
        else:
            model_name = "gemini-2.0-flash"

        return await self._call_gemini(prompt, model_name, context)

    async def _call_gemini(self, prompt: str, model_name: str, context: list = None) -> str:
        """Вызов Google Gemini через Generative AI SDK."""
        try:
            import google.generativeai as genai
            
            if not self.gemini_key:
                return "Ошибка: Не задан GEMINI_API_KEY"

            genai.configure(api_key=self.gemini_key)
            
            # System Prompt для персонализации
            from src.core.prompts import SYSTEM_PROMPT
            
            # Передаем system instruction (поддерживается в Gemini 1.5+)
            model = genai.GenerativeModel(model_name, system_instruction=SYSTEM_PROMPT)
            
            # Если есть контекст (история), можно сформировать chat session, но пока просто prompt
            full_prompt = prompt
            if context:
                history_str = "\n".join([f"{msg.get('role', 'user')}: {msg.get('text', '')}" for msg in context])
                full_prompt = f"History:\n{history_str}\n\nCurrent Request: {prompt}"

            response = await model.generate_content_async(full_prompt)
            return response.text
        except Exception as e:
            logger.error(f"Gemini Error: {e}")
            return f"Ошибка Gemini: {e}"

    def get_ram_usage(self):
        # TODO: Добавить проверку RAM, чтобы не крашнуть систему при загрузке Flux
        pass