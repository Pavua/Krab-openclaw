# -*- coding: utf-8 -*-
"""
Model Manager (Router) –¥–ª—è Krab v6.5.
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Cloud vs Local).

–°—Ç—Ä–∞—Ç–µ–≥–∏—è: Local First ‚Üí Cloud Fallback.
- –ü—Ä–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio/Ollama ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö (–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å + —Å–∫–æ—Ä–æ—Å—Ç—å)
- –ü—Ä–∏ –æ—à–∏–±–∫–µ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ Gemini Cloud
- RAG –∏ Tool Orchestration —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –ö–ê–ñ–î–´–ô –∑–∞–ø—Ä–æ—Å
"""

import os
import time
import asyncio
import aiohttp
from typing import Literal, Optional, Dict, Any, List
# from src.core.rag_engine import RAGEngine # Deprecated

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
import structlog
logger = structlog.get_logger("ModelRouter")

# Gemini SDK (New v1.0+)
try:
    from google import genai
    from google.genai import types
    _GENAI_AVAILABLE = True
except ImportError:
    _GENAI_AVAILABLE = False
    genai = None

class ModelRouter:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.lm_studio_url = config.get("LM_STUDIO_URL", "http://localhost:1234/v1")
        self.ollama_url = config.get("OLLAMA_URL", "http://localhost:11434/api")
        self.gemini_key = config.get("GEMINI_API_KEY")

        # –°—Ç–∞—Ç—É—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.is_local_available = False
        self.local_engine = None  # 'lm-studio' or 'ollama'
        self.active_local_model = None

        # –ö–µ—à –¥–ª—è health-check (—á—Ç–æ–±—ã –Ω–µ –¥—ë—Ä–≥–∞—Ç—å API –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å)
        self._health_cache_ts = 0
        self._health_cache_ttl = 30  # —Å–µ–∫—É–Ω–¥

        # Gemini SDK ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º –û–î–ò–ù –†–ê–ó
        self.gemini_client = None
        if _GENAI_AVAILABLE and self.gemini_key:
            try:
                self.gemini_client = genai.Client(api_key=self.gemini_key)
                logger.info("‚òÅÔ∏è Gemini SDK (google-genai) configured successfully")
            except Exception as e:
                logger.error(f"Failed to init Gemini Client: {e}")

        # RAG Engine (Deprecated, use OpenClaw)
        self.rag = None # RAGEngine()

        # Persona Manager (–Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py)
        self.persona = None
        self.tools = None  # –ù–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py (ToolHandler)

        # –ü—É–ª –º–æ–¥–µ–ª–µ–π ‚Äî —á–∏—Ç–∞–µ–º –∏–∑ .env, –¥–µ—Ñ–æ–ª—Ç—ã –∫–∞–∫ fallback
        self.models = {
            "chat": config.get("GEMINI_CHAT_MODEL", "gemini-2.5-flash"),
            "thinking": config.get("GEMINI_THINKING_MODEL", "gemini-2.0-flash"),
            "pro": config.get("GEMINI_PRO_MODEL", "gemini-2.5-pro"),
            "coding": config.get("GEMINI_CODING_MODEL", "gemini-2.5-flash"),
        }

        # –°—á—ë—Ç—á–∏–∫–∏ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
        self._stats = {
            "local_calls": 0,
            "cloud_calls": 0,
            "local_failures": 0,
            "cloud_failures": 0,
        }

        # Fallback –º–æ–¥–µ–ª–∏ (–¥–ª—è Gemini Quota Handling)
        self.fallback_models = [
            "gemini-2.0-flash-lite-preview-02-05", # Flash Lite (User requested)
            "gemini-2.0-flash",         # –ï—Å–ª–∏ 2.5 –∑–∞–Ω—è—Ç
            "gemini-2.0-flash-001",     # –°—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
            "gemini-flash-latest",      # –ê–ª–∏–∞—Å –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—É—é flash
            "gemini-pro-latest"         # –ê–ª–∏–∞—Å –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—É—é pro
        ]
        
        # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: 'auto', 'force_local', 'force_cloud'
        self.force_mode = "auto"

    def set_force_mode(self, mode: Literal['auto', 'local', 'cloud']) -> str:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Ä–æ—É—Ç–µ—Ä–∞."""
        if mode not in ['auto', 'local', 'cloud']:
            return "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º. –ò—Å–ø–æ–ª—å–∑—É–π: auto, local, cloud"
        
        old = self.force_mode
        if mode == 'local':
            self.force_mode = 'force_local'
        elif mode == 'cloud':
            self.force_mode = 'force_cloud'
        else:
            self.force_mode = 'auto'
            
        return f"–†–µ–∂–∏–º –∏–∑–º–µ–Ω–µ–Ω: {old} -> {self.force_mode}"

    async def check_local_health(self, force: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–ø—É—â–µ–Ω –ª–∏ LM Studio –∏–ª–∏ Ollama.
        """
        now = time.time()
        if not force and (now - self._health_cache_ts) < self._health_cache_ttl:
            return self.is_local_available

        self._health_cache_ts = now

        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º LM Studio (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û /v1 endpoint, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –≤ –ª–æ–≥–∏ LM Studio –æ—à–∏–±–∫–∞–º–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ—Ä–Ω—é
        candidates = []
        if self.lm_studio_url.endswith("/v1"):
            candidates.append(self.lm_studio_url)
        else:
            candidates.append(f"{self.lm_studio_url}/v1")
            
        # –£–±—Ä–∞–ª–∏ fallback –Ω–∞ root URL, —Ç–∞–∫ –∫–∞–∫ 99% OpenAI-compatible —Å–µ—Ä–≤–µ—Ä–æ–≤ –∂–∏–≤—É—Ç –Ω–∞ /v1

        for base_url in candidates:
            try:
                # [NEW] Auto-correct loaded model if needed (via lms CLI)
                if force:
                    await self._ensure_chat_model_loaded()

                # –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–æ 3 —Å–µ–∫ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                timeout = aiohttp.ClientTimeout(total=3)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(f"{base_url}/models") as response:
                        if response.status == 200:
                            data = await response.json()
                            models = data.get('data', [])
                            if models:
                                self.active_local_model = models[0]['id']
                                
                                # Check if it's an embedding model
                                if "embed" in self.active_local_model.lower():
                                     if force:
                                         logger.warning(f"‚ö†Ô∏è Text Embedding model detected ({self.active_local_model}). Attempting to switch to Chat model...")
                                         if await self._ensure_chat_model_loaded():
                                             continue # Retry probe
                                     else:
                                         logger.warning(f"‚ö†Ô∏è Warning: Active model '{self.active_local_model}' appears to be an embedding model!")

                                self.local_engine = 'lm-studio'
                                self.is_local_available = True
                                self.lm_studio_url = base_url
                                logger.info(f"Local AI Available (LM Studio): {self.active_local_model} at {base_url}")
                                return True
            except Exception:
                continue

    async def _ensure_chat_model_loaded(self) -> bool:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å Chat-–º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ 'lms' CLI, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.
        """
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if not os.path.exists(lms_path):
            return False

        try:
            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            proc = await asyncio.create_subprocess_exec(
                lms_path, "ps",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await proc.communicate()
            output = stdout.decode()
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å 'Text Embedding', –≤—ã–≥—Ä—É–∂–∞–µ–º
            if "embed" in output.lower():
                # –ü–∞—Ä—Å–∏–º ID (—É–ø—Ä–æ—â–µ–Ω–Ω–æ: –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –∏—â–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)
                model_id = output.split()[0] if output.split() else "all"
                if "LOADED" not in output: # No models loaded
                     pass
                else:
                    logger.info("üîÑ Unloading Embedding Model...")
                    await asyncio.create_subprocess_exec(lms_path, "unload", "--all")
                    await asyncio.sleep(2) # Wait for unload

            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞ ps, –µ—Å–ª–∏ –ø—É—Å—Ç–æ - –≥—Ä—É–∑–∏–º
            proc_ps = await asyncio.create_subprocess_exec(
                lms_path, "ps",
                stdout=asyncio.subprocess.PIPE
            )
            out_ps, _ = await proc_ps.communicate()
            if "LOADED" in out_ps.decode() and "embed" not in out_ps.decode().lower():
                return True # –£–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ Chat –º–æ–¥–µ–ª—å

            # 3. –ò—â–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ
            proc_ls = await asyncio.create_subprocess_exec(
                lms_path, "ls",
                stdout=asyncio.subprocess.PIPE
            )
            out_ls, _ = await proc_ls.communicate()
            available = out_ls.decode().splitlines()
            
            # –ò—â–µ–º —á—Ç–æ-—Ç–æ –ø–æ—Ö–æ–∂–µ–µ –Ω–∞ Chat/Instruct
            chat_candidate = None
            for line in available:
                lower = line.lower()
                if ("instruct" in lower or "chat" in lower or "llama" in lower or "qwen" in lower) and "embed" not in lower:
                     # lms ls output: "slug   SIZE   ARCH..."
                     # We need the slug (first column)
                     parts = line.split()
                     if parts:
                        chat_candidate = parts[0]
                        break
            
            if chat_candidate:
                logger.info(f"üöÄ Auto-Loading Local Model: {chat_candidate}")
                await asyncio.create_subprocess_exec(lms_path, "load", chat_candidate, "--gpu", "auto")
                await asyncio.sleep(5) # Wait for load
                return True
            else:
                logger.warning("‚ö†Ô∏è No Chat models found in 'lms ls'.")
                return False

        except Exception as e:
            logger.error(f"‚ùå Auto-load failed: {e}")
            return False

    async def list_local_models(self) -> List[str]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (lms ls)."""
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if not os.path.exists(lms_path):
            return ["–û—à–∏–±–∫–∞: lms CLI –Ω–µ –Ω–∞–π–¥–µ–Ω"]

        try:
            proc = await asyncio.create_subprocess_exec(
                lms_path, "ls",
                stdout=asyncio.subprocess.PIPE
            )
            stdout, _ = await proc.communicate()
            models = []
            for line in stdout.decode().splitlines():
                if not line.strip() or "SIZE" in line: continue
                parts = line.split()
                if parts:
                    models.append(parts[0])
            return models
        except Exception as e:
            return [f"–û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"]

    async def list_cloud_models(self) -> List[str]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ Cloud –º–æ–¥–µ–ª–∏ (Gemini)."""
        if not self.gemini_client:
            return ["–û—à–∏–±–∫–∞: Gemini –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"]
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º list_models –∏–∑ v1 SDK
            # client.models.list(config={'page_size': 100}) - check iterator
            models = []
            async for m in await asyncio.to_thread(self.gemini_client.models.list):
                if "generateContent" in m.supported_generation_methods:
                    models.append(m.name.split("/")[-1]) # models/gemini-1.5 -> gemini-1.5
            return sorted(models)
        except Exception as e:
            # Fallback for old SDK logic or errors
            logger.error(f"Cloud scan error: {e}")
            return [f"–û—à–∏–±–∫–∞ API: {e}"]

        # 2. –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º Ollama
        try:
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{self.ollama_url.replace('/api', '/v1')}/models") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.local_engine = 'ollama'
                            self.is_local_available = True
                            logger.info(f"Local AI Available (Ollama): {self.active_local_model}")
                            return True
        except Exception:
            pass

        self.is_local_available = False
        self.local_engine = None
        self.active_local_model = None
        return False

    async def _call_local_llm(self, prompt: str, context: list = None, chat_type: str = "private", is_owner: bool = False) -> str:
        """
        –í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å (aiohttp).
        """
        try:
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt –¥–ª—è –ª–æ–∫–∞–ª–∫–∏
            system_msg = "You are a helpful assistant."
            if self.persona:
                system_msg = self.persona.get_current_prompt(chat_type, is_owner)

            # –í—ã–±–∏—Ä–∞–µ–º URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–≤–∏–∂–∫–∞
            base_url = self.lm_studio_url if self.local_engine == 'lm-studio' else \
                       self.ollama_url.replace('/api', '/v1')

            # –§–æ—Ä–º–∏—Ä—É–µ–º payload
            messages = [{"role": "system", "content": system_msg}]
            if context:
                messages.extend(context)
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self.active_local_model or "local-model",
                "messages": messages,
                "temperature": 0.7
            }

            headers = {"Content-Type": "application/json"}
            
            # –¢–∞–π–º–∞—É—Ç –ø–æ–±–æ–ª—å—à–µ –¥–ª—è –ª–æ–∫–∞–ª–∫–∏
            timeout = aiohttp.ClientTimeout(total=60)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/chat/completions", 
                    json=payload, 
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        # debug log removed to reduce noise
                        
                        choices = data.get('choices')
                        if choices and len(choices) > 0:
                            content = choices[0].get('message', {}).get('content')
                            if content:
                                return content
                        
                        return None 
                    else:
                        error_text = await response.text()
                        logger.error(f"Local LLM HTTP {response.status}: {error_text}")
                        return None 

        except Exception as e:
            self._stats["local_failures"] += 1
            return None  

    async def route_query(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative'] = 'chat',
                          context: list = None,
                          chat_type: str = "private",
                          is_owner: bool = False,
                          use_rag: bool = True):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å Auto-Fallback –∏ RAG.
        """
        
        # 0. RAG Lookup
        # 0. RAG Lookup
        if use_rag and self.rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.1. Tool Orchestration (Phase 6)
        if self.tools:
            tool_data = await self.tools.execute_tool_chain(prompt)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.5. Reasoning Mode (Thinker)
        if task_type == 'reasoning':
            logger.info("üß† Thinking mode activated...")
            return await self._call_gemini(prompt, self.models["thinking"], context, chat_type, is_owner)

        # === FORCE CLOUD MODE ===
        if self.force_mode == 'force_cloud':
            model_name = self.models.get(task_type, self.models["chat"])
            return await self._call_gemini(prompt, model_name, context, chat_type, is_owner)

        # === LOCAL MODE (Auto or Forced) ===
        await self.check_local_health() 

        use_local = False
        if self.force_mode == 'force_local':
            if self.is_local_available:
                use_local = True
            else:
                return "‚ùå –†–µ–∂–∏–º 'Force Local' –≤–∫–ª—é—á–µ–Ω, –Ω–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (LM Studio/Ollama offline)."
        elif self.is_local_available and task_type in ['chat', 'coding']:
            use_local = True

        if use_local:
            logger.info("Routing to LOCAL", model=self.active_local_model)
            response = await self._call_local_llm(prompt, context, chat_type, is_owner)

            if response:
                self._stats["local_calls"] += 1
                return response
            
            if self.force_mode == 'force_local':
                return "‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Force Local active)."
                
            logger.warning("Local LLM failed. Falling back to CLOUD.")

        # === CLOUD FALLBACK ===
        model_name = self.models.get(task_type, self.models["chat"])
        return await self._call_gemini(prompt, model_name, context, chat_type, is_owner)

    async def _call_gemini(self, prompt: str, model_name: str, context: list = None,
                           chat_type: str = "private", is_owner: bool = False, max_retries: int = 2) -> str:
        """
        –í—ã–∑–æ–≤ Google Gemini —á–µ—Ä–µ–∑ google-genai SDK (v1.0+).
        """
        if not self.gemini_client:
            return "‚ùå –û—à–∏–±–∫–∞: Gemini SDK –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å `GEMINI_API_KEY` –≤ `.env`."

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt
        from src.core.prompts import get_system_prompt
        # –ù–∞–º –Ω–µ –Ω—É–∂–µ–Ω —Å—Ç–∞—Ä—ã–π get_system_prompt(is_private) –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å PersonaManager
        # –ù–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –±–∞–∑—É –∏–ª–∏ –∑–∞–º–µ–Ω–∏–º
        base_instructions = get_system_prompt(chat_type == "private")

        persona_prompt = ""
        if self.persona:
            persona_prompt = self.persona.get_current_prompt(chat_type, is_owner)

        system_instructions = f"{persona_prompt}\n\n{base_instructions}"

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        # –í –Ω–æ–≤–æ–º SDK –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª—É—á—à–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —á–µ—Ä–µ–∑ contents, –Ω–æ –ø–æ–∫–∞ —É–ø—Ä–æ—Å—Ç–∏–º:
        full_content = prompt
        if context:
            history_str = "\n".join(
                [f"{msg.get('role', 'user')}: {msg.get('text', '')}" for msg in context]
            )
            full_content = f"History:\n{history_str}\n\nCurrent Request: {prompt}"

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        # –í –Ω–æ–≤–æ–º SDK system_instruction –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ config
        config = types.GenerateContentConfig(
            system_instruction=system_instructions,
            temperature=0.7
        )

        for attempt in range(max_retries + 1):
            try:
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —á–µ—Ä–µ–∑ to_thread (SDK v1.0 –∫–∞–∂–µ—Ç—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, –∏–ª–∏ –∏–º–µ–µ—Ç async –º–µ—Ç–æ–¥—ã?)
                # –ö–ª–∏–µ–Ω—Ç SDK v1.0 –∏–º–µ–µ—Ç .aio.Client –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏, –Ω–æ –º—ã —Å–µ–π—á–∞—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π Client.
                # –ü–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.to_thread –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–≥–æ –≤—ã–∑–æ–≤–∞.
                
                response = await asyncio.to_thread(
                    self.gemini_client.models.generate_content,
                    model=model_name,
                    contents=full_content,
                    config=config
                )

                if not response or not response.text:
                    return "‚ùå AI –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."

                self._stats["cloud_calls"] += 1
                return response.text

            except Exception as e:
                error_str = str(e)
                
                # Quota Check (429)
                if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                     self._stats["cloud_failures"] += 1
                     logger.error("Gemini Quota Exhausted", error=error_str, model=model_name)
                     
                     if model_name in self.fallback_models:
                         fallback_idx = self.fallback_models.index(model_name)
                         if fallback_idx + 1 < len(self.fallback_models):
                             next_model = self.fallback_models[fallback_idx + 1]
                             logger.warning(f"Falling back to {next_model} due to quota limit")
                             return await self._call_gemini(prompt, next_model, context, chat_type, is_owner, max_retries=1)

                     if model_name not in self.fallback_models and self.fallback_models:
                         next_model = self.fallback_models[0]
                         return await self._call_gemini(prompt, next_model, context, chat_type, is_owner, max_retries=1)

                     return f"‚ùå –ö–≤–æ—Ç–∞ Gemini –∏—Å—á–µ—Ä–ø–∞–Ω–∞."

                logger.warning(f"Gemini Attempt {attempt+1} failed: {e}")
                if attempt < max_retries:
                    await asyncio.sleep(2 ** (attempt + 1))
                    continue
                
                self._stats["cloud_failures"] += 1
                return f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}"

    async def route_query_stream(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative'] = 'chat',
                          context: list = None,
                          chat_type: str = "private",
                          is_owner: bool = False,
                          use_rag: bool = True):
        """
        –í–µ—Ä—Å–∏—è route_query —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (–ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è Cloud).
        """
        # 1. –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∞–µ–º –≤—Å—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É (RAG, Tools) - —Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –≤ route_query
        if use_rag and self.rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        if self.tools:
            tool_data = await self.tools.execute_tool_chain(prompt)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 2. –°—Ç—Ä–∏–º–∏–Ω–≥ –ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è Gemini
        model_name = self.models.get(task_type, self.models["chat"])
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω Force Local, —Å—Ç—Ä–∏–º–∏–Ω–≥ –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ, 
        # –ø–æ—ç—Ç–æ–º—É –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤ Force Local –æ—Ç–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –æ–±—ã—á–Ω—ã–π route_query
        if self.force_mode == 'force_local' or (self.is_local_available and task_type in ['chat', 'coding']):
             full_res = await self.route_query(prompt, task_type, context, chat_type, is_owner, use_rag=False) # rag already applied
             yield full_res
             return

        async for chunk in self._call_gemini_stream(prompt, model_name, context, chat_type, is_owner):
            yield chunk

    async def _call_gemini_stream(self, prompt: str, model_name: str, context: list = None,
                                  chat_type: str = "private", is_owner: bool = False):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ Gemini.
        """
        if not self.gemini_client:
            yield "‚ùå –û—à–∏–±–∫–∞: Gemini SDK –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω."
            return

        from src.core.prompts import get_system_prompt
        system_instructions = f"{self.persona.get_current_prompt(chat_type, is_owner) if self.persona else ''}\n\n{get_system_prompt(chat_type == 'private')}"

        full_content = prompt
        if context:
            history_str = "\n".join([f"{msg.get('role', 'user')}: {msg.get('text', '')}" for msg in context])
            full_content = f"History:\n{history_str}\n\nCurrent Request: {prompt}"

        config = types.GenerateContentConfig(system_instruction=system_instructions, temperature=0.7)

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–∑ SDK
            # –í –Ω–æ–≤–æ–º SDK aio.Client.models.generate_content_stream –≤–µ—Ä–Ω–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏—Ç–µ—Ä–∞—Ç–æ—Ä
            # –ù–æ –º—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π Client. 
            # –î–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å aio –∫–ª–∏–µ–Ω—Ç.
            
            # –ü–µ—Ä–µ–∫–ª—é—á–∏–º—Å—è –Ω–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ, –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π —Ü–∏–∫–ª
            # –¢–∞–∫ –∫–∞–∫ –º—ã –≤ Ralph Mode, —è –ø–æ–¥–ø—Ä–∞–≤–ª—é __init__ –ø–æ–∑–∂–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ, 
            # –Ω–æ –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å—Ç—Ä–∏–º —á–µ—Ä–µ–∑ to_thread (–Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)
            # –õ–£–ß–®–ï: –°–æ–∑–¥–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π aio –∫–ª–∏–µ–Ω—Ç
            
            async_client = genai.Client(api_key=self.gemini_key, http_options={'api_version': 'v1alpha'}) # or v1
            
            response_stream = await async_client.aio.models.generate_content_stream(
                model=model_name,
                contents=full_content,
                config=config
            )
            
            full_text = ""
            async for chunk in response_stream:
                if chunk.text:
                    full_text += chunk.text
                    yield full_text
            
            self._stats["cloud_calls"] += 1
        except Exception as e:
            logger.error(f"Stream error: {e}")
            yield f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞: {e}"

    async def diagnose(self) -> dict:
        """
        –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º.
        """
        result = {}

        # 1. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_ok = await self.check_local_health(force=True)
        result["Local AI"] = {
            "ok": local_ok,
            "status": f"{self.local_engine}: {self.active_local_model}" if local_ok else "Offline",
        }

        # 2. Gemini Cloud
        gemini_ok = self.gemini_client is not None
        result["Gemini Cloud"] = {
            "ok": gemini_ok,
            "status": f"Ready ({self.models['chat']})" if gemini_ok else "No API Key",
        }

        # 3. RAG Engine
        if self.rag:
            try:
                rag_count = self.rag.get_total_documents()
                result["RAG Engine"] = {"ok": True, "status": f"{rag_count} documents"}
            except Exception as e:
                result["RAG Engine"] = {"ok": False, "status": str(e)}
        else:
             result["RAG Engine"] = {"ok": True, "status": "Disabled (OpenClaw)"}

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–∑–æ–≤–æ–≤
        result["Call Stats"] = {
            "ok": True,
            "status": (
                f"Local: {self._stats['local_calls']} ok / {self._stats['local_failures']} fail, "
                f"Cloud: {self._stats['cloud_calls']} ok / {self._stats['cloud_failures']} fail"
            ),
        }

        # 5. RAM
        ram_info = self.get_ram_usage()
        if "error" not in ram_info:
            result["System RAM"] = {
                "ok": ram_info["percent"] < 90,
                "status": f"{ram_info['used_gb']}/{ram_info['total_gb']} GB ({ram_info['percent']}%)",
            }
        else:
            result["System RAM"] = {"ok": True, "status": "N/A"}

        return result

    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–∏—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model."""
        return {
            "cloud_models": self.models.copy(),
            "local_engine": self.local_engine,
            "local_model": self.active_local_model,
            "local_available": self.is_local_available,
            "stats": self._stats.copy(),
            "force_mode": self.force_mode,
            "fallback_models": self.fallback_models
        }

    def get_ram_usage(self) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM —á–µ—Ä–µ–∑ SystemMonitor.
        """
        try:
            from src.utils.system_monitor import SystemMonitor
            snapshot = SystemMonitor.get_snapshot()
            return {
                "total_gb": round(snapshot.ram_total_gb, 1),
                "used_gb": round(snapshot.ram_used_gb, 1),
                "available_gb": round(snapshot.ram_available_gb, 1),
                "percent": snapshot.ram_percent,
                "can_load_heavy": SystemMonitor.can_load_heavy_model()
            }
        except Exception as e:
            return {"error": str(e), "can_load_heavy": True}