# -*- coding: utf-8 -*-
"""
Model Manager (Router) –¥–ª—è Krab v6.5.
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Cloud vs Local).

–°—Ç—Ä–∞—Ç–µ–≥–∏—è: Local First ‚Üí Cloud Fallback.
- –ü—Ä–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio/Ollama ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö (–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å + —Å–∫–æ—Ä–æ—Å—Ç—å)
- –ü—Ä–∏ –æ—à–∏–±–∫–µ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ Gemini Cloud
- RAG –∏ Tool Orchestration —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –ö–ê–ñ–î–´–ô –∑–∞–ø—Ä–æ—Å
"""

import os
import time
import asyncio
import json
import difflib
import aiohttp
from pathlib import Path
import re
from datetime import datetime, timezone
from contextlib import asynccontextmanager
from typing import Literal, Optional, Dict, Any, List, Set, AsyncGenerator
# from src.core.rag_engine import RAGEngine # Deprecated

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
import structlog
logger = structlog.get_logger("ModelRouter")

from src.core.openclaw_client import OpenClawClient
from src.core.agent_swarm import SwarmManager
from src.core.stream_client import OpenClawStreamClient, StreamFailure
from src.core.cost_engine import CostEngine

class ModelRouter:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.lm_studio_url = config.get("LM_STUDIO_URL", "http://localhost:1234/v1").rstrip("/")
        if "/v1" not in self.lm_studio_url:
            self.lm_studio_url += "/v1"

        self.ollama_url = config.get("OLLAMA_URL", "http://localhost:11434/api")
        self.gemini_key = config.get("GEMINI_API_KEY")

        # –°—Ç–∞—Ç—É—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.is_local_available = False
        self.local_engine = None  # 'lm-studio' or 'ollama'
        self.active_local_model = None

        # –ö–µ—à –¥–ª—è health-check (—á—Ç–æ–±—ã –Ω–µ –¥—ë—Ä–≥–∞—Ç—å API –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å).
        # –í–∞–∂–Ω–æ: —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–π –æ–ø—Ä–æ—Å `/api/v1/models` –º–æ–∂–µ—Ç —Å–±–∏–≤–∞—Ç—å idle-TTL LM Studio.
        self._health_cache_ts = 0
        try:
            self._health_cache_ttl = max(5, int(config.get("LOCAL_HEALTH_CACHE_TTL_SEC", 30)))
        except (ValueError, TypeError):
            self._health_cache_ttl = 30

        # –†–µ–∂–∏–º health-check:
        # - "light": —Ñ–æ–Ω–æ–≤–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞ (–±–µ–∑ —Å–∫–∞–Ω–∞ –º–æ–¥–µ–ª–µ–π),
        #            –∞ –ø–æ–ª–Ω—ã–π —Å–∫–∞–Ω –º–æ–¥–µ–ª–µ–π –¥–µ–ª–∞–µ–º —Ä–µ–¥–∫–æ.
        # - "models": –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ /api/v1/models (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ).
        self._health_probe_mode = str(config.get("LOCAL_HEALTH_PROBE_MODE", "light")).strip().lower()
        if self._health_probe_mode not in {"light", "models"}:
            self._health_probe_mode = "light"
        self._health_full_scan_ts = 0
        try:
            self._health_full_scan_interval = max(
                60,
                int(config.get("LOCAL_HEALTH_FULL_SCAN_SECONDS", 3600)),
            )
        except (ValueError, TypeError):
            self._health_full_scan_interval = 3600

        # OpenClaw Client (Cloud Model Gateway)
        self.openclaw_client = OpenClawClient(
            base_url=config.get("OPENCLAW_BASE_URL", "http://localhost:18789"),
            api_key=config.get("OPENCLAW_API_KEY")
        )
        # Stream Client –¥–ª—è WebSocket/SSE
        self.stream_client = OpenClawStreamClient(
            base_url=self.lm_studio_url,
            api_key="none"
        )
        logger.info("‚òÅÔ∏è OpenClaw & Stream Clients configured")

        # RAG Engine (Deprecated, use OpenClaw)
        self.rag = None # RAGEngine()

        # Persona Manager (–Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py)
        self.persona = None
        self.tools = None  # –ù–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py (ToolHandler)

        # Agent Swarm Manager
        self.swarm = SwarmManager(model_router=self)

        # –ü—É–ª –º–æ–¥–µ–ª–µ–π ‚Äî —á–∏—Ç–∞–µ–º –∏–∑ .env, –¥–µ—Ñ–æ–ª—Ç—ã –∫–∞–∫ fallback
        self.models = {
            "chat": config.get("GEMINI_CHAT_MODEL", "gemini-2.0-flash"),
            "thinking": config.get("GEMINI_THINKING_MODEL", "gemini-2.0-flash-thinking-exp-01-21"),
            "pro": config.get("GEMINI_PRO_MODEL", "gemini-3-pro-preview"),
            "coding": config.get("GEMINI_CODING_MODEL", "gemini-2.0-flash"),
        }
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ cloud-–º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –ø—É—Å—Ç–∞—è ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ self.models[*].
        self.chat_model_group = str(config.get("GEMINI_CHAT_MODEL_GROUP", "")).strip()
        self.chat_model_owner_private = str(config.get("GEMINI_CHAT_MODEL_OWNER_PRIVATE", "")).strip()
        self.chat_model_owner_private_important = str(
            config.get("GEMINI_CHAT_MODEL_OWNER_PRIVATE_IMPORTANT", "")
        ).strip()
        self.owner_private_always_pro = str(
            config.get("MODEL_OWNER_PRIVATE_ALWAYS_PRO", "0")
        ).strip().lower() in {"1", "true", "yes", "on"}

        # [R11] Cost Engine –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –±—é–¥–∂–µ—Ç–∞
        self.cost_engine = CostEngine(config)

        # –°—á—ë—Ç—á–∏–∫–∏ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
        self._stats = {
            "local_calls": 0,
            "cloud_calls": 0,
            "local_failures": 0,
            "cloud_failures": 0,
        }

        # Fallback –º–æ–¥–µ–ª–∏ (–¥–ª—è Gemini Quota Handling)
        self.fallback_models = [
            "gemini-2.0-flash-lite-preview-02-05", # Flash Lite (User requested)
            "gemini-2.0-flash",         # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–Ω—è—Ç
            "gemini-2.0-flash-001",     # –°—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
            "gemini-1.5-flash",         # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π fallback
            "gemini-1.5-pro"            # –°—Ç–∞–±–∏–ª—å–Ω–∞—è pro
        ]
        
        # –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–∏–∑ .env) ‚Äî –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞,
        # _ensure_chat_model_loaded() –±—É–¥–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–º–µ–Ω–Ω–æ –µ—ë,
        # –∞ –Ω–µ –ø–µ—Ä–≤—É—é –ø–æ–ø–∞–≤—à—É—é—Å—è LLM (—á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ –¥–µ—Ñ–æ–ª—Ç—É –Ω–∞ qwen 7b).
        self.local_preferred_model = config.get("LOCAL_PREFERRED_MODEL", "").strip()
        # –ú–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∏–Ω–≥–∞ (–µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç chat-–º–æ–¥–µ–ª–∏)
        self.local_coding_model = config.get("LOCAL_CODING_MODEL", "").strip()
        # –ó–∞—â–∏—Ç–∞ –æ—Ç —à—Ç–æ—Ä–º–∞ –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:
        # –µ—Å–ª–∏ LM Studio –∂–∏–≤–∞, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º auto-load
        # –Ω–µ —á–∞—â–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞.
        try:
            self.local_autoload_cooldown_sec = max(
                5,
                int(config.get("LOCAL_AUTOLOAD_COOLDOWN_SEC", 30)),
            )
        except Exception:
            self.local_autoload_cooldown_sec = 30
        self._last_local_autoload_ts = 0.0

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # [PHASE 15.1] Context Window Manager Metadata
        # –õ–∏–º–∏—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–≤—Ö–æ–¥–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        self.CONTEXT_WINDOWS = {
            "gemini-2.0-flash": 1048576,
            "gemini-2.0-pro-exp": 2097152,
            "gemini-1.5-pro": 2097152,
            "gemini-1.5-flash": 1048576,
            "gpt-4": 128000,
            "qwen": 32768,
            "llama-3": 8192,
            "mistral": 32768,
            "deepseek": 64000,
            "default": 8192
        }

        # Smart Memory Planner: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ RAM –∏ –∞–≤—Ç–æ-–∑–∞–≥—Ä—É–∑–∫–∞/–≤—ã–≥—Ä—É–∑–∫–∞
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        try:
            self.max_ram_gb = float(config.get("MAX_RAM_GB", 36))
        except (ValueError, TypeError):
            self.max_ram_gb = 36.0
        try:
            self.lm_studio_max_ram_gb = float(config.get("LM_STUDIO_MAX_RAM_GB", self.max_ram_gb * 0.5))
        except (ValueError, TypeError):
            self.lm_studio_max_ram_gb = self.max_ram_gb * 0.5
        try:
            self.auto_unload_idle_min = int(config.get("AUTO_UNLOAD_IDLE_MIN", 30))
        except (ValueError, TypeError):
            self.auto_unload_idle_min = 30

        # LRU-—Ç—Ä–µ–∫–µ—Ä: {model_id: timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è}
        self._model_last_used: Dict[str, float] = {}

        # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: 'auto', 'force_local', 'force_cloud'
        self.force_mode = "auto"

        # –ü–æ–ª–∏—Ç–∏–∫–∞ —Ä–æ—É—Ç–∏–Ω–≥–∞ (Phase D): free-first hybrid.
        self.routing_policy = str(config.get("MODEL_ROUTING_POLICY", "free_first_hybrid")).strip().lower()
        self.require_confirm_expensive = str(config.get("MODEL_REQUIRE_CONFIRM_EXPENSIVE", "0")).strip().lower() in {
            "1", "true", "yes", "on"
        }
        self.enable_cloud_review_for_critical = str(
            config.get("MODEL_ENABLE_CLOUD_REVIEW_CRITICAL", "0")
        ).strip().lower() in {"1", "true", "yes", "on"}

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–¥–æ–≤ –≤ –≤—ã–∑–æ–≤–∞—Ö (–±—é–¥–∂–µ—Ç–Ω—ã–π guardrail –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É).
        try:
            self.cloud_soft_cap_calls = int(config.get("CLOUD_SOFT_CAP_CALLS", 10000))
        except Exception:
            self.cloud_soft_cap_calls = 10000
        self.cloud_soft_cap_reached = False
        try:
            self.cloud_cost_per_call_usd = float(config.get("CLOUD_COST_PER_CALL_USD", 0.01))
        except Exception:
            self.cloud_cost_per_call_usd = 0.01
        try:
            self.local_cost_per_call_usd = float(config.get("LOCAL_COST_PER_CALL_USD", 0.0))
        except Exception:
            self.local_cost_per_call_usd = 0.0
        try:
            self.cloud_monthly_budget_usd = float(config.get("CLOUD_MONTHLY_BUDGET_USD", 25.0))
        except Exception:
            self.cloud_monthly_budget_usd = 25.0
        # –¢–æ—á–µ—á–Ω—ã–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º –æ–±–ª–∞—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–¥–ª—è runway-–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è).
        # –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤ .env –¥–ª—è —Ç–≤–æ–µ–≥–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ç–∞—Ä–∏—Ñ–∞.
        try:
            self.model_cost_flash_lite_usd = float(config.get("MODEL_COST_FLASH_LITE_USD", self.cloud_cost_per_call_usd * 0.7))
        except Exception:
            self.model_cost_flash_lite_usd = float(self.cloud_cost_per_call_usd * 0.7)
        try:
            self.model_cost_flash_usd = float(config.get("MODEL_COST_FLASH_USD", self.cloud_cost_per_call_usd))
        except Exception:
            self.model_cost_flash_usd = float(self.cloud_cost_per_call_usd)
        try:
            self.model_cost_pro_usd = float(config.get("MODEL_COST_PRO_USD", self.cloud_cost_per_call_usd * 3.0))
        except Exception:
            self.model_cost_pro_usd = float(self.cloud_cost_per_call_usd * 3.0)
        try:
            self.monthly_calls_forecast = int(config.get("MONTHLY_CALLS_FORECAST", 5000))
        except Exception:
            self.monthly_calls_forecast = 5000

        # –ü–æ–ª–∏—Ç–∏–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞: 1 heavy + 1 light.
        self._local_heavy_slot = asyncio.Semaphore(1)
        self._local_light_slot = asyncio.Semaphore(1)

        self.local_timeout_seconds = float(config.get("LOCAL_CHAT_TIMEOUT_SECONDS", 900))
        self.local_include_reasoning = str(config.get("LOCAL_INCLUDE_REASONING", "1")).strip().lower() in {
            "1", "true", "yes", "on"
        }
        try:
            self.local_reasoning_max_chars = int(config.get("LOCAL_REASONING_MAX_CHARS", 2000))
            if self.local_reasoning_max_chars < 200:
                self.local_reasoning_max_chars = 200
        except Exception:
            self.local_reasoning_max_chars = 2000
        try:
            self.local_stream_total_timeout_seconds = float(
                config.get("LOCAL_STREAM_TOTAL_TIMEOUT_SECONDS", 75.0)
            )
            if self.local_stream_total_timeout_seconds <= 0:
                self.local_stream_total_timeout_seconds = 75.0
        except Exception:
            self.local_stream_total_timeout_seconds = 75.0
        try:
            self.local_stream_sock_read_timeout_seconds = float(
                config.get("LOCAL_STREAM_SOCK_READ_TIMEOUT_SECONDS", 20.0)
            )
            if self.local_stream_sock_read_timeout_seconds <= 0:
                self.local_stream_sock_read_timeout_seconds = 20.0
        except Exception:
            self.local_stream_sock_read_timeout_seconds = 20.0
        self.local_stream_fallback_to_cloud = str(
            config.get("LOCAL_STREAM_FALLBACK_TO_CLOUD", "1")
        ).strip().lower() in {"1", "true", "yes", "on"}
        self.last_cloud_error: Optional[str] = None
        self.last_cloud_model: Optional[str] = None
        self.last_local_load_error: Optional[str] = None
        self.last_local_load_error_human: Optional[str] = None
        self.lms_gpu_offload = str(config.get("LM_STUDIO_GPU_OFFLOAD", "")).strip().lower()
        self.cloud_priority_models = self._parse_cloud_priority(config.get(
            "MODEL_CLOUD_PRIORITY_LIST",
            "gemini-2.5-flash,gemini-2.5-pro,google/gemini-2.5-flash,google/gemini-2.5-pro,openai/gpt-4o-mini"
        ))
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã cloud-—Ä–æ—Ç–∞—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ –¥–µ—Å—è—Ç–∫–∞—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.
        # –û—Å–æ–±–µ–Ω–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –ø—Ä–∏ force_cloud –∏ —Å–µ—Ç–µ–≤—ã—Ö –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è—Ö.
        try:
            self.cloud_max_candidates_per_request = max(
                1,
                int(config.get("MODEL_CLOUD_MAX_CANDIDATES_PER_REQUEST", 4)),
            )
        except Exception:
            self.cloud_max_candidates_per_request = 4
        try:
            self.cloud_max_candidates_force_cloud = max(
                1,
                int(config.get("MODEL_CLOUD_MAX_CANDIDATES_FORCE_CLOUD", 3)),
            )
        except Exception:
            self.cloud_max_candidates_force_cloud = 3
        # Fail-fast guardrails –¥–ª—è cloud-–≤—ã–∑–æ–≤–æ–≤:
        # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ HTTP-–∑–∞–ø—Ä–æ—Å–∞ –∏ –æ–±—â–∏–π –±—é–¥–∂–µ—Ç –≤—Ä–µ–º–µ–Ω–∏
        # –¥–ª—è force_cloud-–≤–µ—Ç–∫–∏, —á—Ç–æ–±—ã user –Ω–µ –≤–∏–¥–µ–ª –≤–µ—á–Ω–æ–µ ¬´ü§î –î—É–º–∞—é...¬ª.
        try:
            self.cloud_request_timeout_seconds = max(
                5,
                int(config.get("CLOUD_REQUEST_TIMEOUT_SECONDS", 22)),
            )
        except Exception:
            self.cloud_request_timeout_seconds = 22
        try:
            self.cloud_fail_fast_budget_seconds = max(
                1,
                int(config.get("CLOUD_FAIL_FAST_BUDGET_SECONDS", 40)),
            )
        except Exception:
            self.cloud_fail_fast_budget_seconds = 40
        # Provider probe –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, –Ω–æ —Ç–æ—Ä–º–æ–∑–∏—Ç –ø—Ä–æ–¥-–æ—Ç–≤–µ—Ç—ã.
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω –≤ –æ–±—ã—á–Ω–æ–º chat-–∫–æ–Ω—Ç—É—Ä–µ.
        self.cloud_probe_on_chat_error = str(
            config.get("CLOUD_PROVIDER_PROBE_ON_CHAT_ERROR", "0")
        ).strip().lower() in {"1", "true", "yes", "on"}

        # R15: Cloud Preflight Cache (provider -> (expiration_ts, error_msg))
        self._preflight_cache: dict[str, tuple[float, str]] = {}
        self._preflight_ttl_seconds = 300  # 5 –º–∏–Ω—É—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ —Ñ–∞—Ç–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–µ

        # –ü–∞–º—è—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –ø–æ –ø—Ä–æ—Ñ–∏–ª—è–º –∑–∞–¥–∞—á.
        self._routing_memory_path = Path(
            config.get("MODEL_ROUTING_MEMORY_PATH", "artifacts/model_routing_memory.json")
        )
        self._usage_report_path = Path(
            config.get("MODEL_USAGE_REPORT_PATH", "artifacts/model_usage_report.json")
        )
        self._routing_memory = self._load_json(self._routing_memory_path, default={})
        self._usage_report = self._load_json(
            self._usage_report_path,
            default={"profiles": {}, "models": {}, "channels": {"local": 0, "cloud": 0}},
        )
        self._ops_state_path = Path(
            config.get("MODEL_OPS_STATE_PATH", "artifacts/model_ops_state.json")
        )
        self._ops_state = self._load_json(
            self._ops_state_path,
            default={"acknowledged": {}, "history": []},
        )
        if not isinstance(self._ops_state.get("acknowledged"), dict):
            self._ops_state["acknowledged"] = {}
        if not isinstance(self._ops_state.get("history"), list):
            self._ops_state["history"] = []

        # –ö–æ–Ω—Ç—É—Ä –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (1-5) –¥–ª—è —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è —Ä–æ—É—Ç–∏–Ω–≥–∞.
        self._feedback_path = Path(
            config.get("MODEL_FEEDBACK_PATH", "artifacts/model_feedback.json")
        )
        self._feedback_store = self._load_json(
            self._feedback_path,
            default={"profiles": {}, "events": [], "last_route": {}, "updated_at": None},
        )
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —É—Å–ø–µ—à–Ω—ã–π stream-–º–∞—Ä—à—Ä—É—Ç (–æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç route_query/route_tool).
        self._last_stream_route: Dict[str, Any] = {}
        if not isinstance(self._feedback_store.get("profiles"), dict):
            self._feedback_store["profiles"] = {}
        if not isinstance(self._feedback_store.get("events"), list):
            self._feedback_store["events"] = []
        if not isinstance(self._feedback_store.get("last_route"), dict):
            self._feedback_store["last_route"] = {}

        existing_cloud_calls = int(self._usage_report.get("channels", {}).get("cloud", 0))
        if existing_cloud_calls >= self.cloud_soft_cap_calls:
            self.cloud_soft_cap_reached = True
            logger.warning(f"Cloud Soft Cap reached at startup ({existing_cloud_calls}/{self.cloud_soft_cap_calls})")
        else:
            self.cloud_soft_cap_reached = False
            logger.info(f"Cloud Soft Cap status: {existing_cloud_calls}/{self.cloud_soft_cap_calls} ok")

    def set_force_mode(self, mode: Literal['auto', 'local', 'cloud']) -> str:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Ä–æ—É—Ç–µ—Ä–∞."""
        if mode not in ['auto', 'local', 'cloud']:
            return "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º. –ò—Å–ø–æ–ª—å–∑—É–π: auto, local, cloud"
        
        old = self.force_mode
        if mode == 'local':
            self.force_mode = 'force_local'
        elif mode == 'cloud':
            self.force_mode = 'force_cloud'
        else:
            self.force_mode = 'auto'
            
        return f"–†–µ–∂–∏–º –∏–∑–º–µ–Ω–µ–Ω: {old} -> {self.force_mode}"

    def _load_json(self, path: Path, default: dict) -> dict:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JSON-—Ñ–∞–π–ª–∞."""
        try:
            if not path.exists():
                return default
            with path.open("r", encoding="utf-8") as fp:
                data = json.load(fp)
                return data if isinstance(data, dict) else default
        except Exception:
            return default

    @staticmethod
    def _normalize_chat_role(raw_role: str | None) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ —Ä–æ–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã–π –Ω–∞–±–æ—Ä OpenAI/LM Studio:
        user | assistant | system | tool.
        """
        role = str(raw_role or "user").strip().lower()
        if role in {"user", "assistant", "system", "tool"}:
            return role
        if role in {"model", "ai", "bot", "assistant_reply", "vision_analysis"}:
            return "assistant"
        if role in {"context", "memory", "note", "analysis"}:
            return "system"
        return "user"

    def _save_json(self, path: Path, payload: dict) -> None:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–ø–∏—Å—å JSON-—Ñ–∞–π–ª–∞."""
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            with path.open("w", encoding="utf-8") as fp:
                json.dump(payload, fp, ensure_ascii=False, indent=2)
        except Exception as exc:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON –º–µ—Ç—Ä–∏–∫–∏ —Ä–æ—É—Ç–µ—Ä–∞", path=str(path), error=str(exc))

    def _parse_cloud_priority(self, raw: Optional[str]) -> List[str]:
        """
        –†–∞–∑–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ —Å—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ —É–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏.
        """
        if not raw:
            return []
        result: list[str] = []
        seen: Set[str] = set()
        for token in str(raw).split(","):
            token = token.strip()
            if not token or token in seen:
                continue
            seen.add(token)
            result.append(token)
        return result

    def _normalize_cloud_model_name(self, model_name: Optional[str]) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç cloud model id:
        - —Å–Ω–∏–º–∞–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å `models/`,
        - —É–±–∏—Ä–∞–µ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ `-exp` (–∫—Ä–æ–º–µ thinking),
        - –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–π chat-–º–∞—Ä—à—Ä—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        """
        if not model_name:
            return ""
        normalized = str(model_name).strip()
        if not normalized:
            return ""

        if normalized.startswith("models/"):
            normalized = normalized.split("models/", 1)[1].strip()

        lowered = normalized.lower()
        if "-exp" in lowered and "thinking" not in lowered:
            return self.models.get("chat", "gemini-2.5-flash")

        return normalized

    def _sanitize_model_text(self, text: Optional[str]) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç —Å–ª—É–∂–µ–±–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –ø–æ–¥—á–∏—â–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –æ—Ç–≤–µ—Ç–∞ –≤ Telegram/–ø–∞–º—è—Ç—å.
        """
        if not text:
            return ""

        cleaned = str(text)

        # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–ª—É–∂–µ–±–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã box, –Ω–æ –Ω–µ –ø–æ–ª–µ–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏.
        cleaned = cleaned.replace("<|begin_of_box|>", "")
        cleaned = cleaned.replace("<|end_of_box|>", "")
        # [HOTFIX] –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–µ—Ö-—Ç–µ–≥–∏ —Ñ–æ—Ä–º–∞—Ç–∞ <|...|>.
        cleaned = re.sub(r"<\|[^|>]+?\|>", "", cleaned)
        cleaned = cleaned.replace("</s>", "").replace("<s>", "")

        # –¢–æ—á–µ—á–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ —Å —É—Ç–µ—á–∫–æ–π —Å–ª—É–∂–µ–±–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤.
        blocked_fragments = (
            "begin_of_box",
            "end_of_box",
            "no_reply",
            "heartbeat_ok",
            "i will now call the",
            "memory_get",
            "memory_search",
            "sessions_spawn",
            "session_send",
            "sessions_send",
            "\"action\": \"sessions_send\"",
            "\"action\":\"sessions_send\"",
            "\"sessionkey\"",
            "\"default channel",
            "## /users/",
            "# agents.md - workspace agents",
            "## agent list",
            "### default agents",
            "</tool_call>",
            "```json",
        )
        filtered_lines: list[str] = []
        for line in cleaned.splitlines():
            low = line.strip().lower()
            if low in {"```", "```json", "```text", "```yaml"}:
                continue
            if any(fragment in low for fragment in blocked_fragments):
                continue
            filtered_lines.append(line)
        cleaned = "\n".join(filtered_lines)

        # –°—Ö–ª–æ–ø—ã–≤–∞–µ–º –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏–µ –¥—É–±–ª–∏ —Å—Ç—Ä–æ–∫.
        deduped_lines: list[str] = []
        last_norm = ""
        repeat_count = 0
        for line in cleaned.splitlines():
            normalized = re.sub(r"\s+", " ", line).strip().lower()
            if normalized and normalized == last_norm:
                repeat_count += 1
            else:
                last_norm = normalized
                repeat_count = 1
            if repeat_count <= 2:
                deduped_lines.append(line)
        cleaned = "\n".join(deduped_lines)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –∫—Ä–∞—ë–≤.
        cleaned = re.sub(r"\n{3,}", "\n\n", cleaned).strip()
        return cleaned

    def _is_local_only_model_identifier(self, model_name: Optional[str]) -> bool:
        """
        –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–≤–Ω–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ ID, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å—Ç–æ–∏—Ç –ø—Ä–æ–±–æ–≤–∞—Ç—å –≤ cloud.
        """
        if not model_name:
            return False
        lowered = model_name.strip().lower()
        if not lowered:
            return False
        local_markers = (
            "-mlx",
            "_mlx",
            ".mlx",
            ".gguf",
            "gguf",
            "q4_",
            "q5_",
            "q6_",
            "q8_",
            "lm-studio",
            "ollama",
            "local-model",
        )
        return any(marker in lowered for marker in local_markers)

    def _lm_studio_api_root(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –∞–¥—Ä–µ—Å LM Studio –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞ /v1 –∏–ª–∏ /api/v1.
        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–∑–Ω—ã–µ REST-–ø—É—Ç–∏ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω –∫–æ—Ä–µ–Ω—å.
        """
        root = self.lm_studio_url.rstrip("/")
        for suffix in ("/api/v1", "/v1"):
            if root.endswith(suffix):
                root = root[: -len(suffix)]
                break
        return root.rstrip("/")

    def _normalize_model_entries(self, payload: Any) -> List[Dict[str, Any]]:
        """
        –ü—Ä–∏–≤–æ–¥–∏—Ç –æ—Ç–≤–µ—Ç LM Studio / OpenClaw –∫ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–æ–¥–µ–ª—è–º–∏.
        """
        entries: List[Dict[str, Any]] = []
        candidate = []
        if isinstance(payload, dict):
            if isinstance(payload.get("models"), list):
                candidate = payload["models"]
            elif isinstance(payload.get("data"), list):
                candidate = payload["data"]
            elif isinstance(payload.get("result"), list):
                candidate = payload["result"]
            else:
                candidate = []
        elif isinstance(payload, list):
            candidate = payload
        else:
            candidate = []

        for item in candidate:
            if isinstance(item, dict):
                entries.append(item)
            else:
                entries.append({"id": str(item)})
        return entries

    def _extract_model_id(self, entry: Dict[str, Any]) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Ç–∞–µ–º—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –∏–∑ –∑–∞–ø–∏—Å–∏ LM Studio.
        """
        for key in ("id", "key", "modelId", "identifier", "name"):
            value = entry.get(key)
            if value:
                return str(value)
        return None

    def _is_lmstudio_model_loaded(self, entry: Dict[str, Any]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ LM Studio.

        –ü–æ—á–µ–º—É —Ç–∞–∫:
        –í —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö LM Studio loaded-—Å—Ç–∞—Ç—É—Å –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö
        (`loaded_instances`, `loaded`, `state`, `status`, `availability`).
        –ï—Å–ª–∏ —á–∏—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –ø–æ–ª–µ, !status –º–æ–∂–µ—Ç –æ—à–∏–±–æ—á–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å
        `no_model_loaded`, —Ö–æ—Ç—è –º–æ–¥–µ–ª—å —É–∂–µ –æ—Ç–≤–µ—á–∞–µ—Ç –≤ /chat/completions.
        """
        if not isinstance(entry, dict):
            return False

        loaded_instances = entry.get("loaded_instances")
        if isinstance(loaded_instances, list) and len(loaded_instances) > 0:
            return True

        explicit_bool = entry.get("loaded")
        if isinstance(explicit_bool, bool):
            return explicit_bool

        state_fields = []
        for key in ("state", "status", "availability"):
            raw = entry.get(key)
            if raw is None:
                continue
            state_fields.append(str(raw).strip().lower())

        positive_tokens = {"ready", "loaded", "active", "running", "online"}
        negative_tokens = {"unloaded", "not_loaded", "not loaded", "idle_unloaded", "evicted", "offline"}

        for state in state_fields:
            if state in positive_tokens:
                return True
            if state in negative_tokens:
                return False

        return False

    def _is_runtime_error_message(self, text: Optional[str]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç (–æ—Ç Cloud –∏–ª–∏ Local) —è–≤–Ω–æ–π –æ—à–∏–±–∫–æ–π —Ä–∞–Ω—Ç–∞–π–º–∞.
        [R12] Unified detector for local/cloud errors.
        """
        if not text:
            return True
        lowered = text.strip().lower()
        
        # –û–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫
        error_indicators = (
            "‚ùå", "‚ö†Ô∏è", "llm error", "error:", "exception:", "crashed",
            "connection refused", "connection error", "network error",
            "failed to fetch", "internal server error",
            "500 internal", "502 bad gateway", "503 service unavailable", "504 gateway timeout",
            "failed to connect", "upstream connect error", "socket timeout",
            "read timeout", "timed out", "empty response", "no response"
        )
        if any(indicator in lowered for indicator in error_indicators):
            return True

        # –õ–æ–∫–∞–ª—å–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ (LM Studio / Ollama)
        local_errors = (
            "no models loaded", "please load a model", "failed to load model",
            "not loaded", "is not found", "model not active", "server is initializing",
            "insufficient vram", "out of memory"
        )
        if any(err in lowered for err in local_errors):
            return True

        # Cloud —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ (Gemini / OpenClaw)
        cloud_errors = (
            "billing error", "out of credits", "quota exceeded", "api key invalid",
            "safety filter", "blocked by safety", "finish_reason: safety",
            "401 unauthorized", "403 forbidden", "429 too many requests"
        )
        if any(err in lowered for err in cloud_errors):
            return True

        # JSON –æ—à–∏–±–∫–∏
        if lowered.startswith("{") and '"error"' in lowered:
            return True

        return False

    def _is_cloud_error_message(self, text: Optional[str]) -> bool:
        """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤—ã–∑–æ–≤–æ–≤."""
        return self._is_runtime_error_message(text)

    def _is_cloud_billing_error(self, text: str) -> bool:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç billing-–æ—à–∏–±–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
        –ò—Å–∫–ª—é—á–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ Rate Limit (quota exceeded).
        """
        lowered = text.lower()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ rate limit –∏–ª–∏ 429 ‚Äî —ç—Ç–æ –ù–ï –æ—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞, –∞ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∞
        if "rate limit" in lowered or "429" in lowered:
            return False

        billing_keywords = [
            "billing error",
            "out of credits",
            "insufficient balance",
            "insufficient funds",
            "billing",
            "credit balance",
        ]
        
        # 'quota' —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏ –¥–ª—è –±–∏–ª–ª–∏–Ω–≥–∞ –∏ –¥–ª—è —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–æ–≤. 
        # –°—á–∏—Ç–∞–µ–º –∑–∞ –±–∏–ª–ª–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï–¢ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è rate limit.
        if "quota" in lowered and "rate" not in lowered:
             return True

        return any(keyword in lowered for keyword in billing_keywords)

    def _mark_cloud_soft_cap_if_needed(self, error_text: str) -> None:
        """
        –ü—Ä–∏ billing-–æ—à–∏–±–∫–µ –ø–∏—à–µ—Ç –≤ –ª–æ–≥, –Ω–æ –ù–ï –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ–±–ª–∞–∫–æ, 
        —Ç–∞–∫ –∫–∞–∫ –º—ã –¥–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        """
        if self._is_cloud_billing_error(error_text):
            logger.warning("Cloud warning (billing-related): %s. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–ø—ã—Ç–∫–∏.", error_text)
            # self.cloud_soft_cap_reached = True  <-- –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞

    def _is_fatal_cloud_auth_error(self, text: Optional[str]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–∞—Ç–∞–ª—å–Ω—ã–µ cloud-–æ—à–∏–±–∫–∏, –ø—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —Å–º—ã—Å–ª–∞ –ø–µ—Ä–µ–±–∏—Ä–∞—Ç—å
        –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —ç—Ç–æ–≥–æ –∂–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –≤ —Ç–µ–∫—É—â–µ–º –∑–∞–ø—Ä–æ—Å–µ.
        """
        lowered = str(text or "").strip().lower()
        if not lowered:
            return False

        fatal_markers = (
            "unauthorized",
            "invalid api key",
            "incorrect api key",
            "api key was reported as leaked",
            "permission_denied",
            "forbidden",
            "generative language api has not been used",
            "api has not been used in project",
            "it is disabled",
            "enable it by visiting",
            "quota exceeded",
            "out of credits",
            "insufficient balance",
            "billing error",
        )
        return any(marker in lowered for marker in fatal_markers)

    def _classify_cloud_error(self, text: Optional[str]) -> dict:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç cloud-–æ—à–∏–±–∫—É –≤ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–æ–¥ + —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—É—é —Å–≤–æ–¥–∫—É.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ !status/!diagnose –∏ –≤ user-facing fallback-–æ—Ç–≤–µ—Ç–∞—Ö.
        """
        lowered = str(text or "").strip().lower()
        if not lowered:
            return {
                "code": "none",
                "summary": "–æ–±–ª–∞—á–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                "retryable": True,
            }

        if "reported as leaked" in lowered:
            return {
                "code": "api_key_leaked",
                "summary": "–∫–ª—é—á –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ —Å–∫–æ–º–ø—Ä–æ–º–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π (leaked) ‚Äî –Ω—É–∂–µ–Ω –Ω–æ–≤—ã–π API key",
                "retryable": False,
            }
        if "invalid api key" in lowered or "incorrect api key" in lowered:
            return {
                "code": "api_key_invalid",
                "summary": "API key –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π",
                "retryable": False,
            }
        if (
            "generative language api has not been used" in lowered
            or "api has not been used in project" in lowered
            or "it is disabled" in lowered
            or "enable it by visiting" in lowered
        ):
            return {
                "code": "api_disabled",
                "summary": "Generative Language API –Ω–µ –≤–∫–ª—é—á—ë–Ω –≤ Google Cloud –ø—Ä–æ–µ–∫—Ç–µ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª—é—á–∞",
                "retryable": False,
            }
        if "permission_denied" in lowered or "403" in lowered:
            return {
                "code": "permission_denied",
                "summary": "–¥–æ—Å—Ç—É–ø –∫ –æ–±–ª–∞—á–Ω–æ–π –º–æ–¥–µ–ª–∏ –æ—Ç–∫–ª–æ–Ω—ë–Ω –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º (403)",
                "retryable": False,
            }
        if "unauthorized" in lowered or "401" in lowered:
            return {
                "code": "unauthorized",
                "summary": "–æ—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ cloud-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (401)",
                "retryable": False,
            }
        
        if "quota" in lowered or "billing" in lowered or "limit" in lowered:
            return {
                "code": "quota",
                "summary": "–ø—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ —Å –±–∏–ª–ª–∏–Ω–≥–æ–º",
                "retryable": False,
            }

        if "not found" in lowered or "404" in lowered:
            return {
                "code": "model_not_found",
                "summary": "–º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–µ–∫—É—â–µ–º —Ä–µ–≥–∏–æ–Ω–µ/–∞–∫–∫–∞—É–Ω—Ç–µ",
                "retryable": False,
            }

        if "fail-fast budget" in lowered:
            return {
                "code": "timeout",
                "summary": "–ø—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –æ–±–ª–∞—á–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞",
                "retryable": True,
            }

        if "connection error" in lowered or "timeout" in lowered or "network" in lowered:
            return {
                "code": "network",
                "summary": "–ø—Ä–æ–±–ª–µ–º–∞ —Å —Å–µ—Ç—å—é –∏–ª–∏ —Ç–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è",
                "retryable": True,
            }

        return {
            "code": "unknown",
            "summary": lowered[:120] if lowered else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
            "retryable": True,
        }

    def _check_cloud_preflight(self, provider: str) -> Optional[str]:
        """
        R15: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ—Ç –ª–∏ –≤ –∫—ç—à–µ –∞–∫—Ç–∏–≤–Ω–æ–π —Ñ–∞—Ç–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–∏ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ, –µ—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω.
        """
        cached = self._preflight_cache.get(provider)
        if not cached:
            return None
        
        expiration, error_msg = cached
        if time.time() < expiration:
            return f"Preflight: –ø—Ä–æ–≤–∞–π–¥–µ—Ä '{provider}' –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω (R15 Gate): {error_msg}"
        
        # –ò—Å—Ç–µ–∫–ª–æ
        self._preflight_cache.pop(provider, None)
        return None

    def _categorize_cloud_error(self, text: Optional[str]) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é cloud-–æ—à–∏–±–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤ –∏ –ª–æ–≥–æ–≤.

        –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:
        - auth_fatal    ‚Äî leaked/invalid key, permission denied, unauthorized
        - api_disabled  ‚Äî API not enabled in Google Cloud project
        - quota         ‚Äî quota exceeded, billing, out of credits
        - model_not_found ‚Äî 404 NOT_FOUND
        - network       ‚Äî connection error, timeout
        - unknown       ‚Äî –ø—Ä–æ—á–µ–µ

        –ó–∞—á–µ–º: _classify_cloud_error() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict ‚Äî —É–¥–æ–±–Ω–æ –≤–Ω—É—Ç—Ä–∏ —Ä–æ—É—Ç–µ—Ä–∞.
        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω—É–∂–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–æ–≤ –∏ –±—ã—Å—Ç—Ä–æ–≥–æ if/match –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞—Ö.
        """
        info = self._classify_cloud_error(text)
        code = str(info.get("code") or "unknown")
        # –£–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–¥–æ–≤ –ø–æ–¥ –æ–±—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é auth_fatal.
        if code in {"api_key_leaked", "api_key_invalid", "unauthorized", "permission_denied"}:
            return "auth_fatal"
        if code == "api_disabled":
            return "api_disabled"
        if code in {"quota_or_billing", "quota"}:
            return "quota"
        if code == "model_not_found":
            return "model_not_found"
        if code in {"network_error", "timeout", "network"}:
            return "network"
        return "unknown"

    def _summarize_cloud_error_for_user(self, text: Optional[str]) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É cloud-–æ—à–∏–±–∫–∏
        –±–µ–∑ —Å—ã—Ä–æ–≥–æ JSON/stacktrace.
        """
        info = self._classify_cloud_error(text)
        return str(info.get("summary") or "–æ–±–ª–∞—á–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É")

    def get_last_cloud_error_info(self) -> dict:
        """
        –ü—É–±–ª–∏—á–Ω—ã–π runtime-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π cloud-–æ—à–∏–±–∫–µ.
        –ù—É–∂–µ–Ω –∫–æ–º–∞–Ω–¥–∞–º —Å—Ç–∞—Ç—É—Å–∞/–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –≤–µ–±-–ø–∞–Ω–µ–ª–∏.
        """
        raw = str(self.last_cloud_error or "").strip()
        info = self._classify_cloud_error(raw)
        return {
            "has_error": bool(raw),
            "code": str(info.get("code") or "none"),
            "summary": str(info.get("summary") or ""),
            "retryable": bool(info.get("retryable", True)),
            "last_provider_model": str(getattr(self, "last_cloud_model", "") or ""),
            "raw_excerpt": raw[:320] if raw else "",
        }

    def _ensure_feedback_store(self) -> dict:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç feedback store –∫ –æ–∂–∏–¥–∞–µ–º–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ."""
        if not isinstance(self._feedback_store, dict):
            self._feedback_store = {"profiles": {}, "events": [], "last_route": {}, "updated_at": None}
        if not isinstance(self._feedback_store.get("profiles"), dict):
            self._feedback_store["profiles"] = {}
        if not isinstance(self._feedback_store.get("events"), list):
            self._feedback_store["events"] = []
        if not isinstance(self._feedback_store.get("last_route"), dict):
            self._feedback_store["last_route"] = {}
        return self._feedback_store

    def _normalize_channel(self, channel: Optional[str]) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–º—è –∫–∞–Ω–∞–ª–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏."""
        lowered = str(channel or "").strip().lower()
        if lowered in {"local", "cloud"}:
            return lowered
        return "local"

    def _remember_last_route(
        self,
        profile: str,
        task_type: str,
        channel: str,
        model_name: str,
        prompt: str = "",
        route_reason: str = "",
        route_detail: str = "",
        force_mode: Optional[str] = None,
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞,
        —á—Ç–æ–±—ã –≤–ª–∞–¥–µ–ª–µ—Ü –º–æ–≥ –æ—Ü–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞ profile/model.
        """
        store = self._ensure_feedback_store()
        route = {
            "ts": self._now_iso(),
            "profile": (profile or "chat").strip().lower() or "chat",
            "task_type": (task_type or "chat").strip().lower() or "chat",
            "channel": self._normalize_channel(channel),
            "model": (model_name or "unknown").strip() or "unknown",
            "prompt_preview": (prompt or "").strip()[:160],
            "route_reason": (route_reason or "").strip()[:80],
            "route_detail": (route_detail or "").strip()[:240],
            "force_mode": str(force_mode or self.force_mode or "auto").strip() or "auto",
            "local_available": bool(self.is_local_available),
        }
        store["last_route"] = route
        history = store.setdefault("route_history", [])
        if not isinstance(history, list):
            history = []
            store["route_history"] = history
        history.append(route)
        if len(history) > 60:
            del history[: len(history) - 60]
        store["updated_at"] = self._now_iso()
        self._save_json(self._feedback_path, store)

    def _remember_last_stream_route(
        self,
        profile: str,
        task_type: str,
        channel: str,
        model_name: str,
        prompt: str = "",
        route_reason: str = "",
        route_detail: str = "",
        force_mode: Optional[str] = None,
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ stream-–æ—Ç–≤–µ—Ç–∞.
        –ù—É–∂–µ–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –≤ !status –±–µ–∑ —á—Ç–µ–Ω–∏—è –ª–æ–≥–æ–≤.
        """
        self._last_stream_route = {
            "ts": self._now_iso(),
            "profile": (profile or "chat").strip().lower() or "chat",
            "task_type": (task_type or "chat").strip().lower() or "chat",
            "channel": self._normalize_channel(channel),
            "model": (model_name or "unknown").strip() or "unknown",
            "prompt_preview": (prompt or "").strip()[:160],
            "route_reason": (route_reason or "").strip()[:80],
            "route_detail": (route_detail or "").strip()[:240],
            "force_mode": str(force_mode or self.force_mode or "auto").strip() or "auto",
            "local_available": bool(self.is_local_available),
        }

    def _get_model_feedback_stats(self, profile: str, model_name: str) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É feedback –ø–æ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø—Ä–æ—Ñ–∏–ª–µ."""
        store = self._ensure_feedback_store()
        profiles = store.get("profiles", {})
        profile_data = profiles.get(profile, {}) if isinstance(profiles, dict) else {}
        models = profile_data.get("models", {}) if isinstance(profile_data, dict) else {}
        entry = models.get(model_name, {}) if isinstance(models, dict) else {}
        count = int(entry.get("count", 0)) if isinstance(entry, dict) else 0
        avg = float(entry.get("avg", 0.0)) if isinstance(entry, dict) else 0.0
        return {"count": count, "avg": round(avg, 3)}

    def classify_task_profile(self, prompt: str, task_type: str = "chat") -> str:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–æ—É—Ç–∏–Ω–≥–∞.
        –ü—Ä–æ—Ñ–∏–ª–∏: chat, moderation, code, security, infra, review, communication.
        """
        normalized_type = (task_type or "chat").strip().lower()
        if normalized_type in {"coding", "code"}:
            return "code"
        if normalized_type in {"reasoning", "review"}:
            return "review"

        text = (prompt or "").lower()
        keyword_map = {
            "moderation": ["ban", "mute", "warn", "delete message", "—Å–ø–∞–º", "–º–æ–¥–µ—Ä–∞—Ü", "muted"],
            "security": ["vulnerability", "security", "audit", "exploit", "—É—è–∑–≤", "–±–µ–∑–æ–ø–∞—Å"],
            "infra": ["deploy", "terraform", "k8s", "kubernetes", "docker", "infra", "—Å–µ—Ä–≤–µ—Ä", "ci/cd"],
            "review": ["code review", "critique", "–ø—Ä–æ–≤–µ—Ä—å –∫–æ–¥", "—Ä–µ–≤—å—é", "–∫—Ä–∏—Ç–∏–∫–∞"],
            "communication": ["translate", "–ø–µ—Ä–µ–≤–æ–¥", "summary", "—Å–∞–º–º–∞—Ä–∏", "telegram", "—á–∞—Ç"],
            "code": ["python", "typescript", "javascript", "refactor", "bugfix", "–∫–æ–¥", "—Å–∫—Ä–∏–ø—Ç"],
        }
        for profile, markers in keyword_map.items():
            if any(marker in text for marker in markers):
                return profile
        return "chat"

    def _is_critical_profile(self, profile: str) -> bool:
        """–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏, –≥–¥–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞."""
        return profile in {"security", "infra", "review"}

    def _should_use_pro_for_owner_private(self, prompt: str, chat_type: str, is_owner: bool) -> bool:
        """
        –ü–æ–ª–∏—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –≤–ª–∞–¥–µ–ª—å—Ü–∞ –≤ –ª–∏—á–∫–µ:
        –µ—Å–ª–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ –ø—Ä–æ –ø—Ä–æ–µ–∫—Ç/–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ/–∫—Ä–∏—Ç–∏—á–Ω—É—é —Ä–∞–±–æ—Ç—É,
        –≤ cloud-–≤–µ—Ç–∫–µ –ø–æ–¥–Ω–∏–º–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç PRO-–º–æ–¥–µ–ª–∏.
        """
        if not is_owner:
            return False
        if (chat_type or "").strip().lower() != "private":
            return False

        text = (prompt or "").lower()
        pro_markers = (
            "–ø—Ä–æ–µ–∫—Ç",
            "–ø–ª–∞–Ω",
            "roadmap",
            "–∞—Ä—Ö–∏—Ç–µ–∫—Ç",
            "–≤–∞–∂–Ω",
            "–∫—Ä–∏—Ç–∏—á",
            "–ø—Ä–æ–¥",
            "production",
            "–º–∏–≥—Ä–∞—Ü",
            "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä",
            "—Å—Ç—Ä–∞—Ç–µ–≥",
            "–±—é–¥–∂–µ—Ç",
        )
        return any(marker in text for marker in pro_markers)

    @staticmethod
    def _is_group_chat(chat_type: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø–æ–≤—ã–µ —Ç–∏–ø—ã —á–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –±—é–¥–∂–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏.
        """
        normalized = (chat_type or "").strip().lower()
        return normalized in {"group", "supergroup"}

    def _model_tier(self, model_name: Optional[str]) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è scheduler-–∞:
        heavy –∏–ª–∏ light.
        """
        if not model_name:
            return "light"
        lowered = model_name.lower()
        if any(token in lowered for token in ["70b", "72b", "34b", "32b", "30b", "27b", "22b", "20b", "mixtral"]):
            return "heavy"

        match = re.search(r"(\d+)\s*b", lowered)
        if match:
            try:
                size_b = int(match.group(1))
                return "heavy" if size_b >= 20 else "light"
            except ValueError:
                return "light"
        return "light"

    @asynccontextmanager
    async def _acquire_local_slot(self, model_name: Optional[str]):
        """
        –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤:
        - heavy: –º–∞–∫—Å–∏–º—É–º 1 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π heavy.
        - light: –º–∞–∫—Å–∏–º—É–º 1 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π light.
        """
        tier = self._model_tier(model_name)
        semaphore = self._local_heavy_slot if tier == "heavy" else self._local_light_slot
        await semaphore.acquire()
        try:
            yield tier
        finally:
            semaphore.release()

    def _remember_model_choice(self, profile: str, model_name: str, channel: str) -> None:
        """
        –ó–∞–ø–æ–º–∏–Ω–∞–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á.
        """
        if not profile or not model_name:
            return

        memory = self._routing_memory.setdefault("profiles", {})
        profile_entry = memory.setdefault(profile, {"models": {}, "channels": {}})
        profile_entry["models"][model_name] = int(profile_entry["models"].get(model_name, 0)) + 1
        profile_entry["channels"][channel] = int(profile_entry["channels"].get(channel, 0)) + 1
        self._save_json(self._routing_memory_path, self._routing_memory)

    def _update_usage_report(self, profile: str, model_name: str, channel: str) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ—Ç—á—ë—Ç usage/cost guardrails."""
        profiles = self._usage_report.setdefault("profiles", {})
        profiles[profile] = int(profiles.get(profile, 0)) + 1

        models = self._usage_report.setdefault("models", {})
        models[model_name] = int(models.get(model_name, 0)) + 1

        channels = self._usage_report.setdefault("channels", {"local": 0, "cloud": 0})
        channels[channel] = int(channels.get(channel, 0)) + 1

        if channel == "cloud" and channels.get("cloud", 0) >= self.cloud_soft_cap_calls:
            self.cloud_soft_cap_reached = True

        self._save_json(self._usage_report_path, self._usage_report)

    def _get_profile_recommendation(self, profile: str) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –∫–∞–Ω–∞–ª –¥–ª—è –ø—Ä–æ—Ñ–∏–ª—è.
        """
        profile = profile or "chat"
        profiles = self._routing_memory.get("profiles", {})
        memorized = profiles.get(profile, {})
        memorized_models = memorized.get("models", {})
        memorized_channels = memorized.get("channels", {})

        top_model = None
        top_channel = None
        if memorized_models:
            top_model = max(memorized_models.items(), key=lambda item: int(item[1]))[0]
        if memorized_channels:
            top_channel = max(memorized_channels.items(), key=lambda item: int(item[1]))[0]

        # –î–µ—Ñ–æ–ª—Ç—ã ‚Äî LOCAL FIRST –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π, –∫—Ä–æ–º–µ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö
        default_model = self.models.get("chat", "gemini-2.0-flash")
        default_channel = "local"  # Local First —Å—Ç—Ä–∞—Ç–µ–≥–∏—è

        if profile in {"security", "infra", "review"}:
            default_model = self.models.get("pro", self.models.get("thinking", self.models["chat"]))
            default_channel = "cloud"
        elif profile == "code":
            default_model = self.models.get("coding", self.models["chat"])
            default_channel = "local"
        elif profile == "moderation":
            default_model = self.models.get("chat", "gemini-2.0-flash")
            default_channel = "local"
        elif profile == "chat":
            # –û–±—ã—á–Ω—ã–π —á–∞—Ç ‚Äî –í–°–ï–ì–î–ê local first
            default_channel = "local"


        # Adaptive feedback loop: –µ—Å–ª–∏ –ø–æ –º–æ–¥–µ–ª–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω—ã –æ—Ü–µ–Ω–∫–∏,
        # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤–∑–≤–µ—à–∏–≤–∞–µ–º –≤—ã–±–æ—Ä –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É –∫–∞—á–µ—Å—Ç–≤—É.
        store = self._ensure_feedback_store()
        feedback_profiles = store.get("profiles", {})
        feedback_profile = feedback_profiles.get(profile, {}) if isinstance(feedback_profiles, dict) else {}
        feedback_models = feedback_profile.get("models", {}) if isinstance(feedback_profile, dict) else {}

        candidate_models = set(memorized_models.keys()) if isinstance(memorized_models, dict) else set()
        if isinstance(feedback_models, dict):
            candidate_models.update(feedback_models.keys())
        if not candidate_models and default_model:
            candidate_models.add(default_model)

        if candidate_models:
            best_model = None
            best_score = None
            for candidate in candidate_models:
                usage_count = int(memorized_models.get(candidate, 0)) if isinstance(memorized_models, dict) else 0
                feedback_entry = feedback_models.get(candidate, {}) if isinstance(feedback_models, dict) else {}
                feedback_count = int(feedback_entry.get("count", 0)) if isinstance(feedback_entry, dict) else 0
                feedback_avg = float(feedback_entry.get("avg", 0.0)) if isinstance(feedback_entry, dict) else 0.0

                # –ë–∞–∑–æ–≤—ã–π –≤–µ—Å usage + –≤–µ—Å –∫–∞—á–µ—Å—Ç–≤–∞.
                quality_weight = (feedback_avg / 5.0) * min(feedback_count, 12)
                score = float(usage_count) + float(quality_weight)

                # –ñ–µ—Å—Ç–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ —Å–∏—Å—Ç–µ–º–Ω–æ –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏.
                if feedback_count >= 3 and feedback_avg <= 2.4:
                    score -= 4.0

                if best_score is None or score > best_score:
                    best_score = score
                    best_model = candidate

            if best_model:
                top_model = best_model

        selected_model = top_model or default_model
        feedback_hint = self._get_model_feedback_stats(profile, selected_model)

        # –î–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π ‚Äî routing_memory –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç.
        # –î–ª—è –æ–±—ã—á–Ω—ã—Ö (chat, code, moderation) ‚Äî default_channel –≤–∞–∂–Ω–µ–µ,
        # —á—Ç–æ–±—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏—è Local First —Å–æ–±–ª—é–¥–∞–ª–∞—Å—å.
        is_critical_profile = profile in {"security", "infra", "review"}
        resolved_channel = (
            (top_channel or default_channel) if is_critical_profile
            else default_channel
        )

        return {
            "profile": profile,
            "model": selected_model,
            "channel": resolved_channel,
            "critical": self._is_critical_profile(profile),
            "feedback_hint": {
                "avg_score": feedback_hint.get("avg", 0.0),
                "count": feedback_hint.get("count", 0),
            },
        }

    def _resolve_cloud_model(
        self,
        task_type: str,
        profile: str,
        preferred_model: Optional[str] = None,
        chat_type: str = "private",
        is_owner: bool = False,
        prompt: str = "",
    ) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç –æ–±–ª–∞—á–Ω—É—é –º–æ–¥–µ–ª—å —Å —É—á–µ—Ç–æ–º –ø—Ä–æ—Ñ–∏–ª—è –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π."""
        if preferred_model and "gemini" in preferred_model:
            return preferred_model
        if profile in {"security", "infra", "review"}:
            return self.models.get("pro", self.models.get("thinking", self.models["chat"]))
        if profile == "code":
            return self.models.get("coding", self.models["chat"])
        if task_type == "reasoning":
            return self.models.get("thinking", self.models["chat"])
        # –î–ª—è owner/private –¥–µ—Ä–∂–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞:
        # - –≤–∞–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ pro,
        # - –ø—Ä–∏ —è–≤–Ω–æ–º —Ñ–ª–∞–≥–µ always_pro ‚Äî –≤—Å–µ–≥–¥–∞ pro –≤ –ª–∏—á–∫–µ –≤–ª–∞–¥–µ–ª—å—Ü–∞.
        if is_owner and (chat_type or "").strip().lower() == "private":
            if self.owner_private_always_pro:
                return self.models.get("pro", self.models["chat"])
            if self._should_use_pro_for_owner_private(prompt, chat_type, is_owner):
                return self.chat_model_owner_private_important or self.models.get("pro", self.models["chat"])
            if self.chat_model_owner_private:
                return self.chat_model_owner_private

        # –î–ª—è –≥—Ä—É–ø–ø–æ–≤—ã—Ö —á–∞—Ç–æ–≤ –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é –±–æ–ª–µ–µ –±—é–¥–∂–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å.
        if self._is_group_chat(chat_type) and self.chat_model_group:
            return self.chat_model_group

        return self.models.get(task_type, self.models["chat"])

    def _build_cloud_candidates(
        self,
        task_type: str,
        profile: str,
        preferred_model: Optional[str] = None,
        chat_type: str = "private",
        is_owner: bool = False,
        prompt: str = "",
    ) -> List[str]:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–ª—è cloud-–ø–æ–¥—Å–∏—Å—Ç–µ–º—ã.
        """
        base = self._resolve_cloud_model(
            task_type=task_type,
            profile=profile,
            preferred_model=preferred_model,
            chat_type=chat_type,
            is_owner=is_owner,
            prompt=prompt,
        )
        candidates: list[str] = []
        seen: Set[str] = set()

        def add(model_name: Optional[str]) -> None:
            if not model_name:
                return
            normalized = self._normalize_cloud_model_name(model_name)
            if not normalized or normalized in seen:
                return
            if self._is_local_only_model_identifier(normalized):
                logger.info("–ü—Ä–æ–ø—É—Å–∫–∞—é –ª–æ–∫–∞–ª—å–Ω—ã–π model_id –≤ cloud candidate list", model=normalized, profile=profile)
                return
            seen.add(normalized)
            candidates.append(normalized)

        add(preferred_model or "")
        add(base)
        for extra in self.cloud_priority_models:
            add(extra)

        max_candidates = (
            int(self.cloud_max_candidates_force_cloud)
            if self.force_mode == "force_cloud"
            else int(self.cloud_max_candidates_per_request)
        )
        if len(candidates) > max_candidates:
            dropped = candidates[max_candidates:]
            candidates = candidates[:max_candidates]
            logger.info(
                "Cloud candidate list truncated",
                max_candidates=max_candidates,
                force_mode=self.force_mode,
                kept=candidates,
                dropped_count=len(dropped),
            )

        return candidates

    async def unload_models_manual(self) -> bool:
        """
        [R11] –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è RAM).
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Watchdog-–æ–º –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –Ω–µ—Ö–≤–∞—Ç–∫–µ –ø–∞–º—è—Ç–∏.
        """
        logger.warning("üö® Manual model unload requested (Soft Healing)")
        base_root = self._lm_studio_api_root()
        url = f"{base_root}/api/v1/models/unload"
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json={"all": True}, timeout=10) as resp:
                    if resp.status == 200:
                        logger.info("‚úÖ All local models unloaded successfully")
                        self.active_local_model = None
                        self.is_local_available = False
                        return True
                    else:
                        logger.error(f"‚ùå Failed to unload models: status {resp.status}")
        except Exception as e:
            logger.error(f"‚ùå Error during model unload: {e}")
        return False

    async def check_local_health(self, force: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞ (LM Studio ‚Üí Ollama).
        """
        now = time.time()
        if not force and (now - self._health_cache_ts) < self._health_cache_ttl:
            return self.is_local_available

        self._health_cache_ts = now

        base_root = self._lm_studio_api_root()
        if not base_root:
            base_root = self.lm_studio_url.rstrip("/")

        # –í light-—Ä–µ–∂–∏–º–µ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º /models –Ω–∞ –∫–∞–∂–¥–æ–º health-check,
        # —á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞—Ç—å idle-unload –≤ LM Studio.
        need_full_scan = (
            force
            or self._health_probe_mode == "models"
            or self._health_full_scan_ts <= 0
            or (now - self._health_full_scan_ts) >= self._health_full_scan_interval
        )
        if not need_full_scan and self._health_probe_mode == "light":
            lm_server_alive = await self._light_ping_local_server(base_root)
            if lm_server_alive:
                # –°–µ—Ä–≤–µ—Ä –∂–∏–≤ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ª–æ–∫–∞–ª–∫–∏ –±–µ–∑ —Å–∫–∞–Ω–∞ –º–æ–¥–µ–ª–µ–π.
                # –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å—á—ë—Ç loaded-–º–æ–¥–µ–ª–µ–π –ø—Ä–æ–∏–∑–æ–π–¥—ë—Ç –Ω–∞ force-check –∏–ª–∏ —Ä–µ–¥–∫–æ–º full-scan.
                return self.is_local_available
            logger.warning("Light health probe: LM Studio server –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –¥–µ–ª–∞—é fallback-–ø—Ä–æ–≤–µ—Ä–∫—É.")

        if need_full_scan:
            self._health_full_scan_ts = now

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –†–ï–ê–õ–¨–ù–û –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ /api/v1/models
        # (–≤ 0.3.x –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –∏–ª–∏ —ç—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–±)
        try:
            models = await self._scan_local_models()
            loaded_models = [m for m in models if m.get("loaded")]
            
            if loaded_models:
                self.local_engine = "lm-studio"
                self.is_local_available = True
                self.active_local_model = loaded_models[0]["id"]
                self.last_local_load_error = None
                self.last_local_load_error_human = None
                logger.info(f"‚úÖ Local AI active: {self.active_local_model} (LM Studio)")
                return True
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–∞–º–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{base_root}/api/v1/models") as resp:
                    if resp.status == 200:
                        self.local_engine = "lm-studio"
                        self.is_local_available = False # –ù–æ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!
                        self.active_local_model = None
                        self.last_local_load_error = "no_model_loaded"
                        self.last_local_load_error_human = "‚ö†Ô∏è LM Studio –¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."
                        logger.info("üì° LM Studio server alive, but no models loaded.")
                        return False
        except Exception:
            pass

        # Fallback to Ollama
        try:
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{self.ollama_url.replace('/api', '/v1')}/models") as response:
                    if response.status == 200:
                        payload = await response.json()
                        models = self._normalize_model_entries(payload)
                        if models:
                            self.active_local_model = self._extract_model_id(models[0]) or models[0].get("id")
                            self.local_engine = "ollama"
                            self.is_local_available = True
                            self.last_local_load_error = None
                            self.last_local_load_error_human = None
                            return True
        except Exception:
            pass

        self.is_local_available = False
        self.local_engine = None
        self.active_local_model = None
        if not self.last_local_load_error:
            self.last_local_load_error = "local_engine_unreachable"
        if not self.last_local_load_error_human:
            self.last_local_load_error_human = "‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω—ã–π –¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (LM Studio/Ollama unreachable)."
        return False

    async def _light_ping_local_server(self, base_root: str) -> bool:
        """
        –õ—ë–≥–∫–∏–π probe –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio –±–µ–∑ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π.

        –ü–æ—á–µ–º—É —Ç–∞–∫:
        - `/api/v1/models` –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, –Ω–æ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–π –≤—ã–∑–æ–≤
          –º–æ–∂–µ—Ç –º–µ—à–∞—Ç—å –∞–≤—Ç–æ-–≤—ã–≥—Ä—É–∑–∫–µ –ø–æ idle TTL;
        - –∑–¥–µ—Å—å –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ ¬´—Å–µ—Ä–≤–µ—Ä –∂–∏–≤ / —Å–µ—Ä–≤–µ—Ä –º—ë—Ä—Ç–≤¬ª —á–µ—Ä–µ–∑ —à—Ç–∞—Ç–Ω—ã–µ endpoint.
        """
        timeout = aiohttp.ClientTimeout(total=2)
        # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º /health: –≤ —Ä—è–¥–µ –≤–µ—Ä—Å–∏–π LM Studio —ç—Ç–æ —à—É–º–∏—Ç –≤ –ª–æ–≥–∞—Ö
        # —Å–æ–æ–±—â–µ–Ω–∏–µ–º "Unexpected endpoint or method".
        probe_paths = ("/v1/models", "/api/v1/models", "/")
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                for path in probe_paths:
                    try:
                        async with session.get(f"{base_root}{path}") as resp:
                            if resp.status < 500:
                                return True
                    except Exception:
                        continue
        except Exception:
            return False
        return False

    async def _scan_local_models(self) -> List[Dict[str, Any]]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ REST API LM Studio 0.3.x –∏–ª–∏ CLI.
        """
        base = self._lm_studio_api_root()
        url = f"{base}/api/v1/models"
        
        try:
            timeout = aiohttp.ClientTimeout(total=5)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        payload = await resp.json(content_type=None)
                        normalized = []
                        if isinstance(payload, dict):
                            # LM Studio 0.3.x: /api/v1/models ‚Üí {"models": [...]}
                            # OpenAI compat:    /v1/models    ‚Üí {"data": [...]}
                            normalized = payload.get("models") or payload.get("data") or []
                        elif isinstance(payload, list):
                            normalized = payload

                        models = []
                        for m in normalized:
                            # LM Studio 0.3.x –∏—Å–ø–æ–ª—å–∑—É–µ—Ç "key" –∫–∞–∫ ID –º–æ–¥–µ–ª–∏
                            identifier = m.get("key") or self._extract_model_id(m) or m.get("id", "")
                            if not identifier: continue
                            
                            # –í LM Studio –ø–æ–ª—è loaded-—Å—Ç–∞—Ç—É—Å–∞ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –≤–µ—Ä—Å–∏–∏ API.
                            is_loaded = self._is_lmstudio_model_loaded(m)
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ –ø–æ–ª—é "type" –∏–∑ API –∏–ª–∏ –ø–æ –∏–º–µ–Ω–∏
                            model_type = m.get("type", "")
                            if model_type == "embedding" or "embedding" in identifier.lower():
                                mtype = "embedding"
                            else:
                                mtype = "llm"
                            
                            models.append({
                                "id": identifier,
                                "type": mtype,
                                "name": m.get("display_name", m.get("name", identifier)),
                                "loaded": is_loaded,
                                # –†–∞–∑–º–µ—Ä –±–µ—Ä—ë–º –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –ø–æ–ª–µ–π LM Studio/OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.
                                "size_bytes": (
                                    m.get("size_on_disk")
                                    or m.get("size_bytes")
                                    or m.get("size")
                                    or 0
                                ),
                            })
                        return models
        except Exception:
            pass

        # Fallback to CLI only if API fails or exception occurs

        # Fallback to CLI only if API fails
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if not os.path.exists(lms_path):
            return []

        try:
            proc = await asyncio.create_subprocess_exec(
                lms_path, "ls",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await proc.communicate()
            output = stdout.decode()
            
            models = []
            is_embedding_section = False
            for line in output.splitlines():
                line = line.strip()
                if not line or "SIZE" in line: continue
                if "EMBEDDING" in line: is_embedding_section = True; continue
                if "LLM" in line: is_embedding_section = False; continue
                parts = line.split()
                if parts and ("/" in parts[0] or "-" in parts[0]):
                    models.append({
                        "id": parts[0],
                        "type": "embedding" if is_embedding_section else "llm"
                    })
            return models
        except Exception:
            return []

    async def _ensure_chat_model_loaded(self) -> bool:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å LLM –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ REST API.
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: LOCAL_PREFERRED_MODEL ‚Üí instruct/chat ‚Üí –ª—é–±–∞—è LLM.
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å ‚Äî –º–æ–∂–µ—Ç, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å
        if await self.check_local_health(force=True):
            if self.active_local_model and "embed" not in self.active_local_model.lower():
                return True

        models = await self._scan_local_models()
        llm_models = [m for m in models if m["type"] == "llm"]

        if not llm_models:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç LLM-–º–æ–¥–µ–ª–µ–π –≤ LM Studio –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.")
            return False

        chat_candidate = None

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: preferred model –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (LOCAL_PREFERRED_MODEL)
        # ‚Äî —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–µ—Ñ–æ–ª—Ç–∞ –Ω–∞ qwen 7b
        if self.local_preferred_model:
            matching = [
                m["id"] for m in llm_models
                if self.local_preferred_model.lower() in m["id"].lower()
            ]
            if matching:
                chat_candidate = matching[0]
                logger.info(f"‚≠ê –í—ã–±—Ä–∞–Ω–∞ preferred –º–æ–¥–µ–ª—å: {chat_candidate}")

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: instruct/chat –º–æ–¥–µ–ª–∏ (–æ–±—ã—á–Ω–æ –ª—É—á—à–µ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞)
        if not chat_candidate:
            for m in llm_models:
                mid = m["id"].lower()
                if "instruct" in mid or "chat" in mid:
                    chat_candidate = m["id"]
                    logger.info(f"üîÑ –í—ã–±—Ä–∞–Ω–∞ instruct/chat –º–æ–¥–µ–ª—å: {chat_candidate}")
                    break

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –ª—é–±–∞—è LLM (fallback)
        if not chat_candidate:
            chat_candidate = llm_models[0]["id"]
            logger.info(f"üîÑ Fallback –Ω–∞ –ø–µ—Ä–≤—É—é LLM: {chat_candidate}")

        return await self._smart_load(chat_candidate, reason="ensure_chat")

    async def _maybe_autoload_local_model(self, reason: str = "") -> bool:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, –µ—Å–ª–∏:
        - —Ä–µ–∂–∏–º –Ω–µ force_cloud,
        - LM Studio –¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ loaded-–º–æ–¥–µ–ª–∏ –Ω–µ—Ç.
        """
        if self.force_mode == "force_cloud":
            return False
        if self.local_engine != "lm-studio":
            return False
        if self.last_local_load_error != "no_model_loaded":
            return False

        now = time.time()
        if (now - float(self._last_local_autoload_ts)) < float(self.local_autoload_cooldown_sec):
            return False

        self._last_local_autoload_ts = now
        logger.info(
            "Auto-load –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: —Å—Ç–∞—Ä—Ç",
            reason=reason or "unspecified",
            cooldown_sec=self.local_autoload_cooldown_sec,
        )
        loaded = await self._ensure_chat_model_loaded()
        # –û–±–Ω–æ–≤–ª—è–µ–º health –ø–æ—Å–ª–µ –ø–æ–ø—ã—Ç–∫–∏, —á—Ç–æ–±—ã is_local_available/active_local_model –±—ã–ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã.
        await self.check_local_health(force=True)
        logger.info(
            "Auto-load –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: –∑–∞–≤–µ—Ä—à–µ–Ω",
            loaded=bool(loaded),
            active_local_model=self.active_local_model,
            local_available=self.is_local_available,
        )
        return bool(loaded)

    async def list_local_models(self) -> List[str]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (lms ls) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID."""
        models = await self._scan_local_models()
        ids: list[str] = []
        for entry in models:
            identifier = self._extract_model_id(entry)
            if identifier:
                ids.append(identifier)
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –≤ —É—Å—Ç–æ–π—á–∏–≤–æ–º –ø–æ—Ä—è–¥–∫–µ
        return sorted(set(ids))

    @staticmethod
    def _format_size_gb(size_bytes: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ –≥–∏–≥–∞–±–∞–π—Ç—ã –¥–ª—è UI-–∫–æ–º–∞–Ω–¥."""
        try:
            value = float(size_bytes)
        except Exception:
            return "n/a"
        if value <= 0:
            return "n/a"
        return f"{round(value / (1024 ** 3), 2)} GB"

    async def list_local_models_verbose(self) -> List[Dict[str, Any]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:
        id, loaded, type, size_bytes, size_human.
        """
        raw = await self._scan_local_models()
        result: list[Dict[str, Any]] = []
        seen: set[str] = set()
        for entry in raw:
            model_id = self._extract_model_id(entry) if isinstance(entry, dict) else None
            if not model_id:
                continue
            if model_id in seen:
                continue
            seen.add(model_id)
            size_bytes = 0
            if isinstance(entry, dict):
                try:
                    size_bytes = int(entry.get("size_bytes") or entry.get("size_on_disk") or entry.get("size") or 0)
                except Exception:
                    size_bytes = 0
            result.append(
                {
                    "id": model_id,
                    "loaded": bool(entry.get("loaded", False)) if isinstance(entry, dict) else False,
                    "type": str(entry.get("type", "llm")) if isinstance(entry, dict) else "llm",
                    "size_bytes": int(size_bytes),
                    "size_human": self._format_size_gb(size_bytes),
                }
            )
        return sorted(result, key=lambda item: item["id"])

    def _suggest_local_model_ids(self, requested: str, available_ids: List[str], limit: int = 5) -> List[str]:
        """–ü–æ–¥–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ model_id –ø–æ —Å—Ç—Ä–æ–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        if not requested or not available_ids:
            return []

        requested_lower = requested.lower()
        substring_matches = [model_id for model_id in available_ids if requested_lower in model_id.lower()]
        if substring_matches:
            return substring_matches[:limit]

        close = difflib.get_close_matches(requested, available_ids, n=limit, cutoff=0.35)
        if close:
            return close

        return available_ids[:limit]

    def _resolve_local_model_id(self, requested: str, available_ids: List[str]) -> Optional[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π model_id, –µ—Å–ª–∏ –æ–Ω –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–∫–∞–Ω–µ LM Studio."""
        if not requested:
            return None

        requested_clean = requested.strip()
        if not requested_clean:
            return None

        if requested_clean in available_ids:
            return requested_clean

        lowered = requested_clean.lower()
        for model_id in available_ids:
            if model_id.lower() == lowered:
                return model_id

        # –î–æ–ø—É—Å–∫–∞–µ–º –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å—É—Ñ—Ñ–∏–∫—Å—É/–ø—Ä–µ—Ñ–∏–∫—Å—É.
        fuzzy_matches = [
            model_id for model_id in available_ids
            if model_id.lower().endswith(lowered) or model_id.lower().startswith(lowered)
        ]
        if len(fuzzy_matches) == 1:
            return fuzzy_matches[0]

        return None

    def _build_lms_load_command(self, lms_path: str, model_name: str) -> List[str]:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é —Å —Ç–µ–∫—É—â–∏–º lms CLI –∫–æ–º–∞–Ω–¥—É –∑–∞–≥—Ä—É–∑–∫–∏.
        –î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è --gpu: off|max|—á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 1.
        """
        cmd = [lms_path, "load", model_name, "-y"]
        gpu = self.lms_gpu_offload
        if gpu in {"off", "max"}:
            cmd.extend(["--gpu", gpu])
            return cmd

        if gpu:
            try:
                gpu_value = float(gpu)
                if 0.0 <= gpu_value <= 1.0:
                    cmd.extend(["--gpu", str(gpu_value)])
                else:
                    logger.warning("LM_STUDIO_GPU_OFFLOAD –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 0..1, –æ–ø—Ü–∏—è –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è", value=gpu)
            except ValueError:
                logger.warning("LM_STUDIO_GPU_OFFLOAD –∏–º–µ–µ—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –æ–ø—Ü–∏—è –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è", value=gpu)

        return cmd

    async def load_local_model(self, model_name: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ LM Studio —á–µ—Ä–µ–∑ REST API (0.3.x).
        """
        requested_model = (model_name or "").strip()
        self.last_local_load_error = None
        self.last_local_load_error_human = None
        if not requested_model:
            self.last_local_load_error = "model_id_empty"
            self.last_local_load_error_human = "‚ö†Ô∏è –ù–µ —É–∫–∞–∑–∞–Ω model_id –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ LM Studio."
            logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π model_id –¥–ª—è load_local_model.")
            return False

        # Dry precheck: –ø—Ä–æ–≤–µ—Ä—è–µ–º model_id –ø–æ /api/v1/models –¥–æ POST /load.
        available_ids = await self.list_local_models()
        resolved_model = self._resolve_local_model_id(requested_model, available_ids)
        if not resolved_model:
            suggestions = self._suggest_local_model_ids(requested_model, available_ids)
            self.last_local_load_error = f"model_not_found_precheck:{requested_model}"
            self.last_local_load_error_human = (
                f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å `{requested_model}` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ LM Studio scan. "
                "–ü—Ä–æ–≤–µ—Ä—å —Ç–æ—á–Ω—ã–π id —á–µ—Ä–µ–∑ !model scan."
            )
            logger.warning(
                "‚ö†Ô∏è Dry precheck: model_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ LM Studio scan",
                requested=requested_model,
                suggestions=suggestions,
                scanned_count=len(available_ids),
            )
            return False

        base = self._lm_studio_api_root()
        # –í 0.3.x —ç–Ω–¥–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∑–∫–∏: POST /api/v1/models/load
        url = f"{base}/api/v1/models/load"
        last_rest_error_text = ""

        try:
            logger.info(f"üöÄ Loading model via REST API: {resolved_model}")
            # LM Studio 0.3.x: POST /api/v1/models/load
            # –ü—Ä–∏–Ω–∏–º–∞–µ—Ç {"model": "id"} ‚Äî –±–µ–∑ gpu_offload (–≤—ã–∑—ã–≤–∞–µ—Ç unrecognized_keys)
            timeout = aiohttp.ClientTimeout(total=120)  # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–æ–π
            async with aiohttp.ClientSession(timeout=timeout) as session:
                payload = {
                    "model": resolved_model
                }
                async with session.post(url, json=payload) as resp:
                    if resp.status == 200:
                        logger.info(f"‚úÖ REST API Load Success: {resolved_model}")
                        self.active_local_model = resolved_model
                        self.is_local_available = True
                        self.last_local_load_error = None
                        self.last_local_load_error_human = None
                        return True
                    text = await resp.text()
                    last_rest_error_text = text
                    
                    # [HOTFIX v11.4.2] –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ñ–∞—Ç–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–∏ LM Studio
                    if "Utility process" in text or "snapshot of system resources failed" in text:
                        self.last_local_load_error = "lms_resource_error"
                        logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê LM STUDIO: –°–±–æ–π —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ (Utility process). –¢–†–ï–ë–£–ï–¢–°–Ø –ü–ï–†–ï–ó–ê–ì–†–£–ó–ö–ê LM STUDIO.")
                        # –ß—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–≤–∏–¥–µ–ª —ç—Ç–æ —á–µ—Ä–µ–∑ !model status
                        self.last_local_load_error_human = (
                            "‚ö†Ô∏è LM Studio: –æ—à–∏–±–∫–∞ Utility process / snapshot resources. "
                            "–ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ LM Studio –∏ –ø–æ–≤—Ç–æ—Ä–∏ –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏."
                        )
                    else:
                        self.last_local_load_error = f"rest_load_failed:{resp.status}:{text[:220]}"
                        self.last_local_load_error_human = (
                            f"‚ö†Ô∏è LM Studio load failed (HTTP {resp.status}). "
                            "–ü—Ä–æ–≤–µ—Ä—å –ª–æ–≥–∏ LM Studio –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å model_id."
                        )
                    
                    suggestions = self._suggest_local_model_ids(requested_model, available_ids)
                    logger.warning(
                        "‚ö†Ô∏è REST API Load failed",
                        status=resp.status,
                        requested=requested_model,
                        resolved=resolved_model,
                        details=text[:1200],
                        suggestions=suggestions,
                    )
                    lowered = text.lower()
                    if "model_not_found" in lowered or "not found" in lowered:
                        logger.warning(
                            "‚ùó LM Studio –≤–µ—Ä–Ω—É–ª model_not_found. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ—á–Ω—ã–π model_id –∏–∑ `!model scan`.",
                            requested=requested_model,
                            suggestions=suggestions,
                        )
        except Exception as e:
            self.last_local_load_error = f"rest_load_exception:{e}"
            self.last_local_load_error_human = "‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio –≤–æ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏."
            logger.error(f"‚ùå REST API Load Exception: {e}")

        # Fallback to CLI for backwards compatibility
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if os.path.exists(lms_path):
            try:
                cmd = self._build_lms_load_command(lms_path, resolved_model)
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                await proc.communicate()
                if proc.returncode == 0:
                    self.active_local_model = resolved_model
                    self.is_local_available = True
                    self.last_local_load_error = None
                    self.last_local_load_error_human = None
                    logger.info("‚úÖ CLI fallback load success", command=" ".join(cmd), model=resolved_model)
                    return True
                self.last_local_load_error = f"cli_load_failed:{proc.returncode}"
                self.last_local_load_error_human = (
                    f"‚ö†Ô∏è CLI fallback –∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∫–æ–¥–æ–º {proc.returncode}."
                )
                logger.warning(
                    "‚ö†Ô∏è CLI fallback load failed",
                    command=" ".join(cmd),
                    returncode=proc.returncode,
                    requested=requested_model,
                    resolved=resolved_model,
                    rest_error=last_rest_error_text[:300] if last_rest_error_text else "",
                )
            except Exception as exc:
                self.last_local_load_error = f"cli_load_exception:{exc}"
                self.last_local_load_error_human = "‚ö†Ô∏è –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è CLI fallback –∑–∞–≥—Ä—É–∑–∫–∏ LM Studio."
                logger.warning("‚ö†Ô∏è CLI fallback load exception", error=str(exc), requested=requested_model)
        else:
            if not self.last_local_load_error:
                self.last_local_load_error = "lms_cli_not_found"
            if not self.last_local_load_error_human:
                self.last_local_load_error_human = "‚ö†Ô∏è LM Studio CLI –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ ~/.lmstudio/bin/lms."
            logger.warning("‚ö†Ô∏è CLI fallback –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: ~/.lmstudio/bin/lms –Ω–µ –Ω–∞–π–¥–µ–Ω.")

        return False

    async def unload_local_model(self, model_name: str = None) -> bool:
        """
        –í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ LM Studio —á–µ—Ä–µ–∑ REST API.
        """
        base = self._lm_studio_api_root()
        url = f"{base}/api/v1/models/unload"
        
        try:
            payload = {}
            if model_name:
                payload["model"] = model_name
            
            timeout = aiohttp.ClientTimeout(total=15)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, json=payload) as resp:
                    if resp.status == 200:
                        logger.info(f"‚úÖ REST API Unload Success")
                        if not model_name:
                            self.active_local_model = None
                        return True
        except Exception as e:
            logger.error(f"‚ùå REST API Unload failed: {e}")

        # Fallback to CLI
        lms_path = os.path.expanduser("~/.lmstudio/bin/lms")
        if os.path.exists(lms_path):
            try:
                cmd = [lms_path, "unload", "--all"] if not model_name else [lms_path, "unload", model_name]
                proc = await asyncio.create_subprocess_exec(*cmd)
                await proc.communicate()
                return proc.returncode == 0
            except Exception:
                pass
        return False

        # Legacy fallback removed

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Smart Memory Planner: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –∏ –∞–≤—Ç–æ-—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _touch_model_usage(self, model_id: str) -> None:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (LRU-—Ç—Ä–µ–∫–∏–Ω–≥).
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        """
        import time
        self._model_last_used[model_id] = time.time()

    async def _get_system_memory_gb(self) -> Dict[str, float]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–Ω–æ–π –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ macOS sysctl / vm_stat.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {"total": X, "used": Y, "free": Z} –≤ –≥–∏–≥–∞–±–∞–π—Ç–∞—Ö.
        """
        try:
            proc = await asyncio.create_subprocess_exec(
                "sysctl", "-n", "hw.memsize",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, _ = await proc.communicate()
            total_bytes = int(stdout.decode().strip())
            total_gb = total_bytes / (1024 ** 3)

            # vm_stat –¥–∞—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (–∫–∞–∂–¥–∞—è 16384 –±–∞–π—Ç –Ω–∞ ARM mac)
            proc2 = await asyncio.create_subprocess_exec(
                "vm_stat",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout2, _ = await proc2.communicate()
            vm_text = stdout2.decode()

            # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–æ–±–æ–¥–Ω—ã—Ö/inactive —Å—Ç—Ä–∞–Ω–∏—Ü
            import re
            page_size = 16384  # –¥–µ—Ñ–æ–ª—Ç –¥–ª—è Apple Silicon
            ps_match = re.search(r"page size of (\d+) bytes", vm_text)
            if ps_match:
                page_size = int(ps_match.group(1))

            free_pages = 0
            inactive_pages = 0
            for line in vm_text.split("\n"):
                if "Pages free" in line:
                    m = re.search(r"(\d+)", line.split(":")[1])
                    if m:
                        free_pages = int(m.group(1))
                elif "Pages inactive" in line:
                    m = re.search(r"(\d+)", line.split(":")[1])
                    if m:
                        inactive_pages = int(m.group(1))

            free_gb = (free_pages + inactive_pages) * page_size / (1024 ** 3)
            used_gb = total_gb - free_gb

            return {"total": round(total_gb, 2), "used": round(used_gb, 2), "free": round(free_gb, 2)}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—É—é –ø–∞–º—è—Ç—å: {e}")
            return {"total": self.max_ram_gb, "used": 0, "free": self.max_ram_gb}

    async def _get_loaded_models_memory(self) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ—Ü–µ–Ω–∫–æ–π –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è RAM.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LM Studio API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [{"id": "model-name", "size_gb": 4.3, "loaded": True, "last_used": timestamp}]
        """
        models = await self._scan_local_models()
        result = []
        for m in models:
            model_id = m.get("id", "unknown")
            # LM Studio –Ω–µ –≤—Å–µ–≥–¥–∞ –¥–∞—ë—Ç —Ç–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä, –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –∏–º–µ–Ω–∏
            size_gb = self._estimate_model_size_gb(model_id)
            last_used = self._model_last_used.get(model_id, 0)
            result.append({
                "id": model_id,
                "type": m.get("type", "unknown"),
                "loaded": m.get("loaded", False),
                "size_gb": size_gb,
                "last_used": last_used,
            })
        return result

    def _estimate_model_size_gb(self, model_name: str) -> float:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ GB –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö).
        –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: 1B –ø–∞—Ä–∞–º–µ—Ç—Ä ‚âà 0.5-1 GB –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ API –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä.
        """
        import re
        lowered = model_name.lower()
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∏–¥–∞ 7b, 13b, 70b –∏ —Ç.–¥.
        match = re.search(r"(\d+\.?\d*)b", lowered)
        if match:
            params_b = float(match.group(1))
            # MLX/GGUF –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è: ~0.6 GB –Ω–∞ –º–∏–ª–ª–∏–∞—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (4-bit)
            if "mlx" in lowered or "4bit" in lowered or "q4" in lowered:
                return round(params_b * 0.6, 1)
            # 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
            elif "8bit" in lowered or "q8" in lowered:
                return round(params_b * 1.0, 1)
            # FP16 (–ø–æ–ª–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)
            elif "fp16" in lowered or "f16" in lowered:
                return round(params_b * 2.0, 1)
            # –î–µ—Ñ–æ–ª—Ç (4-bit GGUF ‚Äî —Å–∞–º—ã–π —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–π)
            return round(params_b * 0.7, 1)

        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∏–º–µ–Ω–∏
        known_sizes = {
            "glm-4.6v": 5.0, "glm-4": 5.0,
            "phi-3": 2.5, "phi-4": 8.0,
            "llama-3.2-1b": 0.8, "llama-3.2-3b": 2.0,
        }
        for key, size in known_sizes.items():
            if key in lowered:
                return size

        # –°–æ–≤—Å–µ–º –Ω–µ –∑–Ω–∞–µ–º ‚Äî –¥–µ—Ñ–æ–ª—Ç 4 GB (—Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –¥–ª—è 7B –º–æ–¥–µ–ª–∏)
        return 4.0

    async def _can_fit_model(self, model_name: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ RAM –¥–ª—è LM Studio.
        """
        loaded = await self._get_loaded_models_memory()
        current_usage = sum(m["size_gb"] for m in loaded if m["loaded"])
        new_model_size = self._estimate_model_size_gb(model_name)
        projected = current_usage + new_model_size

        logger.info(
            "üß† Memory check",
            current_loaded_gb=round(current_usage, 1),
            new_model_gb=round(new_model_size, 1),
            projected_gb=round(projected, 1),
            limit_gb=self.lm_studio_max_ram_gb,
        )
        return projected <= self.lm_studio_max_ram_gb

    async def _evict_idle_models(self, needed_gb: float = 0) -> float:
        """
        –í—ã–≥—Ä—É–∂–∞–µ—Ç –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ LRU (Least Recently Used).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å–≤–æ–±–æ–∂–¥—ë–Ω–Ω—ã—Ö GB.

        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        1. –°–Ω–∞—á–∞–ª–∞ –≤—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ idle > AUTO_UNLOAD_IDLE_MIN
        2. –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî –≤—ã–≥—Ä—É–∂–∞–µ–º –ø–æ LRU (—Å–∞–º—ã–µ –¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ)
        3. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—ã–≥—Ä—É–∂–∞–µ–º preferred –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –æ–Ω–∞ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è
        """
        import time
        loaded = await self._get_loaded_models_memory()
        loaded_models = [m for m in loaded if m["loaded"] and m["type"] == "llm"]

        if len(loaded_models) <= 1:
            logger.info("üìå –¢–æ–ª—å–∫–æ 1 –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
            return 0.0

        freed_gb = 0.0
        idle_threshold = time.time() - (self.auto_unload_idle_min * 60)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ last_used (—Å–∞–º—ã–µ –¥–∞–≤–Ω–∏–µ ‚Äî –ø–µ—Ä–≤—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ –≤—ã–≥—Ä—É–∑–∫—É)
        candidates = sorted(loaded_models, key=lambda m: m["last_used"])

        for model in candidates:
            if freed_gb >= needed_gb and needed_gb > 0:
                break  # –•–≤–∞—Ç–∞–µ—Ç –º–µ—Å—Ç–∞

            model_id = model["id"]

            # –ó–∞—â–∏—Ç–∞: –Ω–µ –≤—ã–≥—Ä—É–∂–∞–µ–º preferred –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –æ–Ω–∞ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Å—Ç–∞–≤—à–∞—è—Å—è
            remaining = len(loaded_models) - 1
            if remaining <= 0:
                break
            if model_id == self.active_local_model and remaining <= 1:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º idle time (–∏–ª–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –º–µ—Å—Ç–æ)
            is_idle = model["last_used"] < idle_threshold or model["last_used"] == 0
            if is_idle or needed_gb > 0:
                reason = "idle" if is_idle else "memory_pressure"
                logger.info(f"‚ôªÔ∏è –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_id} (–ø—Ä–∏—á–∏–Ω–∞: {reason}, size: {model['size_gb']} GB)")
                success = await self.unload_local_model(model_id)
                if success:
                    freed_gb += model["size_gb"]
                    loaded_models = [m for m in loaded_models if m["id"] != model_id]
                    # –û–±–Ω–æ–≤–ª—è–µ–º active_local_model –µ—Å–ª–∏ –±—ã–ª–æ –≤—ã–≥—Ä—É–∂–µ–Ω–æ
                    if self.active_local_model == model_id:
                        self.active_local_model = None

        if freed_gb > 0:
            logger.info(f"üßπ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ {round(freed_gb, 1)} GB RAM (–º–æ–¥–µ–ª–∏: {len(candidates)} ‚Üí {len(loaded_models)})")
        return freed_gb

    async def _smart_load(self, model_name: str, reason: str = "chat") -> bool:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–∞–º—è—Ç–∏ –∏ LRU-–≤—ã–≥—Ä—É–∑–∫–æ–π.

        1. –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –ø—Ä–æ—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ–º LRU –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True
        2. –ï—Å–ª–∏ –ø–æ–º–µ—â–∞–µ—Ç—Å—è ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º
        3. –ï—Å–ª–∏ –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è ‚Äî –≤—ã–≥—Ä—É–∂–∞–µ–º idle –º–æ–¥–µ–ª–∏, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
        4. –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è ‚Äî –æ—à–∏–±–∫–∞

        Args:
            model_name: ID –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            reason: –ø—Ä–∏—á–∏–Ω–∞ (chat / coding / forced)
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ —É–∂–µ
        loaded = await self._get_loaded_models_memory()
        for m in loaded:
            if m["id"] == model_name and m["loaded"]:
                self._touch_model_usage(model_name)
                self.active_local_model = model_name
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –æ–±–Ω–æ–≤–ª—è–µ–º LRU (reason: {reason})")
                return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏
        if await self._can_fit_model(model_name):
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {model_name} (reason: {reason}), RAM –ø–æ–∑–≤–æ–ª—è–µ—Ç")
            success = await self.load_local_model(model_name)
            if success:
                self._touch_model_usage(model_name)
            return success

        # –ù–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è ‚Äî –ø—Ä–æ–±—É–µ–º –≤—ã–≥—Ä—É–∑–∏—Ç—å idle
        new_size = self._estimate_model_size_gb(model_name)
        current_usage = sum(m["size_gb"] for m in loaded if m["loaded"])
        needed_gb = (current_usage + new_size) - self.lm_studio_max_ram_gb + 0.5  # +0.5 GB –∑–∞–ø–∞—Å

        logger.warning(f"‚ö†Ô∏è –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç RAM –¥–ª—è {model_name} ({new_size} GB). –ù—É–∂–Ω–æ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å {round(needed_gb, 1)} GB")
        freed = await self._evict_idle_models(needed_gb)

        if freed >= needed_gb or await self._can_fit_model(model_name):
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {model_name} –ø–æ—Å–ª–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏")
            success = await self.load_local_model(model_name)
            if success:
                self._touch_model_usage(model_name)
            return success

        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM –¥–ª—è {model_name}")
        return False

    async def get_memory_status(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Å—Ç–∞—Ç—É—Å –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model memory.
        """
        sys_mem = await self._get_system_memory_gb()
        loaded = await self._get_loaded_models_memory()
        loaded_models = [m for m in loaded if m["loaded"]]
        model_usage = sum(m["size_gb"] for m in loaded_models)

        import time
        lines = [
            "üß† **Smart Memory Planner**",
            f"",
            f"üíª –°–∏—Å—Ç–µ–º–Ω–∞—è RAM: {sys_mem['used']}/{sys_mem['total']} GB (—Å–≤–æ–±–æ–¥–Ω–æ: {sys_mem['free']} GB)",
            f"ü§ñ –õ–∏–º–∏—Ç LM Studio: {round(model_usage, 1)}/{self.lm_studio_max_ram_gb} GB",
            f"‚è± –ê–≤—Ç–æ-–≤—ã–≥—Ä—É–∑–∫–∞ idle: {self.auto_unload_idle_min} –º–∏–Ω",
            f"",
            f"**–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:**",
        ]

        if not loaded_models:
            lines.append("  ‚îî‚îÄ (–Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö)")
        else:
            for m in loaded_models:
                last_used = self._model_last_used.get(m["id"], 0)
                if last_used > 0:
                    idle_min = int((time.time() - last_used) / 60)
                    idle_str = f"{idle_min} –º–∏–Ω –Ω–∞–∑–∞–¥"
                else:
                    idle_str = "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                active = " ‚≠ê" if m["id"] == self.active_local_model else ""
                lines.append(f"  ‚îî‚îÄ `{m['id']}` ‚Äî ~{m['size_gb']} GB (idle: {idle_str}){active}")

        return "\n".join(lines)

    async def _scan_cloud_models_via_openclaw_cli(self, all_catalog: bool = True) -> List[Dict[str, Any]]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç Cloud-–∫–∞—Ç–∞–ª–æ–≥ —á–µ—Ä–µ–∑ `openclaw models list`.

        –ü–æ—á–µ–º—É —Ç–∞–∫:
        - HTTP endpoint Gateway –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–±–æ—Ä–∫–∞—Ö –æ—Ç–¥–∞—ë—Ç SPA HTML –¥–ª—è `/v1/models`,
          –∏–∑-–∑–∞ —á–µ–≥–æ –ø—Ä—è–º–æ–π REST-—Å–∫–∞–Ω –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ.
        - CLI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞—Ç–∏–≤–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç OpenClaw –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–¥–∞—ë—Ç JSON-–∫–∞—Ç–∞–ª–æ–≥.
        """
        cmd = ["openclaw", "models", "list"]
        if all_catalog:
            cmd.append("--all")
        cmd.append("--json")

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=20)
        except asyncio.TimeoutError:
            return []
        except Exception:
            return []

        if proc.returncode != 0:
            err = (stderr or b"").decode("utf-8", errors="ignore").strip()
            if err:
                self.last_cloud_error = err
            return []

        raw = (stdout or b"").decode("utf-8", errors="ignore").strip()
        if not raw:
            return []

        try:
            payload = json.loads(raw)
        except Exception:
            return []

        items = payload.get("models", []) if isinstance(payload, dict) else []
        if not isinstance(items, list):
            return []
        return [item for item in items if isinstance(item, dict)]

    async def list_cloud_models(self) -> List[str]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç Cloud-–º–æ–¥–µ–ª–∏ (OpenClaw) —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º CLI-–∫–∞—Ç–∞–ª–æ–≥–∞."""
        if not self.openclaw_client:
            return ["–û—à–∏–±–∫–∞: OpenClaw –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"]

        try:
            # 1) –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å: OpenClaw CLI –∫–∞—Ç–∞–ª–æ–≥ (—É—Å—Ç–æ–π—á–∏–≤–µ–µ —á–µ–º HTTP /v1/models).
            cli_models = await self._scan_cloud_models_via_openclaw_cli(all_catalog=True)
            available: List[str] = []
            configured: List[str] = []

            for item in cli_models:
                model_id = str(item.get("key") or item.get("id") or "").strip()
                if not model_id:
                    continue
                # Local –º–æ–¥–µ–ª–∏ –Ω–µ –æ—Ç–Ω–æ—Å–∏–º –∫ cloud-—Å–ø–∏—Å–∫—É.
                if bool(item.get("local")):
                    continue
                if bool(item.get("missing")):
                    continue

                tags = item.get("tags", []) if isinstance(item.get("tags"), list) else []
                if bool(item.get("available")):
                    available.append(model_id)
                elif "configured" in tags or "default" in tags:
                    # –ï—Å–ª–∏ available –ø—É—Å—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª—é—á –Ω–µ–≤–∞–ª–∏–¥–µ–Ω), –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ö–æ—Ç—è –±—ã
                    # —Ä–µ–∞–ª—å–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ cloud-model_id.
                    configured.append(model_id)

            result = sorted(set(available))
            if not result and configured:
                result = sorted(set(configured))
            if result:
                self.last_cloud_error = None
                return result

            # 2) Fallback: –ø—Ä—è–º–æ–π HTTP get_models (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏).
            raw_models = await self.openclaw_client.get_models()
            models: List[str] = []
            for m in raw_models:
                if isinstance(m, dict) and "id" in m:
                    mid = str(m["id"]).strip()
                    if mid:
                        models.append(mid)
                elif isinstance(m, str):
                    mid = m.strip()
                    if mid:
                        models.append(mid)

            self.last_cloud_error = None
            return sorted(set(models))
        except Exception as e:
            err_msg = str(e)
            logger.error(f"Cloud scan error: {err_msg}")
            # –ï—Å–ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞, –ø–æ–º–µ—á–∞–µ–º soft cap
            self._mark_cloud_soft_cap_if_needed(err_msg)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model scan
            if self._is_cloud_billing_error(err_msg):
                return [f"‚ùå –û—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞ (Cloud): –û–ø–ª–∞—Ç–∏—Ç–µ —Å—á–µ—Ç –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç–µ API –∫–ª—é—á –≤ .env"]
            return [f"–û—à–∏–±–∫–∞ API: {err_msg}"]

    async def _call_local_llm(self, prompt: str, context: list = None, chat_type: str = "private", is_owner: bool = False) -> str:
        """
        –í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥ —Å –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –æ—Ç—Å–µ—á–∫–æ–π (Hard Truncation).
        –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∑–∞—â–∏—Ç—É –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –¥–∞–∂–µ –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç max_tokens.
        """
        try:
            system_msg = "You are a helpful assistant."
            if self.persona:
                system_msg = self.persona.get_current_prompt(chat_type, is_owner)

            if self.local_engine == 'lm-studio':
                base_url = self.lm_studio_url
            else:
                base_url = self.ollama_url.replace('/api', '/v1')

            if "/v1" not in base_url:
                base_url = base_url.rstrip("/") + "/v1"
            base_url = base_url.replace("/v1/v1", "/v1")

            messages = [{"role": "system", "content": system_msg}]
            if context:
                for idx, msg in enumerate(context):
                    if not isinstance(msg, dict): continue
                    mrole = self._normalize_chat_role(msg.get("role"))
                    content = msg.get("content") or msg.get("text") or msg.get("message")
                    if content: messages.append({"role": mrole, "content": str(content)})
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self.active_local_model or "local-model",
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 2048,
                "stop": ["<|im_end|>", "###", "</s>"],
                "stream": True,
                "include_reasoning": self.local_include_reasoning
            }

            headers = {"Content-Type": "application/json"}
            timeout = aiohttp.ClientTimeout(total=300, sock_read=60) # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
            
            full_content = []
            collected_chars = 0
            MAX_CHARS_LIMIT = 8000 # –ü—Ä–∏–º–µ—Ä–Ω–æ 2048 —Ç–æ–∫–µ–Ω–æ–≤
            
            start_t = time.time()
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(f"{base_url}/chat/completions", json=payload, headers=headers) as response:
                    if response.status != 200:
                        err = await response.text()
                        logger.error(f"Local LLM HTTP {response.status}: {err}")
                        return None

                    # –ß–∏—Ç–∞–µ–º —á–∞–Ω–∫–∏ –≤—Ä—É—á–Ω—É—é –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä—ã–≤–∞
                    async for line in response.content:
                        line = line.decode("utf-8").strip()
                        if not line or line == "data: [DONE]":
                            continue
                        
                        if line.startswith("data: "):
                            try:
                                chunk_data = json.loads(line[6:])
                                delta = chunk_data.get("choices", [{}])[0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    full_content.append(content)
                                    collected_chars += len(content)
                                    
                                    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–¢–°–ï–ß–ö–ê
                                    if collected_chars > MAX_CHARS_LIMIT:
                                        logger.warning(f"‚ö†Ô∏è HARD TRUNCATION: Model {self.active_local_model} exceeded {MAX_CHARS_LIMIT} chars. Breaking stream.")
                                        break
                            except Exception:
                                continue
            
            final_text = "".join(full_content)
            duration = time.time() - start_t
            
            cleaned = self._sanitize_model_text(final_text)
            if cleaned:
                logger.info("Local LLM (Stream+Truncate) success", 
                            duration=round(duration, 2), 
                            chars=len(cleaned),
                            truncated=collected_chars > MAX_CHARS_LIMIT)
                return cleaned
            return None

        except Exception as e:
            err_msg = str(e).lower()
            logger.error(f"Local LLM Stream Error: {e}")
            self._stats["local_failures"] += 1
            
            # –ï—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–∞—è –æ—à–∏–±–∫–∞ —Ä–∞–Ω—Ç–∞–π–º–∞ (Connection Refused),
            # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ—ë –∫–∞–∫ —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –≤ —Ä–æ—É—Ç–µ—Ä–µ
            if "connection refused" in err_msg or "failed to connect" in err_msg:
                return f"Error: Local engine connection refused ({e})"
            return None

    async def route_query(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative', 'moderation', 'security', 'infra', 'review'] = 'chat',
                          context: list = None,
                          chat_type: str = "private",
                          is_owner: bool = False,
                          use_rag: bool = True,
                          preferred_model: Optional[str] = None,
                          confirm_expensive: bool = False,
                          skip_swarm: bool = False):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å Auto-Fallback, RAG –∏ policy-—Ä–æ—É—Ç–∏–Ω–≥–æ–º.
        [R12] Resilience Hardening: single fallback limit + explicit telemetry.
        """
        fallback_happened = False
        
        profile = self.classify_task_profile(prompt, task_type)
        recommendation = self._get_profile_recommendation(profile)
        is_critical = recommendation["critical"]
        prefer_pro_for_owner_private = self._should_use_pro_for_owner_private(
            prompt=prompt,
            chat_type=chat_type,
            is_owner=is_owner,
        )

        # 0. RAG Lookup
        if use_rag and self.rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.1. Tool Orchestration
        if self.tools and not skip_swarm:
            # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏—é —á–µ—Ä–µ–∑ skip_swarm=True –≤ Swarm
            tool_data = await self.tools.execute_tool_chain(prompt, skip_swarm=True)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # Smart Memory Planner
        if self.force_mode != "force_cloud" and self.is_local_available:
            preferred = preferred_model or self.local_preferred_model
            if task_type == "coding" and self.local_coding_model:
                preferred = self.local_coding_model
            if preferred:
                await self._smart_load(preferred, reason=task_type)

        await self.check_local_health()
        if not self.is_local_available:
            await self._maybe_autoload_local_model(reason=f"route_query:{task_type}")

        async def _run_local(route_reason: str = "local_primary", route_detail: str = "") -> Any:
            if not self.is_local_available:
                return "unavailable", None
            async with self._acquire_local_slot(self.active_local_model):
                logger.info(
                    "Routing to LOCAL",
                    model=self.active_local_model,
                    profile=profile,
                    reason=route_reason
                )
                local_response = await self._call_local_llm(prompt, context, chat_type, is_owner)
                
                if self._is_runtime_error_message(local_response):
                    logger.warning("Local LLM Runtime Error", model=self.active_local_model, error=local_response)
                    return "runtime_error", local_response

                if local_response and local_response.strip():
                    # Guardrail: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –Ω–µ-–ø—É—Å—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                    if len(local_response.strip()) < 1 and not skip_swarm:
                        return "empty_output", "Local response was too short"
                        
                    self._touch_model_usage(self.active_local_model or "local-model")
                    self._stats["local_calls"] += 1
                    local_model = self.active_local_model or "local-model"
                    self._remember_model_choice(profile, local_model, "local")
                    self._update_usage_report(profile, local_model, "local")
                    self._remember_last_route(
                        profile=profile,
                        task_type=task_type,
                        channel="local",
                        model_name=local_model,
                        prompt=prompt,
                        route_reason=route_reason,
                        route_detail=route_detail,
                        force_mode=self.force_mode,
                    )
                    return "ok", local_response
                return "empty_output", None

        async def _run_cloud(route_reason: str = "cloud_primary", route_detail: str = ""):
            if self.require_confirm_expensive and is_critical and not confirm_expensive:
                return "confirm_needed", "‚ö†Ô∏è –î–ª—è –∫—Ä–∏—Ç–∏—á–Ω–æ–π –∑–∞–¥–∞—á–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ—Ä–æ–≥–æ–≥–æ –æ–±–ª–∞—á–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞."
            
            cloud_preferred = preferred_model or recommendation.get("model")
            if self.cost_engine:
                cloud_preferred = self.cost_engine.get_recommended_model(profile, cloud_preferred)

            if prefer_pro_for_owner_private:
                cloud_preferred = self.models.get("pro", cloud_preferred)

            force_cloud_mode = self.force_mode == "force_cloud"
            deadline = (
                time.monotonic() + float(self.cloud_fail_fast_budget_seconds)
                if force_cloud_mode
                else None
            )

            for i, candidate in enumerate(
                self._build_cloud_candidates(
                    task_type=task_type,
                    profile=profile,
                    preferred_model=cloud_preferred,
                    chat_type=chat_type,
                    is_owner=is_owner,
                    prompt=prompt,
                )
            ):
                provider = candidate.split("/", 1)[0]
                preflight_error = self._check_cloud_preflight(provider)
                if preflight_error:
                    logger.warning("Cloud preflight rejected candidate", candidate=candidate, error=preflight_error)
                    if force_cloud_mode:
                        return "preflight_blocked", f"‚ùå {preflight_error}"
                    continue

                if "-exp" in candidate and "gemini-2.0" in candidate:
                    candidate = candidate.replace("-exp", "")
                if deadline is not None and time.monotonic() >= deadline:
                    self.last_cloud_error = (
                        "Cloud fail-fast budget exceeded "
                        f"({self.cloud_fail_fast_budget_seconds}s)"
                    )
                    logger.warning(
                        "Cloud routing stopped by fail-fast budget",
                        budget_seconds=self.cloud_fail_fast_budget_seconds,
                        attempt=i + 1,
                        reason=route_reason,
                    )
                    break
                    
                logger.info("Routing to CLOUD", model=candidate, profile=profile, reason=route_reason)
                max_retries_cloud = 0 if force_cloud_mode else (1 if i == 0 else 0)
                response = await self._call_gemini(prompt, candidate, context, chat_type, is_owner, max_retries=max_retries_cloud)
                
                if self._is_runtime_error_message(response):
                    logger.warning("Cloud candidate %s failed", candidate, error=response)
                    self._mark_cloud_soft_cap_if_needed(str(response))
                    self.last_cloud_error = str(response)
                    self.last_cloud_model = candidate
                    if self._is_fatal_cloud_auth_error(response):
                        logger.error(
                            "Cloud routing aborted: fatal auth/billing error",
                            model=candidate,
                            error=str(response)[:280],
                        )
                        break
                    continue
                
                # Cloud Success Guardrail
                if not response or len(response.strip()) < 2:
                    logger.warning("Cloud candidate %s returned empty/junk", candidate)
                    continue

                self.last_cloud_error = None
                self.last_cloud_model = candidate
                return candidate, response
            
            return "all_candidates_failed", self.last_cloud_error or "Cloud API failure"

        # --- Execution starts here ---

        if self.force_mode == "force_local":
            l_status, l_resp = await _run_local(route_reason="force_local", route_detail="forced by mode")
            if l_status == "ok": return l_resp
            return f"‚ùå –û—à–∏–±–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Local ({l_status}): {l_resp}" if is_owner else "‚ùå –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–µ–π—á–∞—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."

        if self.force_mode == "force_cloud":
            c_res = await _run_cloud(route_reason="force_cloud", route_detail="forced by mode")
            if isinstance(c_res, tuple) and c_res[0] != "all_candidates_failed" and c_res[0] != "preflight_blocked":
                # Finalize cloud normally
                candidate, response = c_res
                self._remember_model_choice(profile, candidate, "cloud")
                self._update_usage_report(profile, candidate, "cloud")
                self._remember_last_route(profile=profile, task_type=task_type, channel="cloud",
                                          model_name=candidate, prompt=prompt, route_reason="force_cloud")
                return response
            err_msg = self.last_cloud_error or 'Unknown cloud failure'
            if isinstance(c_res, tuple) and c_res[0] == "preflight_blocked":
                err_msg = c_res[1] # Use the preflight error message directly
            summary = self._summarize_cloud_error_for_user(err_msg)
            return (
                f"‚ùå –û—à–∏–±–∫–∞ Cloud (force_cloud): {summary}."
                if is_owner
                else "‚ùå –û–±–ª–∞—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
            )

        # Auto Mode Strategy
        force_local_due_cost = self.cloud_soft_cap_reached and not is_critical
        prefer_cloud = is_critical or task_type == "reasoning"
        if force_local_due_cost: prefer_cloud = False

        local_status = "skipped"
        local_response = None

        # 1. Try Local First if appropriate
        if not prefer_cloud and self.is_local_available:
            local_status, local_response = await _run_local(route_reason="local_primary", route_detail="auto mode")
            if local_status == "ok":
                return local_response

        # 2. Fallback to Cloud or Try Cloud Primary
        if local_status != "ok":
            fallback_happened = (local_status != "skipped")
            c_reason = "cloud_primary" if local_status == "skipped" else "local_fallback_cloud"
            c_detail = f"local_status={local_status}"
            
            cloud_result = await _run_cloud(route_reason=c_reason, route_detail=c_detail)
            
            if isinstance(cloud_result, tuple) and cloud_result[0] != "all_candidates_failed" and cloud_result[0] != "preflight_blocked":
                candidate, c_response = cloud_result
                # Finalize
                self._remember_model_choice(profile, candidate, "cloud")
                self._update_usage_report(profile, candidate, "cloud")
                self._remember_last_route(
                    profile=profile, task_type=task_type, channel="cloud",
                    model_name=candidate, prompt=prompt,
                    route_reason=c_reason, route_detail=c_detail
                )
                return c_response
            
            # 3. Last resort: Local again ONLY if we haven't successfully run it and cloud failed
            if self.is_local_available and not fallback_happened:
                # This would be cloud->local fallback (not requested to be limited, but let's be careful)
                l_status, l_resp = await _run_local(route_reason="cloud_fallback_local", route_detail="cloud failed")
                if l_status == "ok":
                    return l_resp

        err_msg = self.last_cloud_error or "–í—Å–µ –∫–∞–Ω–∞–ª—ã (Local/Cloud) –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –≤–µ—Ä–Ω—É–ª–∏ –æ—à–∏–±–∫—É."
        summary = self._summarize_cloud_error_for_user(err_msg)
        return (
            f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏: {summary}."
            if is_owner
            else "‚ùå –í –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å –∏–∑-–∑–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–π –æ—à–∏–±–∫–∏. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
        )

    async def route_stream(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning'] = 'chat',
                          context: list = None,
                          chat_type: str = "private",
                          is_owner: bool = False,
                          preferred_model: Optional[str] = None,
                          confirm_expensive: bool = False) -> AsyncGenerator[str, None]:
        """
        [PHASE 17.8] –ü–æ—Ç–æ–∫–æ–≤–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π local stream –∏ cloud fallback.
        """
        await self.check_local_health()
        if not self.is_local_available:
            await self._maybe_autoload_local_model(reason=f"route_stream:{task_type}")
        profile = self.classify_task_profile(prompt, task_type)
        recommendation = self._get_profile_recommendation(profile)
        force_cloud_mode = self.force_mode == "force_cloud"
        prefer_pro_for_owner_private = self._should_use_pro_for_owner_private(
            prompt=prompt,
            chat_type=chat_type,
            is_owner=is_owner,
        )

        async def _stream_cloud_fallback(failure_reason: str, failure_detail: str) -> AsyncGenerator[str, None]:
            """
            Fallback –ø—Ä–∏ —Å–±–æ–µ local stream.
            –í–∞–∂–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç: reasoning –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–∏—á–∏–Ω—ã –Ω–µ –ø—É–±–ª–∏–∫—É–µ–º –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç–≤–µ—Ç.
            """
            async def _try_local_recovery_without_reasoning() -> Optional[str]:
                """
                –ê–≤–∞—Ä–∏–π–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π recovery:
                –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å –±–µ–∑ reasoning, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å cloud-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏,
                –µ—Å–ª–∏ LM Studio –¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ stream-—Ñ–∞–∑–∞ —Å–æ—Ä–≤–∞–ª–∞—Å—å guardrail-–¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–º.
                """
                prev_reasoning_flag = self.local_include_reasoning
                try:
                    self.local_include_reasoning = False
                    recovered = await self._call_local_llm(
                        prompt=prompt,
                        context=context,
                        chat_type=chat_type,
                        is_owner=is_owner,
                    )
                    cleaned = self._sanitize_model_text(recovered or "")
                    return cleaned or None
                except Exception as local_exc:
                    logger.warning("Local recovery (reasoning-off) failed", error=str(local_exc))
                    return None
                finally:
                    self.local_include_reasoning = prev_reasoning_flag

            # –í force_cloud –ª–æ–∫–∞–ª—å–Ω—ã–π recovery –∑–∞–ø—Ä–µ—â—ë–Ω –ø–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É.
            allow_local_recovery = not force_cloud_mode

            if allow_local_recovery and failure_reason in {"reasoning_loop", "reasoning_limit", "stream_timeout"}:
                recovered_local = await _try_local_recovery_without_reasoning()
                if recovered_local:
                    logger.info(
                        "Local stream recovery succeeded without reasoning",
                        reason=failure_reason,
                        model=self.active_local_model,
                    )
                    yield recovered_local
                    return

            if not self.local_stream_fallback_to_cloud:
                logger.warning(
                    "Local stream failed, cloud fallback disabled by config",
                    reason=failure_reason,
                    detail=failure_detail,
                )
                yield (
                    f"‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω—ã–π —Å—Ç—Ä–∏–º –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ({failure_reason}). "
                    "Cloud fallback –æ—Ç–∫–ª—é—á—ë–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."
                )
                return

            logger.warning(
                "Local stream failed, switching to cloud fallback",
                reason=failure_reason,
                detail=failure_detail,
                profile=profile,
                model=self.active_local_model,
            )

            cloud_preferred = preferred_model or recommendation.get("model")
            if prefer_pro_for_owner_private:
                cloud_preferred = self.models.get("pro", cloud_preferred)
            deadline = (
                time.monotonic() + float(self.cloud_fail_fast_budget_seconds)
                if failure_reason == "force_cloud"
                else None
            )
            for candidate in self._build_cloud_candidates(
                task_type=task_type,
                profile=profile,
                preferred_model=cloud_preferred,
                chat_type=chat_type,
                is_owner=is_owner,
                prompt=prompt,
            ):
                provider = candidate.split("/", 1)[0]
                preflight_error = self._check_cloud_preflight(provider)
                if preflight_error:
                    logger.warning("Cloud stream preflight rejected candidate", candidate=candidate, error=preflight_error)
                    if force_cloud_mode:
                        yield f"‚ùå {preflight_error}"
                        return
                    continue

                if "-exp" in candidate and "gemini-2.0" in candidate:
                    candidate = candidate.replace("-exp", "")
                if deadline is not None and time.monotonic() >= deadline:
                    self.last_cloud_error = (
                        "Cloud stream fail-fast budget exceeded "
                        f"({self.cloud_fail_fast_budget_seconds}s)"
                    )
                    logger.warning(
                        "Cloud stream fallback stopped by fail-fast budget",
                        budget_seconds=self.cloud_fail_fast_budget_seconds,
                    )
                    break

                logger.info("Routing to CLOUD (Stream Fallback)", model=candidate, profile=profile, reason=failure_reason)
                retries = 0 if failure_reason == "force_cloud" else 1
                response = await self._call_gemini(
                    prompt,
                    candidate,
                    context,
                    chat_type,
                    is_owner,
                    max_retries=retries,
                )
                
                # [R12] –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ—à–∏–±–æ–∫
                if self._is_runtime_error_message(response):
                    err_msg = str(response or "Cloud error")
                    self.last_cloud_error = err_msg
                    self.last_cloud_model = candidate
                    self._mark_cloud_soft_cap_if_needed(err_msg)
                    logger.warning("Cloud fallback candidate failed", model=candidate, error=err_msg[:200])
                    if self._is_fatal_cloud_auth_error(err_msg):
                        logger.error(
                            "Cloud stream fallback aborted: fatal auth/billing error",
                            model=candidate,
                            error=err_msg[:280],
                        )
                        break
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NOT_FOUND (–º–∏—Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —à–ª—é–∑–∞)
                    lowered = err_msg.lower()
                    if "not found" in lowered or "not_found" in lowered:
                         logger.error("OpenClaw provider model mapping error", candidate=candidate)
                         if allow_local_recovery:
                             recovered_local = await _try_local_recovery_without_reasoning()
                             if recovered_local:
                                 yield recovered_local
                                 return
                    continue

                if not response or len(response.strip()) < 2:
                    logger.warning("Cloud fallback candidate %s returned empty/junk", candidate)
                    continue

                cleaned = self._sanitize_model_text(response)
                if not cleaned:
                    continue

                self.last_cloud_error = None
                self.last_cloud_model = candidate
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é
                route_reason = "local_stream_failed_cloud_fallback" if failure_reason != "force_cloud" else "force_cloud"
                route_detail = f"{failure_reason}: {failure_detail}".strip()[:240]
                
                self._remember_model_choice(profile, candidate, "cloud")
                self._update_usage_report(profile, candidate, "cloud")
                self._remember_last_route(
                    profile=profile, task_type=task_type, channel="cloud", model_name=candidate,
                    prompt=prompt, route_reason=route_reason, route_detail=route_detail, force_mode=self.force_mode
                )
                self._remember_last_stream_route(
                    profile=profile, task_type=task_type, channel="cloud", model_name=candidate,
                    prompt=prompt, route_reason=route_reason, route_detail=route_detail, force_mode=self.force_mode
                )
                yield cleaned
                return

            last_error_text = str(self.last_cloud_error or "").strip()
            err_msg = last_error_text or "–æ–±–ª–∞—á–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
            summary = self._summarize_cloud_error_for_user(err_msg)
            msg = (
                f"‚ùå –û—à–∏–±–∫–∞ Cloud ({failure_reason}): {summary}."
                if is_owner
                else "‚ùå –û–±–ª–∞—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
            )
            yield msg
            return

        # –ñ—ë—Å—Ç–∫–∏–π cloud-only —Ä–µ–∂–∏–º: –ª–æ–∫–∞–ª—å–Ω—ã–π —Å—Ç—Ä–∏–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.
        if force_cloud_mode:
            async for chunk in _stream_cloud_fallback(
                failure_reason="force_cloud",
                failure_detail="local stream bypassed by force_cloud mode",
            ):
                yield chunk
            return

        if not self.is_local_available:
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π route_query –µ—Å–ª–∏ —Å—Ç—Ä–∏–º–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ–±–ª–∞–∫–∞ –≤ –¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            res = await self.route_query(
                prompt=prompt,
                task_type=task_type,
                context=context,
                chat_type=chat_type,
                is_owner=is_owner,
                preferred_model=preferred_model,
                confirm_expensive=confirm_expensive,
            )
            yield res
            return

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        system_msg = "You are a helpful assistant."
        if hasattr(self, "persona") and self.persona:
            system_msg = self.persona.get_current_prompt(chat_type, is_owner)

        # –°–±–æ—Ä–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        messages = [{"role": "system", "content": system_msg}]
        if context:
            from src.core.context_manager import ContextKeeper
            for msg in context:
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–æ–ª–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—à–∏–±–æ–∫ —Ç–∏–ø–∞ 'vision_analysis' –≤ LM Studio
                role = ContextKeeper._normalize_role(msg.get("role"))
                content = msg.get("text") or msg.get("content") or ""
                if content:
                    messages.append({"role": role, "content": content})
        messages.append({"role": "user", "content": prompt})

        payload = {
            "model": self.active_local_model or "local-model",
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 2048,
            "include_reasoning": self.local_include_reasoning,
            "stream": True,
            "stop": ["<|endoftext|>", "<|user|>", "<|observation|>", "Observation:", "User:", "###", "---"],
            "presence_penalty": 0.1,
            "frequency_penalty": 0.1,
            # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–æ–ª—è –∫–ª–∏–µ–Ω—Ç–∞ —Å—Ç—Ä–∏–º–∞ (–Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –≤ LM Studio).
            "_krab_max_chars": 4000,
            "_krab_max_reasoning_chars": self.local_reasoning_max_chars,
            "_krab_total_timeout_seconds": self.local_stream_total_timeout_seconds,
            "_krab_sock_read_timeout_seconds": self.local_stream_sock_read_timeout_seconds,
        }

        emitted_chunks = 0
        try:
            async with self._acquire_local_slot(self.active_local_model):
                async for chunk in self.stream_client.stream_chat(payload):
                    emitted_chunks += 1
                    yield chunk
                if emitted_chunks > 0:
                    local_model = self.active_local_model or payload.get("model") or "local-model"
                    self._remember_last_stream_route(
                        profile=profile,
                        task_type=task_type,
                        channel="local",
                        model_name=str(local_model),
                        prompt=prompt,
                        route_reason="local_stream_primary",
                        route_detail="stream completed on local model",
                        force_mode=self.force_mode,
                    )
                return
        except StreamFailure as e:
            logger.warning(
                "Local stream guardrail/failure triggered",
                reason=e.reason,
                detail=e.technical_message,
                emitted_chunks=emitted_chunks,
                model=self.active_local_model,
            )
            async for cloud_chunk in _stream_cloud_fallback(e.reason, e.technical_message):
                yield cloud_chunk
            return
        except Exception as e:
            logger.error("Streaming error in route_stream", error=f"{type(e).__name__}: {e}")
            async for cloud_chunk in _stream_cloud_fallback("connection_error", f"{type(e).__name__}: {e}"):
                yield cloud_chunk
            return

    async def _call_gemini(self, prompt: str, model_name: str, context: list = None,
                           chat_type: str = "private", is_owner: bool = False, max_retries: int = 2) -> str:
        """
        –í—ã–∑–æ–≤ Cloud –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ OpenClaw Gateway.
        """
        # [HOTFIX v11.4.2] –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç–µ—Ä –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–£–°–ò–õ–ï–ù–ù–´–ô)
        if model_name and ("-exp" in model_name or "gemini-2.0-flash-exp" in model_name):
            if "thinking" not in model_name: # Thinking –ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ exp
                stable_chat_model = self.models.get("chat", "gemini-2.5-flash")
                logger.info(f"Filtering out problematic model: {model_name} -> {stable_chat_model}")
                model_name = stable_chat_model
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt
        from src.core.prompts import get_system_prompt
        base_instructions = get_system_prompt(chat_type == "private")

        persona_prompt = ""
        if self.persona:
            persona_prompt = self.persona.get_current_prompt(chat_type, is_owner)

        system_instructions = f"{persona_prompt}\n\n{base_instructions}".strip()

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è OpenClaw (OpenAI-like format)
        messages = []
        if system_instructions:
            messages.append({"role": "system", "content": system_instructions})
        
        if context:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π
            for msg in context:
                role = self._normalize_chat_role(msg.get("role", "user"))
                messages.append({"role": role, "content": msg.get("text", "")})
        
        messages.append({"role": "user", "content": prompt})

        for attempt in range(max_retries + 1):
            try:
                response_text = await self.openclaw_client.chat_completions(
                    messages,
                    model=model_name,
                    timeout_seconds=self.cloud_request_timeout_seconds,
                    probe_provider_on_error=self.cloud_probe_on_chat_error,
                )
                cleaned_response = self._sanitize_model_text(response_text)
                normalized = (cleaned_response or "").strip()
                error_detected = self._is_cloud_error_message(normalized)
                billing_issue = self._is_cloud_billing_error(normalized)
                fatal_auth_error = self._is_fatal_cloud_auth_error(normalized)

                if error_detected or billing_issue:
                    self._mark_cloud_soft_cap_if_needed(normalized or "–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    fatal_auth_error = self._is_fatal_cloud_auth_error(normalized)
                    if fatal_auth_error:
                        # R15: –ö—ç—à–∏—Ä—É–µ–º —Ñ–∞—Ç–∞–ª—å–Ω—É—é –æ—à–∏–±–∫—É –¥–ª—è Preflight Gate
                        provider = model_name.split("/", 1)[0]
                        self._preflight_cache[provider] = (time.time() + self._preflight_ttl_seconds, response_text)
                        
                        self._stats["cloud_failures"] += 1
                        return f"‚ùå –û—à–∏–±–∫–∞ Cloud: {response_text}"

                    category = self._categorize_cloud_error(normalized)
                    if not category.get("retryable", True):
                        provider = model_name.split("/", 1)[0]
                        self._preflight_cache[provider] = (time.time() + self._preflight_ttl_seconds, category.get("summary", "fatal error"))

                    if attempt < max_retries:
                        logger.warning(f"OpenClaw Attempt {attempt+1} failed: {response_text}")
                        await asyncio.sleep(2 ** (attempt + 1))
                        continue
                        
                    self._stats["cloud_failures"] += 1
                    if billing_issue:
                        return f"‚ùå –û—à–∏–±–∫–∞ –±–∏–ª–ª–∏–Ω–≥–∞ (OpenClaw): –ü–æ—Ö–æ–∂–µ, –Ω–∞ –∞–∫–∫–∞—É–Ω—Ç–µ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å —Å—Ä–µ–¥—Å—Ç–≤–∞ –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∞–ª–∞–Ω—Å –Ω–∞ —à–ª—é–∑–µ. (–î–µ—Ç–∞–ª–∏: {response_text})"
                    return f"‚ùå –û—à–∏–±–∫–∞ Cloud: {response_text}"

                self._stats["cloud_calls"] += 1
                return cleaned_response

            except Exception as e:
                logger.error(f"Cloud call failed: {e}")
                if attempt < max_retries:
                    await asyncio.sleep(2 ** (attempt + 1))
                    continue
                
                self._stats["cloud_failures"] += 1
                return f"‚ùå –û—à–∏–±–∫–∞ Cloud: {e}"

    async def route_query_stream(self,
                                 prompt: str,
                                 task_type: Literal['coding', 'chat', 'reasoning', 'creative'] = 'chat',
                                 context: list = None,
                                 chat_type: str = "private",
                                 is_owner: bool = False,
                                 use_rag: bool = True,
                                 skip_swarm: bool = False):
        """
        –í–µ—Ä—Å–∏—è route_query —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (–ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è Cloud).
        """
        # 1. –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∞–µ–º –≤—Å—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É (RAG, Tools) - —Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –≤ route_query
        if use_rag and self.rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        if self.tools and not skip_swarm:
            tool_data = await self.tools.execute_tool_chain(prompt, skip_swarm=True)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        if self.force_mode == 'force_local' and not self.is_local_available:
             yield "‚ùå –†–µ–∂–∏–º 'Force Local' –≤–∫–ª—é—á–µ–Ω, –Ω–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
             return
             
        if not self.is_local_available and not self.openclaw_client:
             yield "‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –æ—Ñ—Ñ–ª–∞–π–Ω, –æ–±–ª–∞—á–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω)."
             return

        # 3. –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è
        model_name = self.models.get(task_type, self.models["chat"])
        
        # –ï—Å–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ª–æ–∫–∞–ª–∫–∞ –∏–ª–∏ –æ–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ —ç—Ç–æ —á–∞—Ç/–∫–æ–¥
        if self.force_mode == 'force_local' or (
            self.force_mode != 'force_cloud'
            and self.is_local_available
            and task_type in ['chat', 'coding']
        ):
             try:
                 full_res = await self.route_query(prompt, task_type, context, chat_type, is_owner, use_rag=False)
                 if full_res and full_res.strip():
                     yield full_res
                 else:
                     logger.warning("Local internal route returned empty content")
                     yield "‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."
             except Exception as e:
                 logger.error(f"Fallback routing in stream failed: {e}")
                 yield f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏: {e}"
             return

        # 4. –°—Ç—Ä–∏–º–∏–Ω–≥ —á–µ—Ä–µ–∑ –æ–±–ª–∞–∫–æ (Gemini) —Å Fallback
        preferred = self.models.get(task_type, self.models["chat"])
        candidates = self._build_cloud_candidates(
            task_type=task_type,
            profile=self.classify_task_profile(prompt, task_type),
            preferred_model=preferred,
            chat_type=chat_type,
            is_owner=is_owner,
            prompt=prompt
        )
        
        last_err = None
        for i, candidate in enumerate(candidates):
            if "-exp" in candidate and "gemini-2.0" in candidate:
                candidate = candidate.replace("-exp", "")
            
            try:
                # –í—Ä–µ–º–µ–Ω–Ω–æ –ø—Å–µ–≤–¥–æ-—Å—Ç—Ä–∏–º–∏–Ω–≥ (–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –∑–∞ —Ä–∞–∑)
                response = await self._call_gemini(
                    prompt, 
                    candidate, 
                    context, 
                    chat_type, 
                    is_owner, 
                    max_retries=(0 if self.force_mode == "force_cloud" else (1 if i == 0 else 0))
                )
                
                if self._is_runtime_error_message(response):
                    self.last_cloud_error = str(response)
                    self.last_cloud_model = candidate
                    self._mark_cloud_soft_cap_if_needed(str(response))
                    last_err = response
                    if self._is_fatal_cloud_auth_error(response):
                        logger.error(
                            "Cloud stream aborted: fatal auth/billing error",
                            model=candidate,
                            error=str(response)[:280],
                        )
                        break
                    continue
                
                if response and len(response.strip()) >= 2:
                    self.last_cloud_error = None
                    self.last_cloud_model = candidate
                    yield self._sanitize_model_text(response)
                    return
            except Exception as e:
                last_err = f"Error from OpenClaw generator: {e}"
                continue
                
        # –ï—Å–ª–∏ –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —É–ø–∞–ª–∏
        err_out = last_err or "Cloud API failure"
        summary = self._summarize_cloud_error_for_user(err_out)
        yield (
            f"‚ùå –û—à–∏–±–∫–∞ Cloud: {summary}."
            if is_owner
            else "‚ùå –û–±–ª–∞—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
        )

    async def _call_gemini_stream(self, prompt: str, model_name: str, context: list = None,
                                  chat_type: str = "private", is_owner: bool = False):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ Cloud (–û—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API).
        """
        full_response = await self._call_gemini(prompt, model_name, context, chat_type, is_owner)
        yield full_response

    async def diagnose(self) -> dict:
        """
        –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º.
        """
        result = {}

        # 1. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_ok = await self.check_local_health(force=True)
        
        # Enhanced diagnostics via CLI scan
        local_models = await self._scan_local_models()
        local_count = len(local_models)
        
        local_status = "Offline"
        if local_ok:
            if self.active_local_model:
                local_status = f"{self.local_engine}: {self.active_local_model} ({local_count} models available)"
            else:
                local_status = f"{self.local_engine}: Ready (No Model Loaded, {local_count} available)"
        elif local_count > 0:
             local_status = f"Offline ({local_count} models detected via CLI)"
                
        result["Local AI"] = {
            "ok": local_ok,
            "status": local_status,
            "engine": self.local_engine or "Unknown",
            "model_count": local_count,
            "active_model": self.active_local_model
        }

        # 2. Gemini Cloud (via OpenClaw)
        openclaw_health = await self.openclaw_client.health_check()
        result["Cloud (OpenClaw)"] = {
            "ok": openclaw_health,
            "status": f"Ready ({self.models['chat']})" if openclaw_health else "Unreachable",
        }
        
        result["Cloud Reliability"] = {
            "ok": not bool(self.last_cloud_error),
            "status": "Errors Present" if self.last_cloud_error else "Stable",
            "last_error": self.last_cloud_error or "None",
            "last_provider_model": getattr(self, "last_cloud_model", "None"),
            "cloud_failures": self._stats.get("cloud_failures", 0),
            "force_mode": self.force_mode,
        }
        cloud_diag = self.get_last_cloud_error_info()
        result["Cloud Reliability"]["error_code"] = cloud_diag.get("code", "none")
        result["Cloud Reliability"]["error_summary"] = cloud_diag.get("summary", "")
        result["Cloud Reliability"]["retryable"] = bool(cloud_diag.get("retryable", True))

        # 3. RAG Engine
        if self.rag:
            try:
                rag_count = self.rag.get_total_documents()
                result["RAG Engine"] = {"ok": True, "status": f"{rag_count} documents"}
            except Exception as e:
                result["RAG Engine"] = {"ok": False, "status": str(e)}
        else:
             result["RAG Engine"] = {"ok": True, "status": "Disabled (OpenClaw)"}

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–∑–æ–≤–æ–≤
        result["Call Stats"] = {
            "ok": True,
            "status": (
                f"Local: {self._stats['local_calls']} ok / {self._stats['local_failures']} fail, "
                f"Cloud: {self._stats['cloud_calls']} ok / {self._stats['cloud_failures']} fail"
            ),
        }

        # 6. Workspace Check
        handover_path = Path(os.getcwd()) / "HANDOVER.md"
        result["üìÅ Workspace"] = {
            "ok": handover_path.exists(),
            "status": f"Root: {os.getcwd()} (HANDOVER.md: {'Found' if handover_path.exists() else 'MISSING'})"
        }

        return result

    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–∏—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model."""
        recommendations = {
            profile: self._get_profile_recommendation(profile)
            for profile in ["chat", "moderation", "code", "security", "infra", "review", "communication"]
        }
        return {
            "cloud_models": self.models.copy(),
            "local_engine": self.local_engine,
            "local_model": self.active_local_model,
            "local_available": self.is_local_available,
            "stats": self._stats.copy(),
            "force_mode": self.force_mode,
            "fallback_models": self.fallback_models,
            "routing_policy": self.routing_policy,
            "cloud_soft_cap_calls": self.cloud_soft_cap_calls,
            "cloud_soft_cap_reached": self.cloud_soft_cap_reached,
            "recommendations": recommendations,
            "usage_report": self._usage_report.copy(),
            "feedback_summary": self.get_feedback_summary(top=3),
        }

    def get_profile_recommendation(self, profile: str = "chat") -> dict:
        """–ü—É–±–ª–∏—á–Ω—ã–π helper –¥–ª—è –ø–æ–∫–∞–∑–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ø—Ä–æ—Ñ–∏–ª—é."""
        return self._get_profile_recommendation(profile)

    def get_last_route(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ —Ä–æ—É—Ç–µ—Ä–∞."""
        store = self._ensure_feedback_store()
        last_route = store.get("last_route", {})
        return dict(last_route) if isinstance(last_route, dict) else {}

    def get_last_stream_route(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ stream-–æ—Ç–≤–µ—Ç–∞."""
        return dict(self._last_stream_route) if isinstance(self._last_stream_route, dict) else {}

    def submit_feedback(
        self,
        score: int,
        profile: str | None = None,
        model_name: str | None = None,
        channel: str | None = None,
        note: str = "",
    ) -> dict:
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ (1-5) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ—ë
        –≤ –ø—Ä–æ—Ñ–∏–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π.
        """
        try:
            normalized_score = int(score)
        except Exception as exc:
            raise ValueError("score_must_be_integer_1_5") from exc
        if normalized_score < 1 or normalized_score > 5:
            raise ValueError("score_out_of_range_1_5")

        store = self._ensure_feedback_store()
        last_route = store.get("last_route", {}) if isinstance(store.get("last_route"), dict) else {}

        resolved_profile = str(profile or last_route.get("profile", "")).strip().lower()
        resolved_model = str(model_name or last_route.get("model", "")).strip()
        resolved_channel = self._normalize_channel(channel or last_route.get("channel"))

        if not resolved_profile or not resolved_model:
            raise ValueError("profile_and_model_required_or_run_task_first")

        profiles = store.setdefault("profiles", {})
        profile_entry = profiles.setdefault(
            resolved_profile,
            {"models": {}, "channels": {}, "feedback_total": 0},
        )
        if not isinstance(profile_entry.get("models"), dict):
            profile_entry["models"] = {}
        if not isinstance(profile_entry.get("channels"), dict):
            profile_entry["channels"] = {}

        model_entry = profile_entry["models"].setdefault(
            resolved_model,
            {"count": 0, "sum": 0, "avg": 0.0, "channels": {}, "last_score": 0, "last_ts": ""},
        )
        model_entry["count"] = int(model_entry.get("count", 0)) + 1
        model_entry["sum"] = int(model_entry.get("sum", 0)) + normalized_score
        model_entry["avg"] = round(model_entry["sum"] / model_entry["count"], 3)
        model_entry["last_score"] = normalized_score
        model_entry["last_ts"] = self._now_iso()
        if not isinstance(model_entry.get("channels"), dict):
            model_entry["channels"] = {}

        model_channel_entry = model_entry["channels"].setdefault(
            resolved_channel,
            {"count": 0, "sum": 0, "avg": 0.0},
        )
        model_channel_entry["count"] = int(model_channel_entry.get("count", 0)) + 1
        model_channel_entry["sum"] = int(model_channel_entry.get("sum", 0)) + normalized_score
        model_channel_entry["avg"] = round(model_channel_entry["sum"] / model_channel_entry["count"], 3)

        profile_channel_entry = profile_entry["channels"].setdefault(
            resolved_channel,
            {"count": 0, "sum": 0, "avg": 0.0},
        )
        profile_channel_entry["count"] = int(profile_channel_entry.get("count", 0)) + 1
        profile_channel_entry["sum"] = int(profile_channel_entry.get("sum", 0)) + normalized_score
        profile_channel_entry["avg"] = round(profile_channel_entry["sum"] / profile_channel_entry["count"], 3)
        profile_entry["feedback_total"] = int(profile_entry.get("feedback_total", 0)) + 1

        events = store.setdefault("events", [])
        if not isinstance(events, list):
            events = []
            store["events"] = events
        events.append(
            {
                "ts": self._now_iso(),
                "score": normalized_score,
                "profile": resolved_profile,
                "model": resolved_model,
                "channel": resolved_channel,
                "note": (note or "").strip()[:240],
            }
        )
        if len(events) > 400:
            del events[: len(events) - 400]

        store["updated_at"] = self._now_iso()
        self._save_json(self._feedback_path, store)
        return {
            "ok": True,
            "score": normalized_score,
            "profile": resolved_profile,
            "model": resolved_model,
            "channel": resolved_channel,
            "used_last_route": bool(not profile and not model_name),
            "profile_model_stats": {
                "count": int(model_entry.get("count", 0)),
                "avg": float(model_entry.get("avg", 0.0)),
            },
            "profile_channel_stats": {
                "count": int(profile_channel_entry.get("count", 0)),
                "avg": float(profile_channel_entry.get("avg", 0.0)),
            },
        }

    def get_feedback_summary(self, profile: str | None = None, top: int = 5) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–≤–æ–¥–∫—É –ø–æ –æ—Ü–µ–Ω–∫–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏.
        """
        safe_top = max(1, min(int(top), 20))
        store = self._ensure_feedback_store()
        profiles = store.get("profiles", {})
        events = store.get("events", [])
        last_route = store.get("last_route", {})

        profile_key = (profile or "").strip().lower() or None
        selected_profiles: list[tuple[str, dict]] = []
        if profile_key:
            selected_profiles.append((profile_key, profiles.get(profile_key, {})))
        else:
            selected_profiles = list(profiles.items())

        top_models: list[dict[str, Any]] = []
        channels_agg: dict[str, dict[str, float]] = {}
        total_feedback = 0

        for profile_name, pdata in selected_profiles:
            if not isinstance(pdata, dict):
                continue
            models = pdata.get("models", {})
            channels = pdata.get("channels", {})
            if not isinstance(models, dict):
                models = {}
            if not isinstance(channels, dict):
                channels = {}

            for model_name, mdata in models.items():
                if not isinstance(mdata, dict):
                    continue
                count = int(mdata.get("count", 0))
                avg = float(mdata.get("avg", 0.0))
                total_feedback += count
                top_models.append(
                    {
                        "profile": profile_name,
                        "model": str(model_name),
                        "count": count,
                        "avg_score": round(avg, 3),
                        "last_score": int(mdata.get("last_score", 0)),
                        "last_ts": str(mdata.get("last_ts", "")),
                    }
                )

            for channel_name, cdata in channels.items():
                if not isinstance(cdata, dict):
                    continue
                entry = channels_agg.setdefault(
                    str(channel_name),
                    {"count": 0, "sum": 0.0},
                )
                ch_count = int(cdata.get("count", 0))
                ch_avg = float(cdata.get("avg", 0.0))
                entry["count"] += ch_count
                entry["sum"] += ch_avg * ch_count

        top_models_sorted = sorted(
            top_models,
            key=lambda item: (float(item.get("avg_score", 0.0)), int(item.get("count", 0))),
            reverse=True,
        )[:safe_top]

        top_channels: list[dict[str, Any]] = []
        for channel_name, cdata in channels_agg.items():
            ccount = int(cdata.get("count", 0))
            csum = float(cdata.get("sum", 0.0))
            avg = (csum / ccount) if ccount > 0 else 0.0
            top_channels.append({"channel": channel_name, "count": ccount, "avg_score": round(avg, 3)})
        top_channels = sorted(
            top_channels,
            key=lambda item: (float(item.get("avg_score", 0.0)), int(item.get("count", 0))),
            reverse=True,
        )[:3]

        recent_events = []
        if isinstance(events, list):
            for item in events[-5:]:
                if isinstance(item, dict):
                    recent_events.append(
                        {
                            "ts": str(item.get("ts", "")),
                            "score": int(item.get("score", 0)),
                            "profile": str(item.get("profile", "")),
                            "model": str(item.get("model", "")),
                            "channel": str(item.get("channel", "")),
                        }
                    )

        return {
            "generated_at": self._now_iso(),
            "profile": profile_key,
            "top_models": top_models_sorted,
            "top_channels": top_channels,
            "total_feedback": total_feedback,
            "recent_events": recent_events,
            "last_route": dict(last_route) if isinstance(last_route, dict) else {},
        }

    def get_task_preflight(
        self,
        prompt: str,
        task_type: str = "chat",
        preferred_model: str | None = None,
        confirm_expensive: bool = False,
    ) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç preflight-–ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –¥–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞:
        - –ø—Ä–æ—Ñ–∏–ª—å –∏ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å;
        - –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª/–º–æ–¥–µ–ª—å;
        - —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è confirm-step;
        - –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è/—Ä–∏—Å–∫–∏;
        - –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è –º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å.
        """
        normalized_prompt = (prompt or "").strip()
        normalized_task_type = (task_type or "chat").strip().lower() or "chat"
        profile = self.classify_task_profile(normalized_prompt, normalized_task_type)
        recommendation = self._get_profile_recommendation(profile)
        is_critical = bool(recommendation.get("critical"))

        chosen_channel = recommendation.get("channel", "local")
        if self.force_mode == "force_local":
            chosen_channel = "local"
        elif self.force_mode == "force_cloud":
            chosen_channel = "cloud"
        else:
            prefer_cloud = is_critical or normalized_task_type == "reasoning"
            # –ù–ï –ø–µ—Ä–µ–±–∏–≤–∞–µ–º recommendation.channel –¥–ª—è non-critical:
            # Local First —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–∂–µ –∑–∞—à–∏—Ç–∞ –≤ _get_profile_recommendation
            if self.cloud_soft_cap_reached and not is_critical:
                prefer_cloud = False
            chosen_channel = "cloud" if prefer_cloud else "local"

        if chosen_channel == "cloud":
            chosen_model = self._resolve_cloud_model(
                task_type=normalized_task_type,
                profile=profile,
                preferred_model=preferred_model or recommendation.get("model"),
                chat_type="private",
                is_owner=False,
                prompt=normalized_prompt,
            )
        else:
            chosen_model = self.active_local_model or "local-auto"

        requires_confirm = bool(
            self.require_confirm_expensive and is_critical and chosen_channel == "cloud" and not confirm_expensive
        )
        can_run_now = not requires_confirm

        warnings: list[str] = []
        if chosen_channel == "local" and not self.is_local_available:
            warnings.append("–õ–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª —Å–µ–π—á–∞—Å offline; –≤–æ–∑–º–æ–∂–µ–Ω fallback –≤ cloud.")
        if self.cloud_soft_cap_reached and chosen_channel == "cloud":
            warnings.append("Cloud soft cap —É–∂–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç: –ø—Ä–æ–≤–µ—Ä—å policy/–ª–∏–º–∏—Ç—ã –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º.")
        if requires_confirm:
            warnings.append("–î–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω confirm-step (`--confirm-expensive`).")
        feedback_hint = recommendation.get("feedback_hint", {})
        feedback_count = int(feedback_hint.get("count", 0)) if isinstance(feedback_hint, dict) else 0
        feedback_avg = float(feedback_hint.get("avg_score", 0.0)) if isinstance(feedback_hint, dict) else 0.0
        if feedback_count >= 3 and feedback_avg <= 2.5:
            warnings.append(
                f"–£ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∏–∑–∫–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥ ({feedback_avg}/5); "
                "—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º."
            )

        marginal_cost_usd = (
            float(self.cloud_cost_per_call_usd)
            if chosen_channel == "cloud"
            else float(self.local_cost_per_call_usd)
        )

        reasons: list[str] = []
        if is_critical:
            reasons.append("–ö—Ä–∏—Ç–∏—á–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –∑–∞–¥–∞—á–∏.")
        if normalized_task_type == "reasoning":
            reasons.append("Reasoning-–∑–∞–¥–∞—á–∞ —Å –ø–æ–≤—ã—à–µ–Ω–Ω—ã–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞.")
        if self.force_mode == "force_local":
            reasons.append("–í–∫–ª—é—á–µ–Ω –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º force_local.")
        elif self.force_mode == "force_cloud":
            reasons.append("–í–∫–ª—é—á–µ–Ω –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º force_cloud.")
        if self.cloud_soft_cap_reached and not is_critical:
            reasons.append("Cloud soft cap –∞–∫—Ç–∏–≤–µ–Ω: non-critical –∑–∞–¥–∞—á–∏ —Å–¥–≤–∏–Ω—É—Ç—ã –≤ local.")
        if feedback_count >= 2:
            reasons.append(
                f"–ò—Å—Ç–æ—Ä–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –º–æ–¥–µ–ª–∏: {feedback_avg}/5 –Ω–∞ {feedback_count} –æ—Ü–µ–Ω–∫–∞—Ö."
            )
        if not reasons:
            reasons.append("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è policy free-first hybrid.")

        return {
            "generated_at": self._now_iso(),
            "task_type": normalized_task_type,
            "profile": profile,
            "critical": is_critical,
            "prompt_preview": normalized_prompt[:240],
            "recommendation": recommendation,
            "execution": {
                "channel": chosen_channel,
                "model": chosen_model,
                "can_run_now": can_run_now,
                "requires_confirm_expensive": requires_confirm,
                "confirm_expensive_received": bool(confirm_expensive),
            },
            "policy": {
                "routing_policy": self.routing_policy,
                "force_mode": self.force_mode,
                "cloud_soft_cap_reached": bool(self.cloud_soft_cap_reached),
                "local_available": bool(self.is_local_available),
            },
            "cost_hint": {
                "marginal_call_cost_usd": round(marginal_cost_usd, 6),
                "cloud_cost_per_call_usd": float(self.cloud_cost_per_call_usd),
                "local_cost_per_call_usd": float(self.local_cost_per_call_usd),
            },
            "warnings": warnings,
            "reasons": reasons,
            "next_step": (
                "–ó–∞–ø—É—Å—Ç–∏ –∑–∞–¥–∞—á—É —Å —Ñ–ª–∞–≥–æ–º --confirm-expensive."
                if requires_confirm
                else "–ú–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –∑–∞–¥–∞—á—É."
            ),
        }

    @staticmethod
    def _humanize_route_reason(route_reason: str, route_channel: str = "") -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ –ø—Ä–∏—á–∏–Ω—ã —Ä–æ—É—Ç–∏–Ω–≥–∞."""
        code = str(route_reason or "").strip().lower()
        channel = str(route_channel or "").strip().lower()
        reason_map = {
            "force_local": "–í—ã–±—Ä–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª –∏–∑-–∑–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ force_local.",
            "force_cloud": "–í—ã–±—Ä–∞–Ω –æ–±–ª–∞—á–Ω—ã–π –∫–∞–Ω–∞–ª –∏–∑-–∑–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ force_cloud.",
            "local_primary": "–°—Ä–∞–±–æ—Ç–∞–ª–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è local-first: –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞.",
            "local_stream_primary": "–ü–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∑–∞–≤–µ—Ä—à—ë–Ω –ª–æ–∫–∞–ª—å–Ω–æ (stream local-primary).",
            "local_unavailable": "–õ–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–ø–æ–ª–Ω–µ–Ω fallback –≤ cloud.",
            "local_failed_cloud_fallback": "–õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –æ—à–∏–±–∫–æ–π, –≤—ã–ø–æ–ª–Ω–µ–Ω fallback –≤ cloud.",
            "policy_prefer_cloud": "–ü–æ–ª–∏—Ç–∏–∫–∞ —Ä–æ—É—Ç–∏–Ω–≥–∞ –≤—ã–±—Ä–∞–ª–∞ cloud –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –∑–∞–¥–∞—á–∏.",
            "cloud_selected": "–í—ã–±—Ä–∞–Ω –æ–±–ª–∞—á–Ω—ã–π –∫–∞–Ω–∞–ª –ø–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ä–æ—É—Ç–µ—Ä–∞.",
            "cloud_failed_local_fallback": "Cloud-–∑–∞–ø—É—Å–∫ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –æ—à–∏–±–∫–æ–π, –≤—ã–ø–æ–ª–Ω–µ–Ω fallback –≤ local.",
            "critical_cloud_review": "–î–ª—è –∫—Ä–∏—Ç–∏—á–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –≤–∫–ª—é—á—ë–Ω cloud-review –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.",
        }
        if code in reason_map:
            return reason_map[code]
        if channel == "local":
            return "–ú–∞—Ä—à—Ä—É—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω —á–µ—Ä–µ–∑ local-–∫–∞–Ω–∞–ª –ø–æ —Ç–µ–∫—É—â–µ–π policy."
        if channel == "cloud":
            return "–ú–∞—Ä—à—Ä—É—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω —á–µ—Ä–µ–∑ cloud-–∫–∞–Ω–∞–ª –ø–æ —Ç–µ–∫—É—â–µ–π policy."
        return "–ü—Ä–∏—á–∏–Ω–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –Ω–µ –±—ã–ª–∞ —è–≤–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞."

    def get_route_explain(
        self,
        *,
        prompt: str = "",
        task_type: str = "chat",
        preferred_model: str | None = None,
        confirm_expensive: bool = False,
    ) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç explainability-—Å—Ä–µ–∑ –ø–æ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏/–∫–∞–Ω–∞–ª–∞.

        –ß—Ç–æ –≤–Ω—É—Ç—Ä–∏:
        1) last_route —Å route_reason/route_detail;
        2) policy snapshot (force_mode, soft-cap, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å local);
        3) preflight (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω prompt);
        4) explainability_score ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–∑—Ä–∞—á–µ–Ω –º–∞—Ä—à—Ä—É—Ç.
        """
        last_route = self.get_last_route()
        route_reason = str(last_route.get("route_reason", "")).strip() if isinstance(last_route, dict) else ""
        route_detail = str(last_route.get("route_detail", "")).strip() if isinstance(last_route, dict) else ""
        route_channel = str(last_route.get("channel", "")).strip() if isinstance(last_route, dict) else ""

        policy_snapshot = {
            "routing_policy": self.routing_policy,
            "force_mode": self.force_mode,
            "cloud_soft_cap_reached": bool(self.cloud_soft_cap_reached),
            "local_available": bool(self.is_local_available),
        }

        preflight_payload: dict[str, Any] | None = None
        normalized_prompt = str(prompt or "").strip()
        if normalized_prompt:
            preflight_payload = self.get_task_preflight(
                prompt=normalized_prompt,
                task_type=task_type,
                preferred_model=preferred_model,
                confirm_expensive=confirm_expensive,
            )

        explainability_score = 0
        if isinstance(last_route, dict) and last_route:
            explainability_score += 40
        if route_reason:
            explainability_score += 30
        if route_detail:
            explainability_score += 10
        if preflight_payload is not None:
            explainability_score += 20
        explainability_score = max(0, min(100, explainability_score))

        if explainability_score >= 80:
            transparency_level = "high"
        elif explainability_score >= 50:
            transparency_level = "medium"
        else:
            transparency_level = "low"

        return {
            "generated_at": self._now_iso(),
            "last_route": last_route if isinstance(last_route, dict) else {},
            "reason": {
                "code": route_reason or "unknown",
                "detail": route_detail or "",
                "human": self._humanize_route_reason(route_reason, route_channel),
            },
            "policy": policy_snapshot,
            "preflight": preflight_payload,
            "explainability_score": explainability_score,
            "transparency_level": transparency_level,
        }

    def get_usage_summary(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π usage-—Å—Ä–µ–∑ –¥–ª—è Ops –ø–∞–Ω–µ–ª–∏ –∏ –∞–ª–µ—Ä—Ç–æ–≤.
        """
        channels = self._usage_report.get("channels", {}) if isinstance(self._usage_report, dict) else {}
        local_calls = int(channels.get("local", 0))
        cloud_calls = int(channels.get("cloud", 0))
        total_calls = local_calls + cloud_calls

        cloud_share = round((cloud_calls / total_calls), 3) if total_calls > 0 else 0.0
        local_share = round((local_calls / total_calls), 3) if total_calls > 0 else 0.0

        models = self._usage_report.get("models", {}) if isinstance(self._usage_report, dict) else {}
        top_models = sorted(
            ((name, int(count)) for name, count in models.items()),
            key=lambda item: item[1],
            reverse=True,
        )[:5]
        profiles = self._usage_report.get("profiles", {}) if isinstance(self._usage_report, dict) else {}
        top_profiles = sorted(
            ((name, int(count)) for name, count in profiles.items()),
            key=lambda item: item[1],
            reverse=True,
        )[:5]

        cloud_remaining = max(0, int(self.cloud_soft_cap_calls) - cloud_calls)
        return {
            "totals": {
                "all_calls": total_calls,
                "local_calls": local_calls,
                "cloud_calls": cloud_calls,
            },
            "ratios": {
                "local_share": local_share,
                "cloud_share": cloud_share,
            },
            "soft_cap": {
                "cloud_soft_cap_calls": int(self.cloud_soft_cap_calls),
                "cloud_soft_cap_reached": bool(self.cloud_soft_cap_reached),
                "cloud_remaining_calls": cloud_remaining,
            },
            "top_models": [{"model": name, "count": count} for name, count in top_models],
            "top_profiles": [{"profile": name, "count": count} for name, count in top_profiles],
        }

    def get_ops_alerts(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤ –∏ –æ–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ops-–∫–æ–Ω—Ç—É—Ä–∞.
        """
        summary = self.get_usage_summary()
        alerts: list[dict[str, str]] = []
        cloud_calls = int(summary["totals"]["cloud_calls"])
        local_calls = int(summary["totals"]["local_calls"])
        soft_cap = int(summary["soft_cap"]["cloud_soft_cap_calls"])
        remaining = int(summary["soft_cap"]["cloud_remaining_calls"])
        cloud_share = float(summary["ratios"]["cloud_share"])

        if bool(summary["soft_cap"]["cloud_soft_cap_reached"]):
            alerts.append(
                {
                    "severity": "high",
                    "code": "cloud_soft_cap_reached",
                    "message": "–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç CLOUD_SOFT_CAP_CALLS, –Ω–µ-–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ —É–π–¥—É—Ç –≤ –ª–æ–∫–∞–ª–∫—É.",
                }
            )
        elif soft_cap > 0 and cloud_calls >= int(soft_cap * 0.8):
            alerts.append(
                {
                    "severity": "medium",
                    "code": "cloud_soft_cap_near",
                    "message": f"Cloud usage –±–ª–∏–∑–∫–æ –∫ –ª–∏–º–∏—Ç—É: –æ—Å—Ç–∞–ª–æ—Å—å {remaining} –≤—ã–∑–æ–≤–æ–≤.",
                }
            )

        if cloud_calls >= 20 and cloud_share >= 0.75:
            alerts.append(
                {
                    "severity": "medium",
                    "code": "cloud_share_high",
                    "message": "–í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –æ–±–ª–∞—á–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤; –ø—Ä–æ–≤–µ—Ä—å –ø–æ–ª–∏—Ç–∏–∫—É free-first –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏.",
                }
            )

        if local_calls == 0 and cloud_calls > 0:
            alerts.append(
                {
                    "severity": "low",
                    "code": "local_usage_absent",
                    "message": "–õ–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è; –ø—Ä–æ–≤–µ—Ä—å LM Studio/Ollama –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é.",
                }
            )

        # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π guardrail: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏.
        store = self._ensure_feedback_store()
        low_quality_models: list[str] = []
        feedback_profiles = store.get("profiles", {})
        if isinstance(feedback_profiles, dict):
            for profile_name, pdata in feedback_profiles.items():
                if not isinstance(pdata, dict):
                    continue
                models = pdata.get("models", {})
                if not isinstance(models, dict):
                    continue
                for model_name, mdata in models.items():
                    if not isinstance(mdata, dict):
                        continue
                    mcount = int(mdata.get("count", 0))
                    mavg = float(mdata.get("avg", 0.0))
                    if mcount >= 3 and mavg <= 2.5:
                        low_quality_models.append(f"{profile_name}:{model_name}({mavg}/5, n={mcount})")
                        if len(low_quality_models) >= 2:
                            break
                if len(low_quality_models) >= 2:
                    break
        if low_quality_models:
            alerts.append(
                {
                    "severity": "medium",
                    "code": "model_quality_degraded",
                    "message": "–ï—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–∏–º user-feedback: " + "; ".join(low_quality_models),
                }
            )

        # –ë—é–¥–∂–µ—Ç–Ω—ã–µ guardrails (–æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ forecast –≤—ã–∑–æ–≤–æ–≤).
        cost_report = self.get_cost_report(monthly_calls_forecast=self.monthly_calls_forecast)
        monthly = cost_report.get("monthly_forecast", {})
        forecast_total = float(monthly.get("forecast_total_cost", 0.0))
        budget = max(0.0, float(self.cloud_monthly_budget_usd))
        if budget > 0:
            ratio = forecast_total / budget if budget else 0.0
            if ratio >= 1.0:
                alerts.append(
                    {
                        "severity": "high",
                        "code": "cloud_budget_exceeded_forecast",
                        "message": (
                            f"–ü—Ä–æ–≥–Ω–æ–∑ –æ–±–ª–∞—á–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ ({forecast_total:.2f}$) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –±—é–¥–∂–µ—Ç "
                            f"({budget:.2f}$) –Ω–∞ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ—Ñ–∏–ª–µ –Ω–∞–≥—Ä—É–∑–∫–∏."
                        ),
                    }
                )
            elif ratio >= 0.9:
                alerts.append(
                    {
                        "severity": "medium",
                        "code": "cloud_budget_near_forecast",
                        "message": (
                            f"–ü—Ä–æ–≥–Ω–æ–∑ –æ–±–ª–∞—á–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ ({forecast_total:.2f}$) –±–ª–∏–∑–æ–∫ –∫ –±—é–¥–∂–µ—Ç—É "
                            f"({budget:.2f}$)."
                        ),
                    }
                )

        acknowledged = self._ops_state.get("acknowledged", {}) if isinstance(self._ops_state, dict) else {}
        for alert in alerts:
            code = str(alert.get("code", "")).strip()
            ack_meta = acknowledged.get(code, {})
            if isinstance(ack_meta, dict) and ack_meta:
                alert["acknowledged"] = True
                alert["ack"] = {
                    "ts": str(ack_meta.get("ts", "")),
                    "actor": str(ack_meta.get("actor", "")),
                    "note": str(ack_meta.get("note", "")),
                }
            else:
                alert["acknowledged"] = False

        payload = {
            "status": "alert" if alerts else "ok",
            "alerts": alerts,
            "summary": summary,
            "cost_report": cost_report,
        }
        self._append_ops_history(payload)
        return payload

    def get_cost_report(self, monthly_calls_forecast: int = 5000) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–æ—á–Ω—ã–π cost-report –ø–æ —Ç–µ–∫—É—â–µ–º—É usage.
        """
        summary = self.get_usage_summary()
        totals = summary.get("totals", {})
        local_calls = int(totals.get("local_calls", 0))
        cloud_calls = int(totals.get("cloud_calls", 0))
        total_calls = int(totals.get("all_calls", local_calls + cloud_calls))

        cloud_cost = round(cloud_calls * float(self.cloud_cost_per_call_usd), 6)
        local_cost = round(local_calls * float(self.local_cost_per_call_usd), 6)
        total_cost = round(cloud_cost + local_cost, 6)
        avg_cost_per_call = round((total_cost / total_calls), 6) if total_calls > 0 else 0.0

        forecast = max(0, int(monthly_calls_forecast))
        cloud_share = float(summary.get("ratios", {}).get("cloud_share", 0.0))
        local_share = float(summary.get("ratios", {}).get("local_share", 0.0))
        forecast_cloud_calls = round(forecast * cloud_share)
        forecast_local_calls = round(forecast * local_share)
        forecast_cloud_cost = round(forecast_cloud_calls * float(self.cloud_cost_per_call_usd), 6)
        forecast_local_cost = round(forecast_local_calls * float(self.local_cost_per_call_usd), 6)
        forecast_total_cost = round(forecast_cloud_cost + forecast_local_cost, 6)

        return {
            "costs_usd": {
                "cloud_calls_cost": cloud_cost,
                "local_calls_cost": local_cost,
                "total_cost": total_cost,
                "avg_cost_per_call": avg_cost_per_call,
            },
            "pricing": {
                "cloud_cost_per_call_usd": float(self.cloud_cost_per_call_usd),
                "local_cost_per_call_usd": float(self.local_cost_per_call_usd),
            },
            "monthly_forecast": {
                "forecast_calls": forecast,
                "forecast_cloud_calls": forecast_cloud_calls,
                "forecast_local_calls": forecast_local_calls,
                "forecast_cloud_cost": forecast_cloud_cost,
                "forecast_local_cost": forecast_local_cost,
                "forecast_total_cost": forecast_total_cost,
            },
            "usage_summary": summary,
            "budget": {
                "cloud_monthly_budget_usd": float(self.cloud_monthly_budget_usd),
                "forecast_ratio": round((forecast_total_cost / float(self.cloud_monthly_budget_usd)), 4)
                if float(self.cloud_monthly_budget_usd) > 0
                else 0.0,
            },
        }

    def get_credit_runway_report(
        self,
        credits_usd: float = 300.0,
        horizon_days: int = 80,
        reserve_ratio: float = 0.1,
        monthly_calls_forecast: int | None = None,
    ) -> dict:
        """
        –°—á–∏—Ç–∞–µ—Ç ¬´–¥–æ—Ä–æ–∂–∫—É —Ä–∞—Å—Ö–æ–¥–∞¬ª –∫—Ä–µ–¥–∏—Ç–∞:
        - —Ü–µ–ª–µ–≤–æ–π –±—é–¥–∂–µ—Ç –≤ –¥–µ–Ω—å (—á—Ç–æ–±—ã –¥–æ–∂–∏—Ç—å –¥–æ horizon_days),
        - –æ—Ü–µ–Ω–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ daily burn-rate,
        - runway –≤ –¥–Ω—è—Ö –ø—Ä–∏ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ—Ñ–∏–ª–µ,
        - —Å—Ü–µ–Ω–∞—Ä–Ω—ã–µ –ª–∏–º–∏—Ç—ã –≤—ã–∑–æ–≤–æ–≤/–¥–µ–Ω—å –ø–æ Flash Lite / Flash / Pro.
        """
        safe_credits = max(0.0, float(credits_usd))
        safe_days = max(1, int(horizon_days))
        safe_reserve = min(0.95, max(0.0, float(reserve_ratio)))
        usable_budget = round(safe_credits * (1.0 - safe_reserve), 6)
        daily_target_budget = round(usable_budget / safe_days, 6)

        forecast_calls = (
            int(monthly_calls_forecast)
            if monthly_calls_forecast is not None
            else int(self.monthly_calls_forecast)
        )
        cost_report = self.get_cost_report(monthly_calls_forecast=forecast_calls)
        costs = cost_report.get("costs_usd", {})
        monthly = cost_report.get("monthly_forecast", {})
        pricing = cost_report.get("pricing", {})

        current_avg_cost = max(0.0, float(costs.get("avg_cost_per_call", 0.0)))
        # –ï—Å–ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∫–∞ –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º cloud baseline.
        if current_avg_cost <= 0:
            current_avg_cost = max(0.000001, float(pricing.get("cloud_cost_per_call_usd", self.cloud_cost_per_call_usd)))

        forecast_monthly_total = max(0.0, float(monthly.get("forecast_total_cost", 0.0)))
        estimated_daily_burn = round(forecast_monthly_total / 30.0, 6)
        if estimated_daily_burn <= 0:
            # –î–µ–≥—Ä–∞–¥–∞—Ü–∏–æ–Ω–Ω—ã–π fallback: —Å—á–∏—Ç–∞–µ–º –æ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ –±—é–¥–∂–µ—Ç–∞.
            estimated_daily_burn = daily_target_budget

        runway_days_at_current = (
            round(safe_credits / estimated_daily_burn, 2)
            if estimated_daily_burn > 0
            else float("inf")
        )
        recommended_calls_per_day = int(daily_target_budget / current_avg_cost) if current_avg_cost > 0 else 0

        def _calls_per_day(unit_cost: float) -> int:
            safe_unit = max(0.000001, float(unit_cost))
            return int(daily_target_budget / safe_unit)

        scenarios = {
            "flash_lite": {
                "unit_cost_usd": round(float(self.model_cost_flash_lite_usd), 6),
                "max_calls_per_day": _calls_per_day(self.model_cost_flash_lite_usd),
            },
            "flash": {
                "unit_cost_usd": round(float(self.model_cost_flash_usd), 6),
                "max_calls_per_day": _calls_per_day(self.model_cost_flash_usd),
            },
            "pro": {
                "unit_cost_usd": round(float(self.model_cost_pro_usd), 6),
                "max_calls_per_day": _calls_per_day(self.model_cost_pro_usd),
            },
        }

        return {
            "credits_usd": safe_credits,
            "horizon_days": safe_days,
            "reserve_ratio": safe_reserve,
            "usable_budget_usd": usable_budget,
            "daily_target_budget_usd": daily_target_budget,
            "estimated_daily_burn_usd": estimated_daily_burn,
            "runway_days_at_current_burn": runway_days_at_current,
            "current_avg_cost_per_call_usd": round(current_avg_cost, 6),
            "recommended_calls_per_day": recommended_calls_per_day,
            "forecast_calls_monthly": forecast_calls,
            "scenarios": scenarios,
            "cost_report": cost_report,
        }

    def acknowledge_ops_alert(self, code: str, actor: str = "owner", note: str = "") -> dict:
        """–ü–æ–º–µ—á–∞–µ—Ç alert –∫–∞–∫ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º."""
        normalized_code = (code or "").strip()
        if not normalized_code:
            raise ValueError("code_required")

        ack = self._ops_state.setdefault("acknowledged", {})
        ack[normalized_code] = {
            "ts": self._now_iso(),
            "actor": (actor or "owner").strip() or "owner",
            "note": (note or "").strip(),
        }
        self._save_json(self._ops_state_path, self._ops_state)
        return {"ok": True, "code": normalized_code, "ack": ack[normalized_code]}

    def clear_ops_alert_ack(self, code: str) -> dict:
        """–°–Ω–∏–º–∞–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ alert –∫–æ–¥–∞."""
        normalized_code = (code or "").strip()
        if not normalized_code:
            raise ValueError("code_required")

        ack = self._ops_state.setdefault("acknowledged", {})
        existed = normalized_code in ack
        ack.pop(normalized_code, None)
        self._save_json(self._ops_state_path, self._ops_state)
        return {"ok": True, "code": normalized_code, "removed": existed}

    def get_ops_history(self, limit: int = 30) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é ops snapshot-–æ–≤."""
        safe_limit = max(1, min(int(limit), 200))
        history = self._ops_state.get("history", []) if isinstance(self._ops_state, dict) else []
        if not isinstance(history, list):
            history = []
        return {
            "items": history[-safe_limit:],
            "count": min(len(history), safe_limit),
            "total": len(history),
        }

    def get_ops_report(self, history_limit: int = 20, monthly_calls_forecast: int | None = None) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–¥–∏–Ω—ã–π ops-–æ—Ç—á–µ—Ç –¥–ª—è API/–∫–æ–º–∞–Ω–¥:
        usage + alerts + costs + history.
        """
        forecast = int(monthly_calls_forecast) if monthly_calls_forecast is not None else int(self.monthly_calls_forecast)
        usage = self.get_usage_summary()
        alerts = self.get_ops_alerts()
        costs = self.get_cost_report(monthly_calls_forecast=forecast)
        history = self.get_ops_history(limit=history_limit)
        return {
            "generated_at": self._now_iso(),
            "usage": usage,
            "alerts": alerts,
            "costs": costs,
            "history": history,
        }

    def get_ops_executive_summary(self, monthly_calls_forecast: int | None = None) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π executive summary –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞:
        KPI, —Ä–∏—Å–∫–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –æ–¥–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ.
        """
        forecast = int(monthly_calls_forecast) if monthly_calls_forecast is not None else int(self.monthly_calls_forecast)
        usage = self.get_usage_summary()
        alerts_payload = self.get_ops_alerts()
        alerts = alerts_payload.get("alerts", [])
        costs = self.get_cost_report(monthly_calls_forecast=forecast)

        totals = usage.get("totals", {})
        ratios = usage.get("ratios", {})
        soft_cap = usage.get("soft_cap", {})
        budget = costs.get("budget", {})
        monthly = costs.get("monthly_forecast", {})

        severities = [str(item.get("severity", "low")).lower() for item in alerts if isinstance(item, dict)]
        risk_level = "low"
        if "high" in severities:
            risk_level = "high"
        elif "medium" in severities:
            risk_level = "medium"

        recommendations: list[str] = []
        cloud_share = float(ratios.get("cloud_share", 0.0))
        budget_ratio = float(budget.get("forecast_ratio", 0.0))
        alert_codes = {
            str(item.get("code", ""))
            for item in alerts
            if isinstance(item, dict)
        }
        if bool(soft_cap.get("cloud_soft_cap_reached")):
            recommendations.append("–°–Ω–∏–∑–∏—Ç—å cloud-–Ω–∞–≥—Ä—É–∑–∫—É: —É–≤–µ—Å—Ç–∏ non-critical –ø—Ä–æ—Ñ–∏–ª–∏ –≤ local.")
        elif cloud_share >= 0.75:
            recommendations.append("–ü–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏ routing policy: —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ–ª—é cloud.")
        if budget_ratio >= 1.0:
            recommendations.append("–°—Ä–æ—á–Ω–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –º–µ—Å—è—á–Ω—ã–π forecast/budget –∏–ª–∏ –ø–æ–Ω–∏–∑–∏—Ç—å cloud —Ç–∞—Ä–∏—Ñ –∑–∞–¥–∞—á.")
        elif budget_ratio >= 0.9:
            recommendations.append("–ë—é–¥–∂–µ—Ç –Ω–∞ –≥—Ä–∞–Ω–∏: –ø—Ä–∏–º–µ–Ω–∏—Ç—å throttling –¥–æ—Ä–æ–≥–∏—Ö cloud –ø—Ä–æ–≥–æ–Ω–æ–≤.")
        if "model_quality_degraded" in alert_codes:
            recommendations.append("–û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã: —É —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —É—Å—Ç–æ–π—á–∏–≤–æ –Ω–∏–∑–∫–∏–π feedback.")
        if int(totals.get("local_calls", 0)) == 0 and int(totals.get("cloud_calls", 0)) > 0:
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å LM Studio/Ollama: –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–∞–Ω–∞–ª —Å–µ–π—á–∞—Å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")
        if not recommendations:
            recommendations.append("–ö–æ–Ω—Ç—É—Ä —Å—Ç–∞–±–∏–ª—å–Ω—ã–π: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ç–µ–∫—É—â—É—é policy –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥.")

        return {
            "generated_at": self._now_iso(),
            "risk_level": risk_level,
            "kpi": {
                "calls_total": int(totals.get("all_calls", 0)),
                "cloud_share": cloud_share,
                "forecast_total_cost": float(monthly.get("forecast_total_cost", 0.0)),
                "budget_ratio": budget_ratio,
                "active_alerts": len(alerts),
            },
            "alerts_brief": [
                {
                    "severity": str(item.get("severity", "info")),
                    "code": str(item.get("code", "")),
                    "acknowledged": bool(item.get("acknowledged", False)),
                }
                for item in alerts[:8]
                if isinstance(item, dict)
            ],
            "recommendations": recommendations[:6],
        }

    def prune_ops_history(self, max_age_days: int = 30, keep_last: int = 100) -> dict:
        """
        –û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é ops snapshot:
        - —É–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ max_age_days,
        - –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–∏–Ω–∏–º—É–º keep_last –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π.
        """
        safe_age_days = max(1, int(max_age_days))
        safe_keep_last = max(1, int(keep_last))
        history = self._ops_state.get("history", []) if isinstance(self._ops_state, dict) else []
        if not isinstance(history, list):
            history = []

        before_count = len(history)
        if before_count == 0:
            return {
                "ok": True,
                "before": 0,
                "after": 0,
                "removed": 0,
                "max_age_days": safe_age_days,
                "keep_last": safe_keep_last,
            }

        cutoff_ts = datetime.now(timezone.utc).timestamp() - (safe_age_days * 86400)
        forced_keep_indices = set(range(max(0, before_count - safe_keep_last), before_count))
        kept: list[dict[str, Any]] = []

        for idx, item in enumerate(history):
            if idx in forced_keep_indices:
                kept.append(item)
                continue
            ts_raw = str(item.get("ts", "")).strip()
            if not ts_raw:
                continue
            ts_norm = ts_raw.replace("Z", "+00:00")
            try:
                item_ts = datetime.fromisoformat(ts_norm).timestamp()
            except Exception:
                # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ timestamp-—ã —É–±–∏—Ä–∞–µ–º –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ.
                continue
            if item_ts >= cutoff_ts:
                kept.append(item)

        self._ops_state["history"] = kept
        self._save_json(self._ops_state_path, self._ops_state)
        after_count = len(kept)
        return {
            "ok": True,
            "before": before_count,
            "after": after_count,
            "removed": max(0, before_count - after_count),
            "max_age_days": safe_age_days,
            "keep_last": safe_keep_last,
        }

    def _append_ops_history(self, payload: dict) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—Ä–∞—Ç–∫–∏–π snapshot ops-–∞–ª–µ—Ä—Ç–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏—é."""
        history = self._ops_state.setdefault("history", [])
        if not isinstance(history, list):
            self._ops_state["history"] = []
            history = self._ops_state["history"]

        alerts = payload.get("alerts", []) if isinstance(payload, dict) else []
        snapshot = {
            "ts": self._now_iso(),
            "status": str(payload.get("status", "unknown")),
            "alerts_count": len(alerts) if isinstance(alerts, list) else 0,
            "codes": [str(item.get("code", "")) for item in (alerts or []) if isinstance(item, dict)],
            "cloud_calls": int(payload.get("summary", {}).get("totals", {}).get("cloud_calls", 0)),
            "local_calls": int(payload.get("summary", {}).get("totals", {}).get("local_calls", 0)),
        }
        history.append(snapshot)
        if len(history) > 500:
            del history[: len(history) - 500]
        self._save_json(self._ops_state_path, self._ops_state)

    def _now_iso(self) -> str:
        return datetime.now(timezone.utc).isoformat(timespec="seconds")

    def get_ram_usage(self) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM —á–µ—Ä–µ–∑ SystemMonitor.
        """
        try:
            from src.utils.system_monitor import SystemMonitor
            snapshot = SystemMonitor.get_snapshot()
            return {
                "total_gb": round(snapshot.ram_total_gb, 1),
                "used_gb": round(snapshot.ram_used_gb, 1),
                "available_gb": round(snapshot.ram_available_gb, 1),
                "percent": snapshot.ram_percent,
                "can_load_heavy": SystemMonitor.can_load_heavy_model()
            }
        except Exception as e:
            return {"error": str(e), "can_load_heavy": True}
