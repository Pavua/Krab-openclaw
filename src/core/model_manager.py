# -*- coding: utf-8 -*-
"""
Model Manager (Router) –¥–ª—è Krab v6.5.
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Cloud vs Local).

–°—Ç—Ä–∞—Ç–µ–≥–∏—è: Local First ‚Üí Cloud Fallback.
- –ü—Ä–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio/Ollama ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö (–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å + —Å–∫–æ—Ä–æ—Å—Ç—å)
- –ü—Ä–∏ –æ—à–∏–±–∫–µ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ Gemini Cloud
- RAG –∏ Tool Orchestration —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –ö–ê–ñ–î–´–ô –∑–∞–ø—Ä–æ—Å
"""

import os
import time
import asyncio
import aiohttp
from typing import Literal, Optional, Dict, Any, List
from src.core.rag_engine import RAGEngine

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
import structlog
logger = structlog.get_logger("ModelRouter")

# Gemini SDK ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏–º–ø–æ—Ä—Ç–µ
try:
    import google.generativeai as genai
    _GENAI_AVAILABLE = True
except ImportError:
    _GENAI_AVAILABLE = False
    genai = None

class ModelRouter:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.lm_studio_url = config.get("LM_STUDIO_URL", "http://localhost:1234/v1")
        self.ollama_url = config.get("OLLAMA_URL", "http://localhost:11434/api")
        self.gemini_key = config.get("GEMINI_API_KEY")

        # –°—Ç–∞—Ç—É—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        self.is_local_available = False
        self.local_engine = None  # 'lm-studio' or 'ollama'
        self.active_local_model = None

        # –ö–µ—à –¥–ª—è health-check (—á—Ç–æ–±—ã –Ω–µ –¥—ë—Ä–≥–∞—Ç—å API –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å)
        self._health_cache_ts = 0
        self._health_cache_ttl = 30  # —Å–µ–∫—É–Ω–¥

        # Gemini SDK ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º –û–î–ò–ù –†–ê–ó
        # –ü—Ä–∏—á–∏–Ω–∞: —Ä–∞–Ω–µ–µ genai.configure() –≤—ã–∑—ã–≤–∞–ª—Å—è –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî –¥–æ—Ä–æ–≥–æ
        self.gemini_client = None
        if _GENAI_AVAILABLE and self.gemini_key:
            genai.configure(api_key=self.gemini_key)
            self.gemini_client = genai
            logger.info("‚òÅÔ∏è Gemini SDK configured successfully")

        # RAG Engine
        self.rag = RAGEngine()

        # Persona Manager (–Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py)
        self.persona = None
        self.tools = None  # –ù–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è –≤ main.py (ToolHandler)

        # –ü—É–ª –º–æ–¥–µ–ª–µ–π ‚Äî —á–∏—Ç–∞–µ–º –∏–∑ .env, –¥–µ—Ñ–æ–ª—Ç—ã –∫–∞–∫ fallback
        # –ü—Ä–∏—á–∏–Ω–∞: —Ö–∞—Ä–¥–∫–æ–¥ gemini-2.0-flash –º–µ—à–∞–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø—Ä–∞–≤–∫–∏ –∫–æ–¥–∞
        self.models = {
            "chat": config.get("GEMINI_CHAT_MODEL", "gemini-2.0-flash"),
            "thinking": config.get("GEMINI_THINKING_MODEL", "gemini-2.0-flash-thinking-exp"),
            "pro": config.get("GEMINI_PRO_MODEL", "gemini-2.0-pro-exp"),
            "coding": config.get("GEMINI_CODING_MODEL", "gemini-2.0-flash"),
        }

        # –°—á—ë—Ç—á–∏–∫–∏ (–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
        self._stats = {
            "local_calls": 0,
            "cloud_calls": 0,
            "local_failures": 0,
            "cloud_failures": 0,
        }

    async def check_local_health(self, force: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–ø—É—â–µ–Ω –ª–∏ LM Studio –∏–ª–∏ Ollama.
        –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–µ—à–∏—Ä—É–µ—Ç—Å—è –Ω–∞ _health_cache_ttl —Å–µ–∫—É–Ω–¥.
        force=True ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–∞.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à (TTL 30—Å) ‚Äî –Ω–µ –¥—ë—Ä–≥–∞–µ–º API –Ω–∞ –∫–∞–∂–¥—ã–π route_query
        now = time.time()
        if not force and (now - self._health_cache_ts) < self._health_cache_ttl:
            return self.is_local_available

        self._health_cache_ts = now

        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º LM Studio (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        try:
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{self.lm_studio_url}/models") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.local_engine = 'lm-studio'
                            self.is_local_available = True
                            logger.info(f"Local AI Available (LM Studio): {self.active_local_model}")
                            return True
        except Exception:
            pass

        # 2. –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º Ollama
        try:
            timeout = aiohttp.ClientTimeout(total=2)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(f"{self.ollama_url.replace('/api', '/v1')}/models") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get('data', [])
                        if models:
                            self.active_local_model = models[0]['id']
                            self.local_engine = 'ollama'
                            self.is_local_available = True
                            logger.info(f"Local AI Available (Ollama): {self.active_local_model}")
                            return True
        except Exception:
            pass

        self.is_local_available = False
        self.local_engine = None
        self.active_local_model = None
        return False

    async def _call_local_llm(self, prompt: str, context: list = None, is_private: bool = True) -> str:
        """
        –í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å (aiohttp).
        """
        try:
            # –í—ã–±–∏—Ä–∞–µ–º URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–≤–∏–∂–∫–∞
            base_url = self.lm_studio_url if self.local_engine == 'lm-studio' else \
                       self.ollama_url.replace('/api', '/v1')

            # –§–æ—Ä–º–∏—Ä—É–µ–º payload
            messages = []
            if context:
                messages.extend(context)
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self.active_local_model or "local-model",
                "messages": messages,
                "temperature": 0.7
            }

            headers = {"Content-Type": "application/json"}
            
            # –¢–∞–π–º–∞—É—Ç –ø–æ–±–æ–ª—å—à–µ –¥–ª—è –ª–æ–∫–∞–ª–∫–∏
            timeout = aiohttp.ClientTimeout(total=60)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/chat/completions", 
                    json=payload, 
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        # LOGGING RAW RESPONSE FOR DEBUGGING
                        logger.info(f"Local LLM Raw Response: {data}")
                        
                        # –ó–∞—â–∏—Ç–∞ –æ—Ç NoneType errors
                        choices = data.get('choices')
                        if choices and len(choices) > 0:
                            content = choices[0].get('message', {}).get('content')
                            if content:
                                return content
                        
                        logger.error(f"Local LLM Invalid Response: {data}")
                        return None # Return None to trigger fallback
                    else:
                        error_text = await response.text()
                        logger.error(f"Local LLM HTTP {response.status}: {error_text}")
                        return None # Return None to trigger fallback

        except Exception as e:
            import traceback
            self._stats["local_failures"] += 1
            logger.error(f"Local LLM Connection Error: {e}\n{traceback.format_exc()}")
            return None  # Return None to trigger fallback

    async def route_query(self,
                          prompt: str,
                          task_type: Literal['coding', 'chat', 'reasoning', 'creative'] = 'chat',
                          context: list = None,
                          is_private: bool = True,
                          use_rag: bool = True):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å Auto-Fallback –∏ RAG.
        """
        
        # 0. RAG Lookup
        if use_rag:
            rag_context = self.rag.query(prompt)
            if rag_context:
                prompt = f"### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ò–ó –¢–í–û–ï–ô –ü–ê–ú–Ø–¢–ò (RAG):\n{rag_context}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.1. Tool Orchestration (Phase 6)
        if self.tools:
            tool_data = await self.tools.execute_tool_chain(prompt)
            if tool_data:
                prompt = f"### –î–ê–ù–ù–´–ï –ò–ó –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í:\n{tool_data}\n\n### –¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°:\n{prompt}"

        # 0.5. Reasoning Mode (Thinker)
        if task_type == 'reasoning':
            logger.info("üß† Thinking mode activated...")
            return await self._call_gemini(prompt, self.models["thinking"], context, is_private)

        await self.check_local_health() 

        # 1. –ü—ã—Ç–∞–µ–º—Å—è –ª–æ–∫–∞–ª—å–Ω–æ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –∏ –∑–∞–¥–∞—á–∞ –ø—Ä–æ—Å—Ç–∞—è/—á–∞—Ç)
        if self.is_local_available and task_type in ['chat', 'coding']:
            logger.info("Routing to LOCAL", model=self.active_local_model)
            response = await self._call_local_llm(prompt, context, is_private)

            if response:  # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
                self._stats["local_calls"] += 1
                return response
            
            logger.warning("Local LLM failed. Falling back to CLOUD.")

        # 2. Fallback –∏–ª–∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ -> Gemini Cloud
        model_name = self.models.get(task_type, self.models["chat"])
        logger.info("Routing to CLOUD", model=model_name)

        return await self._call_gemini(prompt, model_name, context, is_private)

    async def _call_gemini(self, prompt: str, model_name: str, context: list = None,
                           is_private: bool = True, max_retries: int = 2) -> str:
        """
        –í—ã–∑–æ–≤ Google Gemini —á–µ—Ä–µ–∑ Generative AI SDK.
        –í–∫–ª—é—á–∞–µ—Ç retry —Å exponential backoff –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö 429/500.
        """
        if not self.gemini_client:
            return "‚ùå –û—à–∏–±–∫–∞: Gemini SDK –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å `GEMINI_API_KEY` –≤ `.env`."

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π System Prompt –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏—á–Ω–æ—Å—Ç–∏ (Persona)
        from src.core.prompts import get_system_prompt
        base_instructions = get_system_prompt(is_private)

        persona_prompt = ""
        if self.persona:
            persona_prompt = self.persona.get_current_prompt()

        system_instructions = f"{persona_prompt}\n\n{base_instructions}"

        # –ü–µ—Ä–µ–¥–∞—ë–º system instruction
        model = genai.GenerativeModel(model_name, system_instruction=system_instructions)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
        full_prompt = prompt
        if context:
            history_str = "\n".join(
                [f"{msg.get('role', 'user')}: {msg.get('text', '')}" for msg in context]
            )
            full_prompt = f"History:\n{history_str}\n\nCurrent Request: {prompt}"

        # Retry —Å exponential backoff (429 rate limit, 500 server error)
        for attempt in range(max_retries + 1):
            try:
                response = await model.generate_content_async(full_prompt)

                if not response or not response.text:
                    return "‚ùå AI –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (–∏–ª–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∫–æ–Ω—Ç–µ–Ω—Ç)."

                self._stats["cloud_calls"] += 1
                return response.text

            except Exception as e:
                error_str = str(e)
                # Retry –ø—Ä–∏ rate limit (429) –∏–ª–∏ server error (500)
                if attempt < max_retries and ("429" in error_str or "500" in error_str):
                    wait = 2 ** (attempt + 1)  # 2s, 4s
                    logger.warning(f"Gemini retry {attempt+1}/{max_retries}, wait {wait}s", error=error_str)
                    await asyncio.sleep(wait)
                    continue

                self._stats["cloud_failures"] += 1
                logger.error("Gemini API Error", error=error_str, attempt=attempt)
                return f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}"

    async def diagnose(self) -> dict:
        """
        –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: {subsystem: {ok: bool, status: str}}
        """
        result = {}

        # 1. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        local_ok = await self.check_local_health(force=True)
        result["Local AI"] = {
            "ok": local_ok,
            "status": f"{self.local_engine}: {self.active_local_model}" if local_ok else "Offline",
        }

        # 2. Gemini Cloud
        gemini_ok = self.gemini_client is not None
        result["Gemini Cloud"] = {
            "ok": gemini_ok,
            "status": f"Ready ({self.models['chat']})" if gemini_ok else "No API Key",
        }

        # 3. RAG Engine
        try:
            rag_count = self.rag.get_total_documents()
            result["RAG Engine"] = {"ok": True, "status": f"{rag_count} documents"}
        except Exception as e:
            result["RAG Engine"] = {"ok": False, "status": str(e)}

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–∑–æ–≤–æ–≤
        result["Call Stats"] = {
            "ok": True,
            "status": (
                f"Local: {self._stats['local_calls']} ok / {self._stats['local_failures']} fail, "
                f"Cloud: {self._stats['cloud_calls']} ok / {self._stats['cloud_failures']} fail"
            ),
        }

        # 5. RAM
        ram_info = self.get_ram_usage()
        if "error" not in ram_info:
            result["System RAM"] = {
                "ok": ram_info["percent"] < 90,
                "status": f"{ram_info['used_gb']}/{ram_info['total_gb']} GB ({ram_info['percent']}%)",
            }
        else:
            result["System RAM"] = {"ok": True, "status": "N/A"}

        return result

    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–∏—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–º–∞–Ω–¥—ã !model."""
        return {
            "cloud_models": self.models.copy(),
            "local_engine": self.local_engine,
            "local_model": self.active_local_model,
            "local_available": self.is_local_available,
            "stats": self._stats.copy(),
        }

    def get_ram_usage(self) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM —á–µ—Ä–µ–∑ SystemMonitor.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π —Ç—è–∂—ë–ª—ã—Ö –º–æ–¥–µ–ª–µ–π (Flux, Whisper Large)
        —á—Ç–æ–±—ã –Ω–µ –∫—Ä–∞—à–Ω—É—Ç—å —Å–∏—Å—Ç–µ–º—É.
        """
        try:
            from src.utils.system_monitor import SystemMonitor
            snapshot = SystemMonitor.get_snapshot()
            return {
                "total_gb": round(snapshot.ram_total_gb, 1),
                "used_gb": round(snapshot.ram_used_gb, 1),
                "available_gb": round(snapshot.ram_available_gb, 1),
                "percent": snapshot.ram_percent,
                "can_load_heavy": SystemMonitor.can_load_heavy_model()
            }
        except Exception as e:
            logger.warning(f"RAM check failed: {e}")
            return {"error": str(e), "can_load_heavy": True}  # –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ–º