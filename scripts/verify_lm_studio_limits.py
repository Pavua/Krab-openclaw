# -*- coding: utf-8 -*-
import asyncio
import json
import os
import sys
from unittest.mock import MagicMock, patch, AsyncMock
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.append(str(Path(__file__).parent.parent))

from src.core.model_manager import ModelRouter

async def verify_limits():
    print("üß™ Verifying LM Studio Request Payload...")
    
    config = {
        "LOCAL_LLM_URL": "http://localhost:1234/v1",
        "LOCAL_ENGINE": "lm-studio"
    }
    
    router = ModelRouter(config=config)
    router.active_local_model = "test-model"
    router.is_local_available = True
    
    # –ú–æ–∫–∞–µ–º aiohttp.ClientSession.post
    with patch("aiohttp.ClientSession.post") as mock_post:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–∫ –æ—Ç–≤–µ—Ç–∞
        mock_resp = MagicMock()
        mock_resp.status = 200
        mock_resp.json = AsyncMock(return_value={
            "choices": [{"message": {"content": "Hello world"}}]
        })
        mock_post.return_value.__aenter__.return_value = mock_resp
        
        await router._call_local_llm("Test prompt")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤—ã–∑–æ–≤–∞
        args, kwargs = mock_post.call_args
        payload = kwargs.get("json", {})
        
        print(f"üì¶ Sent payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
        
        if payload.get("max_tokens") == 2048:
            print("‚úÖ SUCCESS: max_tokens is present and correct.")
        else:
            print(f"‚ùå FAILED: max_tokens is {payload.get('max_tokens')}")
            
        if "stop" in payload and "<|im_end|>" in payload["stop"]:
            print("‚úÖ SUCCESS: stop sequences are present.")
        else:
            print("‚ùå FAILED: stop sequences are missing.")

if __name__ == "__main__":
    asyncio.run(verify_limits())
